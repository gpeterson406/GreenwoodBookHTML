<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Chi-square tests | Intermediate Statistics with R</title>
  <meta name="description" content="Chapter 5 Chi-square tests | Intermediate Statistics with R" />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Chi-square tests | Intermediate Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="gpeterson406/Greenwood_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Chi-square tests | Intermediate Statistics with R" />
  
  
  

<meta name="author" content="Mark C Greenwood" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter4.html"/>
<link rel="next" href="chapter6.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="font-awesome.min.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intermediate Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#section1-1"><i class="fa fa-check"></i><b>1.1</b> Overview of methods</a></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#section1-2"><i class="fa fa-check"></i><b>1.2</b> Getting started in R</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#section1-3"><i class="fa fa-check"></i><b>1.3</b> Basic summary statistics, histograms, and boxplots using R</a></li>
<li class="chapter" data-level="1.4" data-path="chapter1.html"><a href="chapter1.html#section1-4"><i class="fa fa-check"></i><b>1.4</b> Chapter summary</a></li>
<li class="chapter" data-level="1.5" data-path="chapter1.html"><a href="chapter1.html#section1-5"><i class="fa fa-check"></i><b>1.5</b> Summary of important R code</a></li>
<li class="chapter" data-level="1.6" data-path="chapter1.html"><a href="chapter1.html#section1-6"><i class="fa fa-check"></i><b>1.6</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> (R)e-Introduction to statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#section2-1"><i class="fa fa-check"></i><b>2.1</b> Histograms, boxplots, and density curves</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#section2-2"><i class="fa fa-check"></i><b>2.2</b> Pirate-plots</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#section2-3"><i class="fa fa-check"></i><b>2.3</b> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#section2-4"><i class="fa fa-check"></i><b>2.4</b> Permutation testing for the two sample mean situation</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#section2-5"><i class="fa fa-check"></i><b>2.5</b> Hypothesis testing (general)</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#section2-6"><i class="fa fa-check"></i><b>2.6</b> Connecting randomization (nonparametric) and parametric tests</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#section2-7"><i class="fa fa-check"></i><b>2.7</b> Second example of permutation tests</a></li>
<li class="chapter" data-level="2.8" data-path="chapter2.html"><a href="chapter2.html#section2-8"><i class="fa fa-check"></i><b>2.8</b> Reproducibility Crisis: Moving beyond p &lt; 0.05, publication bias, and multiple testing issues</a></li>
<li class="chapter" data-level="2.9" data-path="chapter2.html"><a href="chapter2.html#section2-9"><i class="fa fa-check"></i><b>2.9</b> Confidence intervals and bootstrapping</a></li>
<li class="chapter" data-level="2.10" data-path="chapter2.html"><a href="chapter2.html#section2-10"><i class="fa fa-check"></i><b>2.10</b> Bootstrap confidence intervals for difference in GPAs</a></li>
<li class="chapter" data-level="2.11" data-path="chapter2.html"><a href="chapter2.html#section2-11"><i class="fa fa-check"></i><b>2.11</b> Chapter summary</a></li>
<li class="chapter" data-level="2.12" data-path="chapter2.html"><a href="chapter2.html#section2-12"><i class="fa fa-check"></i><b>2.12</b> Summary of important R code</a></li>
<li class="chapter" data-level="2.13" data-path="chapter2.html"><a href="chapter2.html#section2-13"><i class="fa fa-check"></i><b>2.13</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> One-Way ANOVA</a><ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#section3-1"><i class="fa fa-check"></i><b>3.1</b> Situation</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#section3-2"><i class="fa fa-check"></i><b>3.2</b> Linear model for One-Way ANOVA (cell means and reference-coding)</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#section3-3"><i class="fa fa-check"></i><b>3.3</b> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#section3-4"><i class="fa fa-check"></i><b>3.4</b> ANOVA model diagnostics including QQ-plots</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#section3-5"><i class="fa fa-check"></i><b>3.5</b> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#section3-6"><i class="fa fa-check"></i><b>3.6</b> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li class="chapter" data-level="3.7" data-path="chapter3.html"><a href="chapter3.html#section3-7"><i class="fa fa-check"></i><b>3.7</b> Pair-wise comparisons for the Overtake data</a></li>
<li class="chapter" data-level="3.8" data-path="chapter3.html"><a href="chapter3.html#section3-8"><i class="fa fa-check"></i><b>3.8</b> Chapter summary</a></li>
<li class="chapter" data-level="3.9" data-path="chapter3.html"><a href="chapter3.html#section3-9"><i class="fa fa-check"></i><b>3.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="3.10" data-path="chapter3.html"><a href="chapter3.html#section3-10"><i class="fa fa-check"></i><b>3.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Two-Way ANOVA</a><ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#section4-1"><i class="fa fa-check"></i><b>4.1</b> Situation</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#section4-2"><i class="fa fa-check"></i><b>4.2</b> Designing a two-way experiment and visualizing results</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#section4-3"><i class="fa fa-check"></i><b>4.3</b> Two-Way ANOVA models and hypothesis tests</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#section4-4"><i class="fa fa-check"></i><b>4.4</b> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#section4-5"><i class="fa fa-check"></i><b>4.5</b> Observational study example: The Psychology of Debt</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#section4-6"><i class="fa fa-check"></i><b>4.6</b> Pushing Two-Way ANOVA to the limit: Un-replicated designs and Estimability</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#section4-7"><i class="fa fa-check"></i><b>4.7</b> Chapter summary</a></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#section4-8"><i class="fa fa-check"></i><b>4.8</b> Summary of important R code</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#section4-9"><i class="fa fa-check"></i><b>4.9</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Chi-square tests</a><ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#section5-1"><i class="fa fa-check"></i><b>5.1</b> Situation, contingency tables, and tableplots</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#section5-2"><i class="fa fa-check"></i><b>5.2</b> Homogeneity test hypotheses</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#section5-3"><i class="fa fa-check"></i><b>5.3</b> Independence test hypotheses</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#section5-4"><i class="fa fa-check"></i><b>5.4</b> Models for R by C tables</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#section5-5"><i class="fa fa-check"></i><b>5.5</b> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#section5-6"><i class="fa fa-check"></i><b>5.6</b> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#section5-7"><i class="fa fa-check"></i><b>5.7</b> Examining residuals for the source of differences</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#section5-8"><i class="fa fa-check"></i><b>5.8</b> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#section5-9"><i class="fa fa-check"></i><b>5.9</b> Political party and voting results: Complete analysis</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#section5-10"><i class="fa fa-check"></i><b>5.10</b> Is cheating and lying related in students?</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#section5-11"><i class="fa fa-check"></i><b>5.11</b> Analyzing a stratified random sample of California schools</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#section5-12"><i class="fa fa-check"></i><b>5.12</b> Chapter summary</a></li>
<li class="chapter" data-level="5.13" data-path="chapter5.html"><a href="chapter5.html#section5-13"><i class="fa fa-check"></i><b>5.13</b> Summary of important R commands</a></li>
<li class="chapter" data-level="5.14" data-path="chapter5.html"><a href="chapter5.html#section5-14"><i class="fa fa-check"></i><b>5.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Correlation and Simple Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#section6-1"><i class="fa fa-check"></i><b>6.1</b> Relationships between two quantitative variables</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#section6-2"><i class="fa fa-check"></i><b>6.2</b> Estimating the correlation coefficient</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#section6-3"><i class="fa fa-check"></i><b>6.3</b> Relationships between variables by groups</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#section6-4"><i class="fa fa-check"></i><b>6.4</b> Inference for the correlation coefficient</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#section6-5"><i class="fa fa-check"></i><b>6.5</b> Are tree diameters related to tree heights?</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#section6-6"><i class="fa fa-check"></i><b>6.6</b> Describing relationships with a regression model</a></li>
<li class="chapter" data-level="6.7" data-path="chapter6.html"><a href="chapter6.html#section6-7"><i class="fa fa-check"></i><b>6.7</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.8" data-path="chapter6.html"><a href="chapter6.html#section6-8"><i class="fa fa-check"></i><b>6.8</b> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li class="chapter" data-level="6.9" data-path="chapter6.html"><a href="chapter6.html#section6-9"><i class="fa fa-check"></i><b>6.9</b> Outliers: leverage and influence</a></li>
<li class="chapter" data-level="6.10" data-path="chapter6.html"><a href="chapter6.html#section6-10"><i class="fa fa-check"></i><b>6.10</b> Residual diagnostics – setting the stage for inference</a></li>
<li class="chapter" data-level="6.11" data-path="chapter6.html"><a href="chapter6.html#section6-11"><i class="fa fa-check"></i><b>6.11</b> Old Faithful discharge and waiting times</a></li>
<li class="chapter" data-level="6.12" data-path="chapter6.html"><a href="chapter6.html#section6-12"><i class="fa fa-check"></i><b>6.12</b> Chapter summary</a></li>
<li class="chapter" data-level="6.13" data-path="chapter6.html"><a href="chapter6.html#section6-13"><i class="fa fa-check"></i><b>6.13</b> Summary of important R code</a></li>
<li class="chapter" data-level="6.14" data-path="chapter6.html"><a href="chapter6.html#section6-14"><i class="fa fa-check"></i><b>6.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Simple linear regression inference</a><ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#section7-1"><i class="fa fa-check"></i><b>7.1</b> Model</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#section7-2"><i class="fa fa-check"></i><b>7.2</b> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#section7-3"><i class="fa fa-check"></i><b>7.3</b> Bozeman temperature trend</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#section7-4"><i class="fa fa-check"></i><b>7.4</b> Randomization-based inferences for the slope coefficient</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#section7-5"><i class="fa fa-check"></i><b>7.5</b> Transformations part I: Linearizing relationships</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#section7-6"><i class="fa fa-check"></i><b>7.6</b> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#section7-7"><i class="fa fa-check"></i><b>7.7</b> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li class="chapter" data-level="7.8" data-path="chapter7.html"><a href="chapter7.html#section7-8"><i class="fa fa-check"></i><b>7.8</b> Chapter summary</a></li>
<li class="chapter" data-level="7.9" data-path="chapter7.html"><a href="chapter7.html#section7-9"><i class="fa fa-check"></i><b>7.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="7.10" data-path="chapter7.html"><a href="chapter7.html#section7-10"><i class="fa fa-check"></i><b>7.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#section8-1"><i class="fa fa-check"></i><b>8.1</b> Going from SLR to MLR</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#section8-2"><i class="fa fa-check"></i><b>8.2</b> Validity conditions in MLR</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#section8-3"><i class="fa fa-check"></i><b>8.3</b> Interpretation of MLR terms</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#section8-4"><i class="fa fa-check"></i><b>8.4</b> Comparing multiple regression models</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#section8-5"><i class="fa fa-check"></i><b>8.5</b> General recommendations for MLR interpretations and VIFs</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#section8-6"><i class="fa fa-check"></i><b>8.6</b> MLR inference: Parameter inferences using the t-distribution</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#section8-7"><i class="fa fa-check"></i><b>8.7</b> Overall F-test in multiple linear regression</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#section8-8"><i class="fa fa-check"></i><b>8.8</b> Case study: First year college GPA and SATs</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#section8-9"><i class="fa fa-check"></i><b>8.9</b> Different intercepts for different groups: MLR with indicator variables</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#section8-10"><i class="fa fa-check"></i><b>8.10</b> Additive MLR with more than two groups: Headache example</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#section8-11"><i class="fa fa-check"></i><b>8.11</b> Different slopes and different intercepts</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#section8-12"><i class="fa fa-check"></i><b>8.12</b> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#section8-13"><i class="fa fa-check"></i><b>8.13</b> AICs for model selection</a></li>
<li class="chapter" data-level="8.14" data-path="chapter8.html"><a href="chapter8.html#section8-14"><i class="fa fa-check"></i><b>8.14</b> Case study: Forced expiratory volume model selection using AICs</a></li>
<li class="chapter" data-level="8.15" data-path="chapter8.html"><a href="chapter8.html#section8-15"><i class="fa fa-check"></i><b>8.15</b> Chapter summary</a></li>
<li class="chapter" data-level="8.16" data-path="chapter8.html"><a href="chapter8.html#section8-16"><i class="fa fa-check"></i><b>8.16</b> Summary of important R code</a></li>
<li class="chapter" data-level="8.17" data-path="chapter8.html"><a href="chapter8.html#section8-17"><i class="fa fa-check"></i><b>8.17</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Case studies</a><ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#section9-1"><i class="fa fa-check"></i><b>9.1</b> Overview of material covered</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#section9-2"><i class="fa fa-check"></i><b>9.2</b> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#section9-3"><i class="fa fa-check"></i><b>9.3</b> Ants learn to rely on more informative attributes during decision-making</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#section9-4"><i class="fa fa-check"></i><b>9.4</b> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#section9-5"><i class="fa fa-check"></i><b>9.5</b> What do didgeridoos really do about sleepiness?</a></li>
<li class="chapter" data-level="9.6" data-path="chapter9.html"><a href="chapter9.html#section9-6"><i class="fa fa-check"></i><b>9.6</b> General summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Intermediate Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter5" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Chi-square tests</h1>
<div id="section5-1" class="section level2">
<h2><span class="header-section-number">5.1</span> Situation, contingency tables, and tableplots</h2>
<p>In this chapter, the focus shifts briefly from analyzing quantitative
response variables to methods
for handling categorical response variables. This is important because in some
situations it is not possible to measure the response variable quantitatively.
For example, we will analyze the results from a clinical trial where the results
for the subjects were measured as one of three categories: <em>no improvement</em>,
<em>some improvement</em>, and <em>marked improvement</em>. While that type of response
could be treated as
numerical, coded possibly as 1, 2, and 3, it would be difficult to assume that
the responses such as those follow a normal distribution since they are
<strong><em>discrete</em></strong> (not continuous, measured at whole number values only) and,
more importantly,
the difference between <em>no improvement</em> and <em>some improvement</em> is not
necessarily the same as the difference between <em>some</em> and <em>marked improvement</em>.
If it is treated numerically, then the differences between levels are assumed to be the same
unless a different coding scheme is used (say 1, 2, and 5). It is better to
treat this type of responses as being in one of the three categories and use
statistical methods that don’t make unreasonable and arbitrary assumptions about what the
numerical coding might mean. The study being performed here involved subjects
randomly assigned to either a treatment or a placebo (control) group and we
want to address research questions similar to those considered in
Chapters <a href="chapter2.html#chapter2">2</a> and <a href="chapter3.html#chapter3">3</a> –
assessing differences in a response variable among two or more groups. With quantitative
responses, the differences in the distributions are parameterized via the means
of the groups and we used linear models. With
categorical responses, the focus is on the probabilities of getting responses in
each category and whether they differ among the groups.</p>
<p>We start with some useful summary techniques, both numerical and graphical,
applied to some examples of
studies these methods can be used to analyze. Graphical techniques provide
opportunities for assessing specific patterns in variables, relationships
between variables, and for generally understanding the responses obtained.
There are many different types of plots and each can elucidate certain features
of data. The <em>tableplot</em>, briefly introduced in Chapter <a href="chapter4.html#chapter4">4</a>, is a great and often fun starting point for working with data sets that contain categorical variables. We will start here with using it to help us
understand some aspects of the results from a double-blind randomized clinical
trial investigating a treatment for rheumatoid arthritis.

These data are available
in the <code>Arthritis</code> data set available in the <code>vcd</code> package <span class="citation">(Meyer, Zeileis, and Hornik <a href="#ref-R-vcd" role="doc-biblioref">2017</a>)</span>.

There were <span class="math inline">\(n=84\)</span> subjects, with some demographic
information recorded
along with the <code>Treatment</code> status (<em>Treated</em>, <em>Placebo</em>) and whether the
patients’ arthritis symptoms <code>Improved</code> (with levels of <em>None</em>, <em>Some</em>,
and <em>Marked</em>). When using <code>tableplot</code>, we may
not want to display everything in the tibble and can just select some
of the variables. We use <code>Treatment</code>, <code>Improved</code>, <code>Gender</code>, and <code>Age</code>
in the <code>select=...</code> option with a <code>c()</code> and commas between the names of
the variables we want to display as shown below. The first one in the list is also the one that
the data are sorted on and is what we want here – to start with sorting observations based on <code>Treatment</code> status.</p>

<div class="sourceCode" id="cb393"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb393-1"><a href="chapter5.html#cb393-1"></a><span class="kw">library</span>(vcd)</span>
<span id="cb393-2"><a href="chapter5.html#cb393-2"></a><span class="kw">data</span>(Arthritis) <span class="co">#Double-blind clinical trial with treatment and control groups</span></span>
<span id="cb393-3"><a href="chapter5.html#cb393-3"></a><span class="kw">library</span>(tibble)</span>
<span id="cb393-4"><a href="chapter5.html#cb393-4"></a>Arthritis &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(Arthritis)</span>
<span id="cb393-5"><a href="chapter5.html#cb393-5"></a><span class="co">#Homogeneity example</span></span>
<span id="cb393-6"><a href="chapter5.html#cb393-6"></a><span class="kw">library</span>(tabplot)</span>
<span id="cb393-7"><a href="chapter5.html#cb393-7"></a><span class="kw">library</span>(RColorBrewer)</span>
<span id="cb393-8"><a href="chapter5.html#cb393-8"></a><span class="co"># Options needed to prevent errors on PC</span></span>
<span id="cb393-9"><a href="chapter5.html#cb393-9"></a><span class="kw">options</span>(<span class="dt">ffbatchbytes =</span> <span class="dv">1024</span><span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="dv">128</span>); <span class="kw">options</span>(<span class="dt">ffmaxbytes =</span> <span class="dv">1024</span><span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="dv">128</span> <span class="op">*</span><span class="st"> </span><span class="dv">32</span>) </span>
<span id="cb393-10"><a href="chapter5.html#cb393-10"></a><span class="kw">tableplot</span>(Arthritis,<span class="dt">select=</span><span class="kw">c</span>(Treatment,Improved,Sex,Age),<span class="dt">pals=</span><span class="kw">list</span>(<span class="st">&quot;BrBG&quot;</span>))</span></code></pre></div>
<div class="figure"><span id="fig:Figure5-1"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-1-1.png" alt="Tableplot of the arthritis data set." width="960" />
<p class="caption">
Figure 5.1: Tableplot of the arthritis data set.
</p>
</div>
<p>The first thing we can gather from Figure <a href="chapter5.html#fig:Figure5-1">5.1</a> is that there
are no red cells so there were no missing
observations in the data set. Missing observations regularly arise in real
studies when observations are not obtained for many different reasons and it is
always good to check for missing data issues – this plot provides a quick visual
method for doing that check. Primarily we are interested in whether the
treatment led to a different pattern (or rates) of improvement responses. There
seems to be more light (<em>Marked</em>) improvement responses in the treatment
group and more dark (<em>None</em>) responses in the placebo group.
This sort of plot also helps us to simultaneously consider the role of other
variables in the observed responses. You can see the sex of each subject in the
vertical panel for <code>Sex</code> and it seems
that there is a relatively balanced mix of males and females in the
treatment/placebo groups. Quantitative variables are also displayed with
horizontal bars corresponding to the responses (the x-axis provides the units of the responses, here in years). From the panel for
<code>Age</code>, we can see that the ages of subjects ranged from the 20s to 70s
and that there is no clear difference in
the ages between the treated and placebo groups. If, for example, all the male
subjects had ended up being randomized into the treatment group, then we might
have worried about whether sex and treatment were confounded and whether any
differences in the responses might be due to sex instead of the treatment. The
random assignment of treatment/placebo to the subjects appears to have been
successful here in generating a mix of ages and sexes among the
two treatment groups<a href="#fn86" class="footnote-ref" id="fnref86"><sup>86</sup></a>. The main benefit of this sort of plot is the ability to
visualize more than two categorical variables simultaneously. But now we want
to focus more directly on the researchers’ main question – does the treatment
lead to different improvement outcomes than the placebo?</p>
<p>To directly assess the effects of the treatment, we
want to display just the two variables of interest. <strong><em>Stacked bar charts</em></strong>
provide a method of displaying the response patterns (in <code>Improved</code>) across
the levels of a predictor variable (<code>Treatment</code>) by displaying a bar for each
predictor variable level and the proportions of responses in each category of
the response in each of those groups. If the placebo is as effective as the
treatment, then we would expect similar proportions of responses in each
improvement category. A difference in the effectiveness would manifest in
different proportions in the different improvement categories between <em>Treated</em>
and <em>Placebo</em>. To get information in this direction, we start with
obtaining the counts in each combination of categories using the <code>tally</code>
function to generate contingency tables.

<strong><em>Contingency tables</em></strong> with
<strong><em>R</em></strong> rows and <strong><em>C</em></strong> columns (called <strong><em>R by C tables</em></strong>) summarize
the counts of observations in each combination of the explanatory and
response variables.

In these data, there are <span class="math inline">\(R=2\)</span> rows and <span class="math inline">\(C=3\)</span> columns
making a <span class="math inline">\(2\times 3\)</span> table – note that you do not count the row
and column for the “Totals” in defining the size of the table. In the table,
there seems to be many more <em>Marked</em> improvement responses (21 vs 7) and
fewer <em>None</em> responses (13 vs 29) in the treated group compared to the
placebo group.</p>
<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb394-1"><a href="chapter5.html#cb394-1"></a><span class="kw">library</span>(mosaic)</span>
<span id="cb394-2"><a href="chapter5.html#cb394-2"></a><span class="kw">tally</span>(<span class="op">~</span>Treatment<span class="op">+</span>Improved, <span class="dt">data=</span>Arthritis, <span class="dt">margins=</span>T)</span></code></pre></div>
<pre><code>##          Improved
## Treatment None Some Marked Total
##   Placebo   29    7      7    43
##   Treated   13    7     21    41
##   Total     42   14     28    84</code></pre>
<p>Using the <code>tally</code> function with <code>~x+y</code> provides a contingency table with
the <code>x</code> variable on the rows and the <code>y</code> variable on the columns, with
<code>margins=T</code> as an option so we can obtain the totals along the rows,
columns, and table total of <span class="math inline">\(N=84\)</span>.


In general, contingency tables contain
the counts <span class="math inline">\(n_{rc}\)</span> in the <span class="math inline">\(r^{th}\)</span> row and <span class="math inline">\(c^{th}\)</span> column where
<span class="math inline">\(r=1,\ldots,R\)</span> and <span class="math inline">\(c=1,\ldots,C\)</span>. We can also define the <strong><em>row totals</em></strong>
as the sum across the columns of the counts in row <span class="math inline">\(r\)</span> as</p>
<p><span class="math display">\[\mathbf{n_{r\bullet}}=\Sigma^C_{c=1}n_{rc},\]</span></p>
<p>the <strong><em>column totals</em></strong> as the sum across the rows for the counts in column <span class="math inline">\(c\)</span> as</p>
<p><span class="math display">\[\mathbf{n_{\bullet c}}=\Sigma^R_{r=1}n_{rc},\]</span></p>
<p>and the <strong><em>table total</em></strong> as</p>
<p><span class="math display">\[\mathbf{N}=\Sigma^R_{r=1}\mathbf{n_{r\bullet}} = \Sigma^C_{c=1}\mathbf{n_{\bullet c}}
= \Sigma^R_{r=1}\Sigma^C_{c=1}\mathbf{n_{rc}}.\]</span></p>
<p>We’ll need these quantities to do some calculations in a bit. A generic
contingency table with added row, column,
and table totals just like the previous result from the <code>tally</code>
function is provided in Table <a href="chapter5.html#tab:Table5-1">5.1</a>.
</p>

<table>
<caption><span id="tab:Table5-1">Table 5.1: </span> General notation for counts in an <em>R</em> by <em>C</em> contingency table.</caption>
<colgroup>
<col width="7%" />
<col width="17%" />
<col width="17%" />
<col width="17%" />
<col width="5%" />
<col width="17%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center"><strong>Response<br />
Level 1</strong></th>
<th align="center"><strong>Response<br />
Level 2</strong></th>
<th align="center"><strong>Response<br />
Level 3</strong></th>
<th align="center"><br />
<strong>…</strong></th>
<th align="center"><strong>Response<br />
Level C</strong></th>
<th align="center"><br />
<strong>Totals</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Group 1</strong></td>
<td align="center"><span class="math inline">\(n_{11}\)</span></td>
<td align="center"><span class="math inline">\(n_{12}\)</span></td>
<td align="center"><span class="math inline">\(n_{13}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(n_{1C}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{n_{1 \bullet}}\)</span></td>
</tr>
<tr class="even">
<td align="center"><strong>Group 2</strong></td>
<td align="center"><span class="math inline">\(n_{21}\)</span></td>
<td align="center"><span class="math inline">\(n_{22}\)</span></td>
<td align="center"><span class="math inline">\(n_{23}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(n_{2C}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{n_{2 \bullet}}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><strong>…</strong></td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center"><strong>…</strong></td>
</tr>
<tr class="even">
<td align="center"><strong>Group R</strong></td>
<td align="center"><span class="math inline">\(n_{R1}\)</span></td>
<td align="center"><span class="math inline">\(n_{R2}\)</span></td>
<td align="center"><span class="math inline">\(n_{R3}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(n_{RC}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{n_{R \bullet}}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><strong>Totals</strong></td>
<td align="center"><span class="math inline">\(\boldsymbol{n_{\bullet 1}}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{n_{\bullet 2}}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{n_{\bullet 3}}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(\boldsymbol{n_{\bullet C}}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{N}\)</span></td>
</tr>
</tbody>
</table>
<p>Comparing counts from the contingency table is useful, but comparing proportions
in each category is better, especially when the sample sizes in the levels of
the explanatory variable differ. Switching the formula used in the <code>tally</code>
function formula to <code>~ y|x</code> and adding the <code>format="proportion"</code>
option provides the proportions in the response categories conditional on the
category of the predictor (these are
called <strong><em>conditional proportions</em></strong> or the <strong><em>conditional distribution</em></strong> of,
here, <em>Improved</em> on <em>Treatment</em>)<a href="#fn87" class="footnote-ref" id="fnref87"><sup>87</sup></a>.

Note that they sum to 1.0 in each level of x, <em>placebo</em> or <em>treated</em>:</p>
<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb396-1"><a href="chapter5.html#cb396-1"></a><span class="kw">tally</span>(<span class="op">~</span>Improved<span class="op">|</span>Treatment, <span class="dt">data=</span>Arthritis, <span class="dt">format=</span><span class="st">&quot;proportion&quot;</span>, <span class="dt">margins=</span>T)</span></code></pre></div>
<pre><code>##         Treatment
## Improved   Placebo   Treated
##   None   0.6744186 0.3170732
##   Some   0.1627907 0.1707317
##   Marked 0.1627907 0.5121951
##   Total  1.0000000 1.0000000</code></pre>
<p>This version of the <code>tally</code> result switches the variables between the rows and columns from the
first summary of the data but the single
“Total” row makes it clear to read the proportions down the columns in this
version of the table.

In this application, it shows how the proportions seem to be different among categories of <em>Improvement</em> between the placebo and treatment groups. This matches the previous thoughts on
these data, but now a difference of marked improvement of 16% vs 51% is more
clearly a big difference. We can also display this result using a
<strong><em>stacked bar chart</em></strong>  that displays the same information using the <code>plot</code>
function with a <code>y~x</code> formula:</p>

<div class="sourceCode" id="cb398"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb398-1"><a href="chapter5.html#cb398-1"></a><span class="kw">plot</span>(Improved<span class="op">~</span>Treatment, <span class="dt">data=</span>Arthritis,</span>
<span id="cb398-2"><a href="chapter5.html#cb398-2"></a>     <span class="dt">main=</span><span class="st">&quot;Stacked Bar Chart of Arthritis Data&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure5-2"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-2-1.png" alt="Stacked bar chart of Arthritis data. The left bar is for the Placebo group and the right bar is for the Treated group. The width of the bars is based on relative size of each group and the portion of the total height of each shaded area is the proportion of that group in each category. The darkest shading is for “none”, medium shading for “some”, and the lightest shading for “marked”, as labeled on the y-axis." width="480" />
<p class="caption">
Figure 5.2: Stacked bar chart of Arthritis data. The left bar is for the Placebo group and the right bar is for the Treated group. The width of the bars is based on relative size of each group and the portion of the total height of each shaded area is the proportion of that group in each category. The darkest shading is for “none”, medium shading for “some”, and the lightest shading for “marked”, as labeled on the y-axis.
</p>
</div>
<p>The stacked bar chart in Figure <a href="chapter5.html#fig:Figure5-2">5.2</a> displays the previous
conditional proportions for the groups, with
the same relatively clear difference between the groups persisting. If you run
the <code>plot</code> function with variables that are
coded numerically, it will make a very different looking graph (R is smart!) so
again be careful that you are instructing R to treat your variables as
categorical if they really are categorical. R is powerful but can’t read your
mind!</p>
<p>In this chapter, we analyze data collected in two different fashions and
modify the hypotheses to
reflect the differences in the data collection processes, choosing either
between what are called Homogeneity and Independence tests. The previous
situation where levels of a treatment are randomly assigned to the subjects in
a study describes the situation for what is called a <strong><em>Homogeneity Test</em></strong>.

Homogeneity also applies when random samples are taken from each population of
interest to generate the observations in each group of the explanatory variable based on the population groups.

These sorts of situations resemble many of the examples from
Chapter <a href="chapter3.html#chapter3">3</a> where treatments were assigned to subjects. The other
situation considered is where a
single sample is collected to represent a population and then a contingency
table is formed based on responses on two categorical variables.

When one
sample is collected and analyzed using a contingency table, the appropriate
analysis is called a Chi-square test of <strong><em>Independence</em></strong> or <strong><em>Association</em></strong>.

In this
situation, it is not necessary to have variables that are clearly classified
as explanatory or response although it is certainly possible. Data that often
align with Independence testing are collected using surveys of
subjects randomly selected from a single, large population. An example,
analyzed below, involves a survey of voters and whether their party affiliation
is related to who they voted for – the republican, democrat, or other
candidate. There is clearly an explanatory variable of the <em>Party affiliation</em>
but a single large sample was taken from the population of all likely voters
so the Independence test needs to be applied.
Another example where Independence is appropriate involves a study of student
cheating behavior. Again, a single sample was taken from the population of
students at a university and this determines that it will be an Independence
test. Students responded to questions about lying to get out of turning in a
paper and/or taking an exam (<em>none</em>, <em>either</em>, or <em>both</em>) and copying on an
exam and/or turning in a paper written by
someone else (<em>neither</em>, <em>either</em>, or <em>both</em>). In this situation, it is not
clear which variable is response or explanatory (which should explain the other) and it does not matter
with the Independence testing framework.

Figure <a href="chapter5.html#fig:Figure5-3">5.3</a> contains
a diagram of the
data collection processes and can help you to identify the appropriate analysis
situation.</p>

<div class="figure" style="text-align: center"><span id="fig:Figure5-3"></span>
<img src="chapter5_files/image027.png" alt="Diagram of the scenarios involved in Homogeneity and Independence tests. Homogeneity testing involves R random samples or subjects assigned to R groups. Independence testing involves a single random sample and measurements on two categorical variables." width="100%" />
<p class="caption">
Figure 5.3: Diagram of the scenarios involved in Homogeneity and Independence tests. Homogeneity testing involves R random samples or subjects assigned to R groups. Independence testing involves a single random sample and measurements on two categorical variables.
</p>
</div>
<p>You will discover that the test statistics are the same for both methods,
which can create some desire
to assume that the differences in the data collection don’t matter. In
Homogeneity designs, the sample size in each group
<span class="math inline">\((\mathbf{n_{1\bullet}},\mathbf{n_{2\bullet},\ldots,\mathbf{n_{R\bullet}}})\)</span>
is fixed (researcher chooses the size of each group).

In Independence situations, the total sample size <span class="math inline">\(\mathbf{N}\)</span> is
fixed but all the <span class="math inline">\(\mathbf{n_{r\bullet}}\text{&#39;s}\)</span> are random (we need the data set to know how many are in each group).

These
differences impact the graphs, hypotheses, and conclusions used even though
the test statistics and p-values are calculated the same way – so we only
need to learn one test statistic to handle the two situations, but we need
to make sure we know which we’re doing!</p>
</div>
<div id="section5-2" class="section level2">
<h2><span class="header-section-number">5.2</span> Homogeneity test hypotheses</h2>
<p>If we define some additional notation, we can then define hypotheses that allow us
to assess evidence related to whether the treatment “matters” in Homogeneity
situations.

This situation is similar to what we did in the One-Way ANOVA (Chapter <a href="chapter3.html#chapter3">3</a>)
situation with quantitative responses but the parameters now
relate to proportions in the response variable categories across the groups.
First we can define the conditional population proportions in level <span class="math inline">\(c\)</span> (column
<span class="math inline">\(c=1,\ldots,C\)</span>) of group <span class="math inline">\(r\)</span> (row <span class="math inline">\(r=1,\ldots,R\)</span>) as <span class="math inline">\(p_{rc}\)</span>.
Table <a href="chapter5.html#tab:Table5-2">5.2</a> shows the proportions, noting that the proportions
in each row sum to 1 since they are conditional on the group of
interest. A <strong><em>transposed</em></strong> (rows and columns flipped) version of this table is
produced by the <code>tally</code> function if you use the formula <code>~y|x</code>.
</p>

<table>
<caption><span id="tab:Table5-2">Table 5.2: </span> Table of conditional proportions in the Homogeneity testing scenario.</caption>
<colgroup>
<col width="8%" />
<col width="18%" />
<col width="18%" />
<col width="18%" />
<col width="5%" />
<col width="18%" />
<col width="12%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center"><strong>Response<br />
Level 1</strong></th>
<th align="center"><strong>Response<br />
Level 2</strong></th>
<th align="center"><strong>Response<br />
Level 3</strong></th>
<th align="center"><br />
<strong>…</strong></th>
<th align="center"><strong>Response<br />
Level C</strong></th>
<th align="center"><br />
<strong>Totals</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Group 1</strong></td>
<td align="center"><span class="math inline">\(p_{11}\)</span></td>
<td align="center"><span class="math inline">\(p_{12}\)</span></td>
<td align="center"><span class="math inline">\(p_{13}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(p_{1C}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{1.0}\)</span></td>
</tr>
<tr class="even">
<td align="center"><strong>Group 2</strong></td>
<td align="center"><span class="math inline">\(p_{21}\)</span></td>
<td align="center"><span class="math inline">\(p_{22}\)</span></td>
<td align="center"><span class="math inline">\(p_{23}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(p_{2C}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{1.0}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><strong>…</strong></td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center"><strong>…</strong></td>
</tr>
<tr class="even">
<td align="center"><strong>Group R</strong></td>
<td align="center"><span class="math inline">\(p_{R1}\)</span></td>
<td align="center"><span class="math inline">\(p_{R2}\)</span></td>
<td align="center"><span class="math inline">\(p_{R3}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(p_{RC}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{1.0}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><strong>Totals</strong></td>
<td align="center"><span class="math inline">\(\boldsymbol{p_{\bullet 1}}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{n_{\bullet 2}}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{p_{\bullet 3}}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(\boldsymbol{p_{\bullet C}}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{1.0}\)</span></td>
</tr>
</tbody>
</table>
<p>In the Homogeneity situation, the null hypothesis is that the distributions are the same in all
the <span class="math inline">\(R\)</span> populations.

This means that the null hypothesis is:</p>
<!-- \newpage -->
<p><span class="math display">\[\begin{array}{rl}
\mathbf{H_0:}\  &amp; \mathbf{p_{11}=p_{21}=\ldots=p_{R1}} \textbf{ and } \mathbf{p_{12}=p_{22}=\ldots=p_{R2}}  \textbf{ and } \mathbf{p_{13}=p_{23}=\ldots=p_{R3}} \\ 
&amp; \textbf{ and } \mathbf{\ldots} \textbf{ and }\mathbf{p_{1C}=p_{2C}=\ldots=p_{RC}}. \\
\end{array}\]</span></p>
<div style="page-break-after: always;"></div>
<p>If all the groups are the same, then they all have the same conditional proportions and we can
more simply write the null hypothesis as:</p>
<p><span class="math display">\[\mathbf{H_0:(p_{r1},p_{r2},\ldots,p_{rC})=(p_1,p_2,\ldots,p_C)} \textbf{ for all } \mathbf{r}.\]</span></p>
<p>In other words, the pattern of proportions across the columns are <strong>the same for all the</strong>
<span class="math inline">\(\mathbf{R}\)</span> <strong>groups</strong>. The alternative is that there is some difference in the proportions
of at least one
response category for at least one group. In slightly more gentle and easier to
reproduce words, equivalently, we can say:</p>
<ul>
<li><span class="math inline">\(\mathbf{H_0:}\)</span> <strong>The population distributions of the responses for variable</strong> <span class="math inline">\(\mathbf{y}\)</span>
<strong>are the same across the</strong> <span class="math inline">\(\mathbf{R}\)</span> <strong>groups</strong>.</li>
</ul>
<p>The alternative hypothesis is then:</p>
<ul>
<li><span class="math inline">\(\mathbf{H_A:}\)</span> <strong>The population distributions of the responses for variable</strong> <span class="math inline">\(\mathbf{y}\)</span>
<strong>are NOT ALL the same across the</strong> <span class="math inline">\(\mathbf{R}\)</span> <strong>groups</strong>.</li>
</ul>
<p>To make this concrete, consider what the proportions could look like if they satisfied
the null hypothesis for the <em>Arthritis</em> example, as displayed in Figure <a href="chapter5.html#fig:Figure5-4">5.4</a>.</p>

<div class="figure"><span id="fig:Figure5-4"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-4-1.png" alt="Plot of one way that the Arthritis proportions could have been if the null hypothesis had been true." width="480" />
<p class="caption">
Figure 5.4: Plot of one way that the Arthritis proportions could have been if the null hypothesis had been true.
</p>
</div>
<p>Note that the proportions in the different response categories do not need to be the
same just that the distribution needs
to be the same across the groups. The null hypothesis does <em>not</em>
require that all three response categories (<em>none</em>, <em>some</em>, <em>marked</em>) be equally
likely. It assumes
that whatever the distribution of proportions is across these three levels of the response that
there is no difference in that distribution between the explanatory variable
(here treated/placebo) groups. Figure <a href="chapter5.html#fig:Figure5-4">5.4</a> shows an example of a
situation where
the null hypothesis is true and the distributions of responses across the
groups look the same but the proportions for <em>none</em>, <em>some</em> and <em>marked</em> are
not all equally likely. That
situation satisfies the null hypothesis. Compare this plot to the one for the
real data set in Figure <a href="chapter5.html#fig:Figure5-2">5.2</a>. It looks like there might be
some differences in
the responses between the treated and placebo groups as that plot looks much
different from this one, but we will need a test statistic and a p-value to
fully address the evidence relative to the previous null hypothesis.</p>
</div>
<div id="section5-3" class="section level2">
<h2><span class="header-section-number">5.3</span> Independence test hypotheses</h2>
<p>When we take a single random sample of size <span class="math inline">\(N\)</span> and make a contingency
table, our inferences relate to whether there is a relationship or
<strong><em>association</em></strong> (that they are not independent) between the variables.
This is related to whether the distributions of proportions match
across rows in the table but is a more general question since we do not
need to determine a variable to condition on, one that takes on the role
of an explanatory variable, from the two variables of interest. In general,
the hypotheses for an Independence test for variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are:
</p>
<ul>
<li><p><span class="math inline">\(\mathbf{H_0}\)</span>: <strong>There is no relationship between</strong> <span class="math inline">\(\mathbf{x}\)</span> <strong>and</strong>
<span class="math inline">\(\mathbf{y}\)</span> <strong>in the population.</strong></p>
<ul>
<li>Or: <span class="math inline">\(H_0\)</span>: <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are independent in the population.</li>
</ul></li>
<li><p><span class="math inline">\(\mathbf{H_A}\)</span>: <strong>There is a relationship between</strong> <span class="math inline">\(\mathbf{x}\)</span> <strong>and</strong>
<span class="math inline">\(\mathbf{y}\)</span> <strong>in the population.</strong></p>
<ul>
<li>Or: <span class="math inline">\(H_A\)</span>: <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are dependent in the population.</li>
</ul></li>
</ul>
<p>To illustrate a test of independence, consider an example involving
data from a national random sample
taken prior to the 2000 U.S. elections from the data set <code>election</code>
from the package <code>poLCA</code> (<span class="citation">Linzer and Lewis. (<a href="#ref-R-poLCA" role="doc-biblioref">2014</a>)</span>, <span class="citation">Linzer and Lewis (<a href="#ref-Linzer2011" role="doc-biblioref">2011</a>)</span>). Each respondent’s
democratic-republican partisan identification was collected,
provided in the <code>PARTY</code> variable for measurements on a seven-point
scale from (1) <em>Strong Democrat</em>, (2) <em>Weak Democrat</em>,
(3) <em>Independent-Democrat</em>, (4) <em>Independent-Independent</em>,
(5) <em>Independent-Republican</em>, (6) <em>Weak Republican</em>, to
(7) <em>Strong Republican</em>. The <code>VOTEF</code> variable that is created below
will contain the candidate that the participants voted for (the data set was
originally coded with 1, 2, and 3 for the candidates and we replaced those
<code>levels</code>  with the candidate names). The contingency table shows some expected
results, that individuals with strong party affiliations tend to vote for the
party nominee with strong support for Gore in the democrats
(<code>PARTY</code> = 1 and 2) and strong support for Bush in the republicans
(<code>PARTY</code> = 6 and 7). As always, we want to support our explorations with
statistical inferences, here with the potential to extend inferences to
the overall population of
voters. The inferences in an independence test are related to whether there is a
relationship between the two variables in the population.

A <strong><em>relationship</em></strong> between variables occurs when knowing the level of
one variable for a person,
say that they voted for Gore, informs the types of responses that you would
expect for that person, here that they are likely affiliated with the Democratic
Party. When there is no relationship (the null hypothesis here), knowing the
level of one variable is not informative about the level of the other variable.</p>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb399-1"><a href="chapter5.html#cb399-1"></a><span class="kw">library</span>(poLCA)</span>
<span id="cb399-2"><a href="chapter5.html#cb399-2"></a><span class="co"># 2000 Survey - use package=&quot;&quot; because other data sets in R have same name</span></span>
<span id="cb399-3"><a href="chapter5.html#cb399-3"></a><span class="kw">data</span>(election, <span class="dt">package=</span><span class="st">&quot;poLCA&quot;</span>) </span>
<span id="cb399-4"><a href="chapter5.html#cb399-4"></a>election &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(election)</span>
<span id="cb399-5"><a href="chapter5.html#cb399-5"></a><span class="co"># Subset variables and remove missing values</span></span>
<span id="cb399-6"><a href="chapter5.html#cb399-6"></a>election2 &lt;-<span class="st"> </span><span class="kw">na.omit</span>(election[,<span class="kw">c</span>(<span class="st">&quot;PARTY&quot;</span>,<span class="st">&quot;VOTE3&quot;</span>)])</span>
<span id="cb399-7"><a href="chapter5.html#cb399-7"></a>election2<span class="op">$</span>VOTEF &lt;-<span class="st"> </span><span class="kw">factor</span>(election2<span class="op">$</span>VOTE3)</span>
<span id="cb399-8"><a href="chapter5.html#cb399-8"></a><span class="kw">levels</span>(election2<span class="op">$</span>VOTEF) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Gore&quot;</span>,<span class="st">&quot;Bush&quot;</span>,<span class="st">&quot;Other&quot;</span>) <span class="co">#Replace 1,2,3 with meaningful names</span></span>
<span id="cb399-9"><a href="chapter5.html#cb399-9"></a><span class="kw">levels</span>(election2<span class="op">$</span>VOTEF) <span class="co">#Check new names of levels in VOTEF</span></span></code></pre></div>
<pre><code>## [1] &quot;Gore&quot;  &quot;Bush&quot;  &quot;Other&quot;</code></pre>
<div class="sourceCode" id="cb401"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb401-1"><a href="chapter5.html#cb401-1"></a>electable &lt;-<span class="st"> </span><span class="kw">tally</span>(<span class="op">~</span>PARTY<span class="op">+</span>VOTEF, <span class="dt">data=</span>election2) <span class="co">#Contingency table</span></span>
<span id="cb401-2"><a href="chapter5.html#cb401-2"></a>electable</span></code></pre></div>
<pre><code>##      VOTEF
## PARTY Gore Bush Other
##     1  238    6     2
##     2  151   18     1
##     3  113   31    13
##     4   37   37    11
##     5   21  124    12
##     6   20  121     2
##     7    3  189     1</code></pre>
<p>The hypotheses for an Independence/Association Test here are:</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: There is no relationship between party affiliation and voting
status in the population.</p>
<ul>
<li>Or: <span class="math inline">\(H_0\)</span>: Party affiliation and voting status are independent in the
population.</li>
</ul></li>
<li><p><span class="math inline">\(H_A\)</span>: There is a relationship between party affiliation and voting
status in the population.</p>
<ul>
<li>Or: <span class="math inline">\(H_A\)</span>: Party affiliation and voting status are dependent in the
population.</li>
</ul></li>
</ul>
<p>You could also write these hypotheses with the variables switched and
that is also perfectly acceptable. Because
these hypotheses are ambivalent about the choice of a variable as an “x” or a
“y”, the summaries of results should be consistent with that idea. We should
not calculate conditional proportions or make stacked bar charts since they
imply a directional relationship from x to y (or results for y conditional on
the levels of x) that might be hard to justify. Our summaries in these
situations are the contingency table (<code>tally(~var1+var2, data=DATASETNAME)</code>)
and a new graph called a <strong><em>mosaic plot</em></strong> (using the <code>mosaicplot</code>
function).
</p>
<p>Mosaic plots display a box for each cell count whose area corresponds
to the proportion of the <em>total</em> data set that is in that cell
<span class="math inline">\((n_{rc}/\mathbf{N})\)</span>. In some cases, the bars can be short or narrow
if proportions of the total are small and the labels can be
hard to read but the same bars or a single line exist for each category of the
variables in all rows and columns. The mosaic plot makes it easy to identify
the most common combination of categories. For example, in
Figure <a href="chapter5.html#fig:Figure5-5">5.5</a> the <em>Gore</em> and <code>PARTY</code> = 1 (<em>Strong Democrat</em>)
box in the top segment under column 1 of the plot has the largest area
so is the highest proportion of the total. Similarly, the middle segment
on the right for the <code>PARTY</code> category 7s corresponds to the <em>Bush</em>
voters who were a 7 (<em>Strong Republican</em>). Knowing that the
middle box in each column is for Bush voters is a little difficult as “Other”
and “Bush” overlap each other in the y-axis labeling but it is easy enough to
sort out the story here if we have briefly explored the contingency table. We
can also get information about the variable used to make the
columns as the width
of the columns is proportional to the number of subjects in each
<code>PARTY</code> category in this plot. There were relatively few 4s
(<em>Independent-Independent</em> responses) in total in the data set.
Also, the <em>Other</em> category was the highest proportion of any
vote-getter in the
<code>PARTY</code> = 4 column but there were actually slightly more
<em>Other</em> votes out of the total in the 3s (<em>Independent-Democrat</em>)
party affiliation. Comparing the size of the 4s &amp; <em>Other</em> segment
with the 3s &amp; <em>Other</em> segment, one should conclude that the 3s &amp; <em>Other</em>
segment is a slightly larger portion of the total data set. There is
generally a gradient of decreasing/increasing voting rates for the
two main party candidates
across the party affiliations, but there are a few exceptions. For
example, the
proportion of <em>Gore</em> voters goes up slightly between the <code>PARTY</code>
affiliations of 5s and 6s – as the voters become more strongly republican. To
have evidence of a relationship, there just needs to be a pattern of variation
across the plot of some sort but it does not need to follow such an easily
described pattern, especially when the categorical variables do not contain
natural ordering.</p>
<p>The mosaic plots are best made on the tables created by the <code>tally</code>
function from a table that just contains the counts (<strong>no totals</strong>):
</p>

<div class="sourceCode" id="cb403"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb403-1"><a href="chapter5.html#cb403-1"></a><span class="co"># Makes a mosaic plot where areas are related to the proportion of</span></span>
<span id="cb403-2"><a href="chapter5.html#cb403-2"></a><span class="co"># the total in the table</span></span>
<span id="cb403-3"><a href="chapter5.html#cb403-3"></a><span class="kw">mosaicplot</span>(electable) </span></code></pre></div>
<div class="figure"><span id="fig:Figure5-5"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-5-1.png" alt="Mosaic plot of the 2000 election data comparing party affiliation and voting results." width="480" />
<p class="caption">
Figure 5.5: Mosaic plot of the 2000 election data comparing party affiliation and voting results.
</p>
</div>
<p>In general, the results here are not too surprising as the respondents
became more heavily republican,
they voted for Bush and the same pattern occurs as you look at more democratic
respondents. As the voters leaned towards being independent, the proportion
voting for “Other” increased. So it certainly seems that there is some sort of
relationship between party affiliation and voting status. As always, it is good
to compare the observed results to what we would expect if the null hypothesis
is true. Figure <a href="chapter5.html#fig:Figure5-6">5.6</a> assumes that the null
hypothesis is true and shows the variation
in the proportions in each category in the columns and variation in the
proportions across the rows, but displays no relationship between
<code>PARTY</code> and <code>VOTEF</code>. Essentially, the pattern down a
column is the same for all
the columns or vice-versa for the rows. The way to think of “no relationship”
here would involve considering whether knowing the party level could help you
predict the voting response and that is not the case in
Figure <a href="chapter5.html#fig:Figure5-6">5.6</a> but was in certain places in
Figure <a href="chapter5.html#fig:Figure5-5">5.5</a>.</p>

<div class="figure"><span id="fig:Figure5-6"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-6-1.png" alt="Mosaic plot of what the 2000 election data would look like if the null hypothesis of no relationship were true." width="480" />
<p class="caption">
Figure 5.6: Mosaic plot of what the 2000 election data would look like if the null hypothesis of no relationship were true.
</p>
</div>
</div>
<div id="section5-4" class="section level2">
<h2><span class="header-section-number">5.4</span> Models for R by C tables</h2>
<p>This section is very short in this chapter because we really do not use any
“models” in this Chapter. There are some complicated
statistical models that can be employed in these situations, but they are
beyond the scope of this book. What we do have in this situation is our
original data summary in the form of a contingency table, graphs of the results
like those seen above, a hypothesis test and p-value (presented below), and
some post-test plots that we can use to understand the “source” of any evidence
we found in the test.</p>
</div>
<div id="section5-5" class="section level2">
<h2><span class="header-section-number">5.5</span> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</h2>
<p>In order to assess the evidence against our null hypotheses of
no difference in distributions or no
relationship between the variables, we need to define a test
statistic and find
its distribution under the null hypothesis. The test statistic
used with both
types of tests is called the <span class="math inline">\(\mathbf{X^2}\)</span> <strong><em>statistic</em></strong>
(we want to call the statistic X-square not Chi-square). The statistic
compares the
observed counts in the contingency table to the <strong><em>expected counts</em></strong>
under the null hypothesis, with large differences between what
we observed and what we
expect under the null leading to evidence against the null hypothesis. To help
this statistic to follow a named parametric distribution and provide some
insights into sources of interesting differences from the null hypothesis, we <strong><em>standardize</em></strong><a href="#fn88" class="footnote-ref" id="fnref88"><sup>88</sup></a>
the difference between the observed and expected counts by the square-root of
the expected count.
 
The <span class="math inline">\(\mathbf{X^2}\)</span> <strong><em>statistic</em></strong> is based on
the sum of squared standardized differences,</p>
<p><span class="math display">\[\boldsymbol{X^2 = \Sigma^{RC}_{i=1}\left(\frac{Observed_i-Expected_i}
{\sqrt{Expected_i}}\right)^2},\]</span></p>
<p>which is the sum over all (<span class="math inline">\(R\)</span> times <span class="math inline">\(C\)</span>) cells in the contingency
table of the square of the difference between observed and expected
cell counts divided by the square root of the
expected cell count. To calculate this test statistic, it useful to start with
a table of expected cell counts to go with our contingency table of observed
counts.  The expected cell counts are easiest to understand in the
homogeneity situation but are calculated the same in either scenario.</p>
<p>The idea underlying finding the <strong><em>expected cell counts</em></strong> is to find
how many observations we would expect in category <span class="math inline">\(c\)</span> given the sample
size in that group, <span class="math inline">\(\mathbf{n_{r\bullet}}\)</span>, if the null hypothesis is true.
Under the null hypothesis across all <span class="math inline">\(R\)</span> groups
the conditional probabilities in each response category must be the
same. Consider Figure <a href="chapter5.html#fig:Figure5-7">5.7</a> where, under the null
hypothesis, the probability of <em>None</em>, <em>Some</em>, and <em>Marked</em> are the same
in both treatment groups. Specifically we have <span class="math inline">\(\text{Pr}(None)=0.5\)</span>,
<span class="math inline">\(\text{Pr}(Some)=0.167\)</span>, and <span class="math inline">\(\text{Pr}(Marked)=0.333\)</span>. With
<span class="math inline">\(\mathbf{n_{Placebo\bullet}}=43\)</span> and <span class="math inline">\(\text{Pr}(None)=0.50\)</span>, we would
expect <span class="math inline">\(43*0.50=21.5\)</span> subjects to be found in the <em>Placebo, None</em>
combination if the null hypothesis were true. Similarly, with
<span class="math inline">\(\text{Pr}(Some)=0.167\)</span>, we would expect <span class="math inline">\(43*0.167=7.18\)</span> in the
<em>Placebo, Some</em> cell. And for the <em>Treated</em> group with
<span class="math inline">\(\mathbf{n_{Treated\bullet}}=41\)</span>, the expected count in the <em>Marked</em>
improvement group would be <span class="math inline">\(41*0.333=13.65\)</span>. Those conditional
probabilities came from aggregating across the rows because, under the null,
the row (<em>Treatment</em>) should not matter. So, the conditional
probability was actually calculated as <span class="math inline">\(\mathbf{n_{\bullet c}/N}\)</span> =
total number of responses in category <span class="math inline">\(c\)</span> divided by the table total. Since each
expected cell count was a conditional probability times the number of
observations in the row, we can re-write the expected cell count formula for
row <span class="math inline">\(r\)</span> and column <span class="math inline">\(c\)</span> as:</p>
<p><span class="math display">\[\mathbf{Expected\ cell\ count_{rc} = \frac{(n_{r\bullet}*n_{\bullet c})}{N}} = 
\frac{(\text{row } r \text{ total }*\text{ column } c \text{ total})}
{\text{table total}}.\]</span></p>
<p>Table <a href="chapter5.html#tab:Table5-3">5.3</a> demonstrates the calculations of the
expected cell counts using this formula for all 6 cells in the <span class="math inline">\(2\times 3\)</span>
table.</p>

<div class="figure"><span id="fig:Figure5-7"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-7-1.png" alt="Stacked bar chart that could occur if the null hypothesis were true for the Arthritis study." width="480" />
<p class="caption">
Figure 5.7: Stacked bar chart that could occur if the null hypothesis were true for the Arthritis study.
</p>
</div>

<table>
<caption><span id="tab:Table5-3">Table 5.3: </span> Demonstration of calculation of expected cell counts for Arthritis data.</caption>
<colgroup>
<col width="8%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">    </th>
<th align="left"><strong>None</strong></th>
<th align="left"><strong>Some</strong></th>
<th align="left"><strong>Marked</strong></th>
<th align="left"><strong>Totals</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Placebo</strong></td>
<td align="left"><span class="math inline">\(\boldsymbol{\dfrac{n_{\text{Placebo}\bullet}*n_{\bullet\text{None}}}{N}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\dfrac{43*42}{84}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\color{red}{\mathbf{21.5}}}\)</span></td>
<td align="left"><span class="math inline">\(\boldsymbol{\dfrac{n_{\text{Placebo}\bullet}*n_{\bullet\text{Some}}}{N}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\dfrac{43*14}{84}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\color{red}{\mathbf{7.167}}}\)</span></td>
<td align="left"><span class="math inline">\(\boldsymbol{\dfrac{n_{\text{Placebo}\bullet}*n_{\bullet\text{Marked}}}{N}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\dfrac{43*28}{84}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\color{red}{\mathbf{14.33}}}\)</span><br />
</td>
<td align="left"><span class="math inline">\(\boldsymbol{n_{\text{Placebo}\bullet}=43}\)</span></td>
</tr>
<tr class="even">
<td align="left"><strong>Treated</strong></td>
<td align="left"><span class="math inline">\(\boldsymbol{\dfrac{n_{\text{Treated}\bullet}*n_{\bullet\text{None}}}{N}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\dfrac{41*42}{84}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\color{red}{\mathbf{20.5}}}\)</span></td>
<td align="left"><span class="math inline">\(\boldsymbol{\dfrac{n_{\text{Treated}\bullet}*n_{\bullet\text{Some}}}{N}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\dfrac{41*14}{84}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\color{red}{\mathbf{6.83}}}\)</span></td>
<td align="left"><span class="math inline">\(\boldsymbol{\dfrac{n_{\text{Treated}\bullet}*n_{\bullet\text{Marked}}}{N}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\dfrac{41*28}{84}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\color{red}{\mathbf{13.67}}}\)</span><br />
</td>
<td align="left"><span class="math inline">\(\boldsymbol{n_{\text{Treated}\bullet}=41}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><strong>Totals</strong></td>
<td align="left"><span class="math inline">\(\boldsymbol{n_{\bullet\text{None}}=42}\)</span></td>
<td align="left"><span class="math inline">\(\boldsymbol{n_{\bullet\text{Some}}=14}\)</span></td>
<td align="left"><span class="math inline">\(\boldsymbol{n_{\bullet\text{Marked}}=28}\)</span></td>
<td align="left"><span class="math inline">\(\boldsymbol{N=84}\)</span></td>
</tr>
</tbody>
</table>

<p>Of course, using R can help us avoid tedium like this… The main
engine for results in this chapter is the <code>chisq.test</code>
function.  It operates on a table of
counts that has been produced <strong>without row or column totals</strong>.</p>
<!-- \newpage -->
<p>For example, <code>Arthtable</code> below contains just the observed cell
counts. Applying the <code>chisq.test</code> function<a href="#fn89" class="footnote-ref" id="fnref89"><sup>89</sup></a> to <code>Arthtable</code>
provides a variety of useful output. For the moment, we are just
going to extract the information in the “<code>expected</code>” attribute of
the results from running this function (using <code>chisq.test(TABLENAME)$expected)</code>.
These are the expected cell counts which match the previous calculations
except for some rounding in the hand-calculations.</p>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb404-1"><a href="chapter5.html#cb404-1"></a>Arthtable &lt;-<span class="st"> </span><span class="kw">tally</span>(<span class="op">~</span>Treatment<span class="op">+</span>Improved, <span class="dt">data=</span>Arthritis)</span>
<span id="cb404-2"><a href="chapter5.html#cb404-2"></a>Arthtable</span></code></pre></div>
<pre><code>##          Improved
## Treatment None Some Marked
##   Placebo   29    7      7
##   Treated   13    7     21</code></pre>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb406-1"><a href="chapter5.html#cb406-1"></a><span class="kw">chisq.test</span>(Arthtable)<span class="op">$</span>expected</span></code></pre></div>
<pre><code>##          Improved
## Treatment None     Some   Marked
##   Placebo 21.5 7.166667 14.33333
##   Treated 20.5 6.833333 13.66667</code></pre>
<p>With the observed and expected cell counts in hand, we can turn our
attention to calculating the test
statistic. It is possible to lay out the “contributions” to the
<span class="math inline">\(X^2\)</span> statistic in a table format, allowing a simple way to finally
calculate the statistic without losing any information. For <strong>each cell</strong>
we need to find</p>
<p><span class="math display">\[(\text{observed}-\text{expected})/\sqrt{\text{expected}}),\]</span></p>
<p><strong>square them</strong>, and then we need to add them <strong>all</strong> up. In the
current example, there are 6 cells to add up (<span class="math inline">\(R=2\)</span> times <span class="math inline">\(C=3\)</span>), shown
in Table <a href="chapter5.html#tab:Table5-4">5.4</a>.</p>

<table>
<caption><span id="tab:Table5-4">Table 5.4: </span> <span class="math inline">\(X^2\)</span> contributions for the Arthritis data.</caption>
<colgroup>
<col width="10%" />
<col width="29%" />
<col width="29%" />
<col width="30%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">    </th>
<th align="left"><strong>None</strong></th>
<th align="left"><strong>Some</strong></th>
<th align="left"><strong>Marked</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Placebo</strong></td>
<td align="left"><span class="math inline">\(\left(\frac{29-21.5}{\sqrt{21.5}}\right)^2=\color{red}{\mathbf{2.616}}\)</span></td>
<td align="left"><span class="math inline">\(\left(\frac{7-7.167}{\sqrt{7.167}}\right)^2=\color{red}{\mathbf{0.004}}\)</span></td>
<td align="left"><span class="math inline">\(\left(\frac{7-14.33}{\sqrt{14.33}}\right)^2=\color{red}{\mathbf{3.752}}\)</span></td>
</tr>
<tr class="even">
<td align="left"><strong>Treated</strong></td>
<td align="left"><span class="math inline">\(\left(\frac{13-20.5}{\sqrt{20.5}}\right)^2=\color{red}{\mathbf{2.744}}\)</span></td>
<td align="left"><span class="math inline">\(\left(\frac{7-6.833}{\sqrt{6.833}}\right)^2=\color{red}{\mathbf{0.004}}\)</span></td>
<td align="left"><span class="math inline">\(\left(\frac{21-13.67}{\sqrt{13.67}}\right)^2=\color{red}{\mathbf{3.935}}\)</span></td>
</tr>
</tbody>
</table>
<p>Finally, the <span class="math inline">\(X^2\)</span> statistic here is the sum of these six results
<span class="math inline">\(={\color{red}{2.616+0.004+3.752+2.744+0.004+3.935}}=13.055\)</span></p>
<p>Our favorite function in this chapter, <code>chisq.test</code>, does not provide
the contributions to the <span class="math inline">\(X^2\)</span> statistic directly. It provides a related
quantity called the </p>
<p><span class="math display">\[\textbf{standardized residual}=\left(\frac{\text{Observed}_i -
\text{Expected}_i}{\sqrt{\text{Expected}_i}}\right),\]</span></p>
<p>which, when squared (in R, squaring is accomplished using <code>^2</code>),
is the contribution of that particular cell to the <span class="math inline">\(X^2\)</span>
statistic that is displayed in Table <a href="chapter5.html#tab:Table5-4">5.4</a>.</p>
<!-- \newpage -->
<div class="sourceCode" id="cb408"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb408-1"><a href="chapter5.html#cb408-1"></a>(<span class="kw">chisq.test</span>(Arthtable)<span class="op">$</span>residuals)<span class="op">^</span><span class="dv">2</span></span></code></pre></div>
<pre><code>##          Improved
## Treatment        None        Some      Marked
##   Placebo 2.616279070 0.003875969 3.751937984
##   Treated 2.743902439 0.004065041 3.934959350</code></pre>
<p>The most common error made in calculating the <span class="math inline">\(X^2\)</span> statistic by hand
involves having observed less than expected
and then failing to make the <span class="math inline">\(X^2\)</span> contribution positive for all cells
(remember you are <strong>squaring the entire quantity</strong> in the parentheses
and so the sign has to go positive!). In R, we can add up the cells using
the <code>sum</code> function over the entire table of numbers:</p>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb410-1"><a href="chapter5.html#cb410-1"></a><span class="kw">sum</span>((<span class="kw">chisq.test</span>(Arthtable)<span class="op">$</span>residuals)<span class="op">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 13.05502</code></pre>
<p>Or we can let R do all this hard work for us and get straight to the
good stuff:</p>
<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb412-1"><a href="chapter5.html#cb412-1"></a><span class="kw">chisq.test</span>(Arthtable)</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  Arthtable
## X-squared = 13.055, df = 2, p-value = 0.001463</code></pre>
<p>The <code>chisq.test</code>  function reports a p-value by
default. Before we discover how it got that result, we can rely on our
permutation methods to obtain a distribution for the <span class="math inline">\(X^2\)</span> statistic
under the null hypothesis. As in Chapters <a href="chapter2.html#chapter2">2</a> and <a href="chapter3.html#chapter3">3</a>,
this will allow us to find a
p-value while relaxing one of our assumptions<a href="#fn90" class="footnote-ref" id="fnref90"><sup>90</sup></a>.
In the One-WAY ANOVA in Chapter <a href="chapter3.html#chapter3">3</a>, we permuted the
grouping variable relative
to the responses, mimicking the null hypothesis that the groups are the same
and so we can shuffle them around if the null is true. That same technique is
useful here. If we randomly permute the grouping variable used to form the rows
in the contingency table relative to the responses in the other variable and
track the possibilities available for the <span class="math inline">\(X^2\)</span> statistic under
permutations, we can find the probability of getting a result as extreme as or
more extreme than what we observed assuming the null is true, our p-value.

The observed statistic is the
<span class="math inline">\(X^2\)</span> calculated using the formula above.

Like the <span class="math inline">\(F\)</span>-statistic, it ends up
that only results in the right tail of this distribution are desirable for
finding evidence against the null hypothesis
because all the values showing deviation from the null in any direction going into the statistic have to be positive. You can see this by observing that
values of the <span class="math inline">\(X^2\)</span> statistic close to 0 are generated when the
observed values are close to the expected values and that sort of result should
not be used to find evidence against the null. When the observed and expected
values are “far apart”, then we should find evidence against the null. It is
helpful to work through some examples to be able to understand how the <span class="math inline">\(X^2\)</span>
statistic “measures” differences between observed and expected.</p>
<p>To start, compare the previous observed <span class="math inline">\(X^2\)</span> of 13.055 to the sort of
results we obtain in a single permutation of the treated/placebo labels
– Figure <a href="chapter5.html#fig:Figure5-8">5.8</a> (top left panel) shows
a permuted data set that produced <span class="math inline">\(X^{2*} = 0.62\)</span>. Visually, you can only
see minimal differences between the treatment and placebo groups showing up in
the stacked bar chart. Three other permuted data sets are displayed in
Figure <a href="chapter5.html#fig:Figure5-8">5.8</a> showing the variability in results in
permutations but that none get close
to showing the differences in the bars observed in the real data set in Figure <a href="chapter5.html#fig:Figure5-2">5.2</a>.</p>
<div class="sourceCode" id="cb414"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb414-1"><a href="chapter5.html#cb414-1"></a>Arthperm &lt;-<span class="st"> </span>Arthritis</span>
<span id="cb414-2"><a href="chapter5.html#cb414-2"></a>Arthperm<span class="op">$</span>PermTreatment &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">shuffle</span>(Arthperm<span class="op">$</span>Treatment))</span></code></pre></div>
<div class="sourceCode" id="cb415"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb415-1"><a href="chapter5.html#cb415-1"></a><span class="kw">plot</span>(Improved<span class="op">~</span>PermTreatment, <span class="dt">data=</span>Arthperm,</span>
<span id="cb415-2"><a href="chapter5.html#cb415-2"></a>     <span class="dt">main=</span><span class="st">&quot;Stacked Bar Chart of Permuted Arthritis Data&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb416"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb416-1"><a href="chapter5.html#cb416-1"></a>Arthpermtable &lt;-<span class="st"> </span><span class="kw">tally</span>(<span class="op">~</span>PermTreatment<span class="op">+</span>Improved, <span class="dt">data=</span>Arthperm)</span>
<span id="cb416-2"><a href="chapter5.html#cb416-2"></a>Arthpermtable</span></code></pre></div>
<pre><code>##              Improved
## PermTreatment None Some Marked
##       Placebo   22    6     15
##       Treated   20    8     13</code></pre>
<div class="sourceCode" id="cb418"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb418-1"><a href="chapter5.html#cb418-1"></a><span class="kw">chisq.test</span>(Arthpermtable)</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  Arthpermtable
## X-squared = 0.47646, df = 2, p-value = 0.788</code></pre>

<div class="figure"><span id="fig:Figure5-8"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-8-1.png" alt="Stacked bar charts of four permuted Arthritis data sets that produced \(X^2\) between 0.62 and 2.38." width="672" />
<p class="caption">
Figure 5.8: Stacked bar charts of four permuted Arthritis data sets that produced <span class="math inline">\(X^2\)</span> between 0.62 and 2.38.
</p>
</div>
<p>To build the permutation-based null distribution for the <span class="math inline">\(X^2\)</span> statistic,
we need to collect up the test statistics (<span class="math inline">\(X^{2*}\)</span>) in many of these permuted
results. The code is similar to permutation tests in Chapters
<a href="chapter2.html#chapter2">2</a> and <a href="chapter3.html#chapter3">3</a> except
that each permutation generates a new contingency table that is summarized and
provided to <code>chisq.test</code> to analyze. We extract the
<code>$statistic</code> attribute of the results from running <code>chisq.test</code>.
</p>

<div class="sourceCode" id="cb420"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb420-1"><a href="chapter5.html#cb420-1"></a>Tobs &lt;-<span class="st"> </span><span class="kw">chisq.test</span>(Arthtable)<span class="op">$</span>statistic; Tobs</span></code></pre></div>
<pre><code>## X-squared 
##  13.05502</code></pre>
<div class="sourceCode" id="cb422"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb422-1"><a href="chapter5.html#cb422-1"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb422-2"><a href="chapter5.html#cb422-2"></a>B &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb422-3"><a href="chapter5.html#cb422-3"></a>Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</span>
<span id="cb422-4"><a href="chapter5.html#cb422-4"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</span>
<span id="cb422-5"><a href="chapter5.html#cb422-5"></a>  Tstar[b] &lt;-<span class="st"> </span><span class="kw">chisq.test</span>(<span class="kw">tally</span>(<span class="op">~</span><span class="kw">shuffle</span>(Treatment)<span class="op">+</span>Improved,</span>
<span id="cb422-6"><a href="chapter5.html#cb422-6"></a>                               <span class="dt">data=</span>Arthritis))<span class="op">$</span>statistic</span>
<span id="cb422-7"><a href="chapter5.html#cb422-7"></a>}</span>
<span id="cb422-8"><a href="chapter5.html#cb422-8"></a><span class="kw">pdata</span>(Tstar, Tobs, <span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>## [1] 0.002</code></pre>
<div class="sourceCode" id="cb424"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb424-1"><a href="chapter5.html#cb424-1"></a><span class="kw">hist</span>(Tstar, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,Tobs<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb424-2"><a href="chapter5.html#cb424-2"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb424-3"><a href="chapter5.html#cb424-3"></a><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>,</span>
<span id="cb424-4"><a href="chapter5.html#cb424-4"></a>     <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,Tobs<span class="op">+</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="dv">2</span>)</span>
<span id="cb424-5"><a href="chapter5.html#cb424-5"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure5-9"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-9-1.png" alt="Permutation distribution for the \(X^2\) statistic for the Arthritis data with an observed \(X^2\) of 13.1 (bold, vertical line)." width="480" />
<p class="caption">
Figure 5.9: Permutation distribution for the <span class="math inline">\(X^2\)</span> statistic for the Arthritis data with an observed <span class="math inline">\(X^2\)</span> of 13.1 (bold, vertical line).
</p>
</div>
<p>For an observed <span class="math inline">\(X^2\)</span> statistic of 13.055, two out of 1,000 permutation
results matched or exceeded this value (<code>pdata</code> returned a value of
0.002) as displayed in Figure <a href="chapter5.html#fig:Figure5-9">5.9</a>.

This suggests that our observed result is quite extreme
relative to the null
hypothesis and provides strong evidence against it.
</p>
<!-- \newpage -->
<p><strong>Validity conditions</strong> for a permutation <span class="math inline">\(X^2\)</span> test are:
</p>
<ol style="list-style-type: decimal">
<li><p>Independence of observations.
</p></li>
<li><p>Both variables are categorical.</p></li>
<li><p>Expected cell counts &gt; 0 (otherwise <span class="math inline">\(X^2\)</span> is not defined).</p></li>
</ol>
<p>For the permutation approach described here to provide valid inferences
we need to be working with
observations that are independent of one another. One way that a violation of
independence can sometimes occur in this situation is when a single subject
shows up in the table more than once. For example, if a single individual
completes a survey more than once and those results are reported as if they
came from <span class="math inline">\(N\)</span> independent individuals.
Be careful about this as it is really easy to make tables of poorly collected
or non-independent observations and then consider them for these analyses. Poor
data still lead to poor conclusions even if you have fancy new statistical
tools to use!</p>
</div>
<div id="section5-6" class="section level2">
<h2><span class="header-section-number">5.6</span> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</h2>
<p>When one additional assumption beyond the previous assumptions for
the permutation
test is met, it is possible to avoid permutations to find the distribution of
the <span class="math inline">\(X^2\)</span> statistic under the null hypothesis and get a p-value using
what is called the <strong><em>Chi-square or</em></strong> <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-<strong>distribution</strong>.

The name of our test statistic, X-squared, is meant to allude to the
potential that this will follow a <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution
in certain situations but may not do that all the time and we still
can use the methods in Section <a href="chapter5.html#section5-5">5.5</a>. Along with the
previous assumption
regarding independence and all expected cell counts are greater than 0, we make a
requirement that <strong><em>N</em></strong> (the total sample size) is “large enough” and
this assumption is written in terms of the expected cell counts.
If <strong><em>N</em></strong> is large, then all the expected cell counts should also be
large because all those observations have
to go somewhere. The problems for the <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution
as an approximation to the distribution
of the <span class="math inline">\(X^2\)</span> statistic under the null hypothesis come when expected
cell counts are below 5. And the smaller the expected cell counts become, the
more problematic the <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution is as an
approximation of the sampling distribution of the <span class="math inline">\(X^2\)</span> statistic under
the null hypothesis. <strong>The standard rule of thumb is that all the expected
cell counts need to exceed 5 for the parametric approach to be valid</strong>.

When
this condition is violated, it is better to use the permutation approach.
The <code>chisq.test</code> function will provide a warning message
to help you notice this. But it is good practice to always explore
the expected cell counts using <code>chisq.test(...)$expected</code>.</p>
<div class="sourceCode" id="cb425"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb425-1"><a href="chapter5.html#cb425-1"></a><span class="kw">chisq.test</span>(Arthtable)<span class="op">$</span>expected</span></code></pre></div>
<pre><code>##          Improved
## Treatment None     Some   Marked
##   Placebo 21.5 7.166667 14.33333
##   Treated 20.5 6.833333 13.66667</code></pre>
<p>In the Arthritis data set, the sample size was sufficiently large for
the <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution to provide an accurate p-value
since the smallest expected cell count is 6.833 (so all expected counts are larger than 5).</p>
<p>The <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution is a right-skewed distribution
that starts at 0 as shown in Figure <a href="chapter5.html#fig:Figure5-10">5.10</a>. Its shape
changes as a function of its degrees of freedom. In the contingency
table analyses, the <strong><em>degrees of freedom</em></strong> for the Chi-square test
are calculated as 
</p>
<p><span class="math display">\[\textbf{DF} \mathbf{=(R-1)*(C-1)} = (\text{number of rows }-1)*
(\text{number of columns }-1).\]</span></p>
<p>In the <span class="math inline">\(2 \times 3\)</span> table above, the <span class="math inline">\(\text{DF}=(2-1)*(3-1)=2\)</span> leading
to a Chi-square
distribution with 2 <em>df</em> for the distribution of <span class="math inline">\(X^2\)</span> under the null
hypothesis. The p-value is based on the area to the right of the observed
<span class="math inline">\(X^2\)</span> value of 13.055 and the <code>pchisq</code> function provides that area as
0.00146.


Note that this is very similar to the permutation result found
previously for these data.</p>
<div class="sourceCode" id="cb427"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb427-1"><a href="chapter5.html#cb427-1"></a><span class="kw">pchisq</span>(<span class="fl">13.055</span>, <span class="dt">df=</span><span class="dv">2</span>, <span class="dt">lower.tail=</span>F)</span></code></pre></div>
<pre><code>## [1] 0.001462658</code></pre>
<p>We’ll see more examples of the <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distributions
in each of the examples that follow.</p>

<div class="figure"><span id="fig:Figure5-10"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-10-1.png" alt="\(\boldsymbol{\chi^2}\)-distribution with two degrees of freedom with the observed statistic of 13.1 indicated with a vertical line." width="480" />
<p class="caption">
Figure 5.10: <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution with two degrees of freedom with the observed statistic of 13.1 indicated with a vertical line.
</p>
</div>
<p>A small side note about sample sizes is warranted here. In
contingency tables, especially those based
on survey data, it is common to have large overall sample sizes (<span class="math inline">\(N\)</span>).
With large sample sizes, it becomes easy to find strong evidence against the null hypothesis, even when the “distance” from the null
is relatively minor and possibly unimportant. By this we mean that the observed proportions are a small practical distance
from the situation described in the null. After obtaining a small p-value, we
need to consider whether we have obtained <strong><em>practical significance</em></strong>
(or maybe better described as <strong><em>practical importance</em></strong>) to
accompany our discussion
of strong evidence against the null hypothesis. Whether a result is large enough to
be of practical
importance can only be judged by knowing something about the situation we are
studying and by providing a good summary of our results to allow experts to
assess the size and importance of the result. Unfortunately, many
researchers are so happy to see small p-values that this is their last step. We encountered a similar situation in the car overtake distance data set where a large sample size provided a data set that had a small p-value and possibly minor differences in the means driving it.</p>
<p>If we revisit our observed results, re-plotted in Figure <a href="chapter5.html#fig:Figure5-11">5.11</a>
since Figure <a href="chapter5.html#fig:Figure5-2">5.2</a> is many pages
earlier, knowing that we have strong evidence against the null hypothesis
of no difference between <em>Placebo</em> and <em>Treated</em> groups, what can we
say about the effectiveness of the arthritis medication? It seems that there is a real and important increase in the proportion
of patients getting improvement (<em>Some</em> or <em>Marked</em>). If the differences
“looked” smaller, even with a small p-value you<a href="#fn91" class="footnote-ref" id="fnref91"><sup>91</sup></a> might not recommend someone take the drug…</p>

<div class="figure"><span id="fig:Figure5-11"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-11-1.png" alt="Stacked bar chart of the Arthritis data comparing Treated and Placebo." width="960" />
<p class="caption">
Figure 5.11: Stacked bar chart of the Arthritis data comparing <em>Treated</em> and <em>Placebo</em>.
</p>
</div>
</div>
<div id="section5-7" class="section level2">
<h2><span class="header-section-number">5.7</span> Examining residuals for the source of differences</h2>
<p>Small p-values are generated by large <span class="math inline">\(X^2\)</span> values. If we want to understand
the source of a small p-value, we need to understand what made the test
statistic large. To get a large <span class="math inline">\(X^2\)</span> value, we either need many small
contributions from lots of cells or a few large contributions. In most
situations, there are just a few cells that show large deviations between the
null hypothesis (expected cell counts) and what was observed (observed cell
counts). It is possible to explore the “size” and direction of the differences
between observed and expected counts to learn something about the behavior of
the relationship between the variables, especially as it relates to evidence
against the null hypothesis of no difference or no relationship. The <strong><em>standardized residual</em></strong>,</p>
<p><span class="math display">\[\boldsymbol{\left(\frac{\textbf{Observed}_i - 
\textbf{Expected}_i}{\sqrt{\textbf{Expected}_i}}\right)},\]</span></p>
<p>provides a measure of deviation of the observed from expected which
retains the <strong>direction of deviation</strong> (whether observed was
<strong>more or less than expected</strong> is interesting for interpretations) for
each cell in the table. It is scaled much like a standard normal distribution
providing a scale for “large” deviations for absolute values that are over 2 or
3. In other words, values with magnitude over 2 should be your focus in the
standardized residuals, noting whether the observed counts were much more or
less than expected. On the <span class="math inline">\(X^2\)</span> scale, standardized residuals of 2 or
more mean that the cells are contributing 4 or more units to the overall
statistic, which is a pretty noticeable bump up in the size of the statistic. A
few contributions at 4 or higher and you will likely end up with a small
p-value.
</p>
<p>There are two ways to explore standardized residuals. First, we can
obtain them via the <code>chisq.test</code> and manually identify the “big
ones”. Second, we can augment a mosaic plot of the table with the
standardized results by turning on the <code>shade=T</code> option and have
the plot help us find the big differences. This technique can be
applied whether we are performing an Independence or
Homogeneity test – both are evaluated with the same <span class="math inline">\(X^2\)</span> statistic so
the large standardized residuals are of interest in both situations. Both types
of results are shown for the Arthritis data table:</p>

<pre><code>##          Improved
## Treatment        None        Some      Marked
##   Placebo  1.61749160 -0.06225728 -1.93699199
##   Treated -1.65647289  0.06375767  1.98367320</code></pre>
<div class="figure"><span id="fig:Figure5-12"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-12-1.png" alt="Mosaic plot of the Arthritis data with large standardized residuals indicated (actually, there were none that were indicated because all were less than 2). Note that dashed borders correspond to negative standardized residuals (observed less than expected) and solid borders are positive standardized residuals (observed more than expected)." width="960" />
<p class="caption">
Figure 5.12: Mosaic plot of the Arthritis data with large standardized residuals indicated (actually, there were none that were indicated because all were less than 2). Note that dashed borders correspond to negative standardized residuals (observed less than expected) and solid borders are positive standardized residuals (observed more than expected).
</p>
</div>
<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb430-1"><a href="chapter5.html#cb430-1"></a><span class="kw">chisq.test</span>(Arthtable)<span class="op">$</span>residuals</span>
<span id="cb430-2"><a href="chapter5.html#cb430-2"></a><span class="kw">mosaicplot</span>(Arthtable, <span class="dt">shade=</span>T)</span></code></pre></div>
<p>In these data, the standardized residuals are all less than 2 in
magnitude so Figure <a href="chapter5.html#fig:Figure5-12">5.12</a> isn’t
too helpful but this type of plot is in other examples. The largest
contributions to the <span class="math inline">\(X^2\)</span> statistic come from the <em>Placebo</em> and <em>Treated</em>
groups in the <em>Marked</em> improvement cells. Those standardized residuals
are -1.94 and 1.98 (both really close to 2), showing that the <em>placebo</em>
group had <strong>noticeably fewer</strong> <em>Marked</em> improvement
<strong>results than expected</strong> and the <em>Treated</em> group <strong>had noticeably more</strong>
<em>Marked</em> improvement responses <strong>than expected if the null hypothesis was true</strong>. Similarly but with smaller magnitudes, there were more <em>None</em>
results than expected in the <em>Placebo</em> group and fewer <em>None</em> results
than expected in the <em>Treated</em> group. The standardized residuals were
very small in the two cells for the <em>Some</em> improvement category, showing
that the treatment/placebo were similar in this response category and
that the results
were about what would be expected if the null hypothesis of no difference
were true.</p>
</div>
<div id="section5-8" class="section level2">
<h2><span class="header-section-number">5.8</span> General protocol for <span class="math inline">\(X^2\)</span> tests</h2>
<p>In any contingency table situation, there is a general protocol to
completing an analysis.</p>
<ol style="list-style-type: decimal">
<li><p>Identify the data collection method and whether the proper analysis
is based on the Independence or Homogeneity hypotheses
(Section <a href="chapter5.html#section5-1">5.1</a>).</p></li>
<li><p>Make contingency table and get a general sense of response patterns.
Pay attention to “small” counts, especially cells with 0 counts.</p>
<ol style="list-style-type: lower-alpha">
<li>If there are many small count cells, consider combining categories
on one or both variables to make a new variable with fewer categories
that has larger counts per cell to have more robust inferences (see
Section <a href="chapter5.html#section5-10">5.10</a> for a related example).</li>
</ol></li>
<li><p>Make the appropriate graphical display of results and generally
describe the pattern of responses.</p>
<ol style="list-style-type: lower-alpha">
<li><p>For Homogeneity, make a stacked bar chart.</p></li>
<li><p>For Independence, make a mosaic plot.</p></li>
<li><p>Consider a more general exploration using a tableplot if other
variables were measured to check for confounding and other interesting multi-variable relationships. Also check for missing data if you have not done this before.</p></li>
</ol></li>
<li><p>Conduct the 6+ steps of the appropriate type of hypothesis test.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Use permutations if any expected cell counts are below 5.</p></li>
<li><p>If all expected cell counts greater than 5, either permutation or parametric
approaches are acceptable.</p></li>
</ol></li>
<li><p>Explore the standardized residuals for the “source” of any evidence
against the null – this can be the start of your “size” discussion.</p>
<ol style="list-style-type: lower-alpha">
<li>Tie the interpretation of the “large” standardized residuals
and their direction (above or below expected under the
null) back into the original data display (this really gets to “size”). Work to find a story
for the pattern of responses. If little evidence is found against the null, there is not much to do here.</li>
</ol></li>
</ol>
</div>
<div id="section5-9" class="section level2">
<h2><span class="header-section-number">5.9</span> Political party and voting results: Complete analysis</h2>
<p>As introduced in Section <a href="chapter5.html#section5-3">5.3</a>, a national random sample
of voters was obtained
related to the 2000 Presidential Election with the party affiliations and
voting results recorded for each subject. The data are available in
<code>election</code> in the <code>poLCA</code> package <span class="citation">(Linzer and Lewis. <a href="#ref-R-poLCA" role="doc-biblioref">2014</a>)</span>. It
is always good to start with a bit of data exploration with a tableplot,
displayed in Figure <a href="chapter5.html#fig:Figure5-13">5.13</a>.

Many of the lines of code here
are just for making
sure that R is treating the categorical variables that were coded numerically
as categorical variables.</p>

<div class="sourceCode" id="cb431"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb431-1"><a href="chapter5.html#cb431-1"></a>election<span class="op">$</span>VOTEF &lt;-<span class="st"> </span><span class="kw">factor</span>(election<span class="op">$</span>VOTE3)</span>
<span id="cb431-2"><a href="chapter5.html#cb431-2"></a>election<span class="op">$</span>PARTY &lt;-<span class="st"> </span><span class="kw">factor</span>(election<span class="op">$</span>PARTY)</span>
<span id="cb431-3"><a href="chapter5.html#cb431-3"></a>election<span class="op">$</span>EDUC &lt;-<span class="st"> </span><span class="kw">factor</span>(election<span class="op">$</span>EDUC)</span>
<span id="cb431-4"><a href="chapter5.html#cb431-4"></a>election<span class="op">$</span>GENDER &lt;-<span class="st"> </span><span class="kw">factor</span>(election<span class="op">$</span>GENDER)</span>
<span id="cb431-5"><a href="chapter5.html#cb431-5"></a><span class="kw">levels</span>(election<span class="op">$</span>VOTEF) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Gore&quot;</span>,<span class="st">&quot;Bush&quot;</span>,<span class="st">&quot;Other&quot;</span>)</span>
<span id="cb431-6"><a href="chapter5.html#cb431-6"></a><span class="co"># Required options to avoid error when running on a PC, </span></span>
<span id="cb431-7"><a href="chapter5.html#cb431-7"></a><span class="co"># should have no impact on other platforms</span></span>
<span id="cb431-8"><a href="chapter5.html#cb431-8"></a><span class="kw">options</span>(<span class="dt">ffbatchbytes =</span> <span class="dv">1024</span><span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="dv">128</span>); <span class="kw">options</span>(<span class="dt">ffmaxbytes =</span> <span class="dv">1024</span><span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="dv">128</span> <span class="op">*</span><span class="st"> </span><span class="dv">32</span>) </span>
<span id="cb431-9"><a href="chapter5.html#cb431-9"></a><span class="kw">tableplot</span>(election, <span class="dt">select=</span><span class="kw">c</span>(VOTEF,PARTY,EDUC,GENDER),<span class="dt">pals=</span><span class="kw">list</span>(<span class="st">&quot;BrBG&quot;</span>))</span></code></pre></div>
<div class="figure"><span id="fig:Figure5-13"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-13-1.png" alt="Tableplot of vote, party affiliation, education, and gender from election survey data. Note that missing observations are present in all variables except for Gender. Education is coded from 1 to 7 with higher values related to higher educational attainment. Gender code 1 is for male and 2 is for female." width="960" />
<p class="caption">
Figure 5.13: Tableplot of vote, party affiliation, education, and gender from election survey data. Note that missing observations are present in all variables except for <code>Gender</code>. Education is coded from 1 to 7 with higher values related to higher educational attainment. <code>Gender</code> code 1 is for male and 2 is for female.
</p>
</div>
<p>In Figure <a href="chapter5.html#fig:Figure5-13">5.13</a>, we can see many missing <code>VOTEF</code>
responses but also some missingness in <code>PARTY</code> and <code>EDUC</code>
(<em>Education</em>) status. While we don’t know
too much about why people didn’t respond on the Vote question – they could have
been unwilling to answer it or may not have voted. It looks like those subjects
have more of the lower education level responses (more dark colors, especially level 2 of education) than in the responders to this question. There are many “middle” ratings
in the party affiliation responses for the missing <code>VOTEF</code> responses,
suggesting that independents were less likely to answer the question in the
survey for whatever reason. Even though this comes with concerns about who these results actually apply to (likely not the population that was sampled from), we want to focus on those that did respond in
<code>VOTEF</code>, so will again use <code>na.omit</code> to clean out any subjects with any
missing responses on these four variables and remake this plot
(Figure <a href="chapter5.html#fig:Figure5-14">5.14</a>). The code also adds the <code>sort</code> option to the
<code>tableplot</code> function call that provides an easy way to sort the data set
based on other variables.

It is interesting, for example, to sort the
responses by <em>Education</em> level and explore the differences in other variables.
These explorations are omitted here but easily available by changing the
sorting column from 1 to <code>sort=3</code> or <code>sort=EDUC</code>. Figure <a href="chapter5.html#fig:Figure5-14">5.14</a> shows us that there are clear differences
in party affiliation based on voting for <em>Bush</em>, <em>Gore</em>, or
<em>Other</em>. It is harder to see if there are differences in education level or gender
based on the voting status in this plot, but, as noted above, sorting on these
other variables can sometimes help to see other relationships between
variables.</p>

<div class="sourceCode" id="cb432"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb432-1"><a href="chapter5.html#cb432-1"></a>election2 &lt;-<span class="st"> </span><span class="kw">na.omit</span>(election[,<span class="kw">c</span>(<span class="st">&quot;VOTEF&quot;</span>,<span class="st">&quot;PARTY&quot;</span>,<span class="st">&quot;EDUC&quot;</span>,<span class="st">&quot;GENDER&quot;</span>)])</span>
<span id="cb432-2"><a href="chapter5.html#cb432-2"></a><span class="kw">tableplot</span>(election2, <span class="dt">select=</span><span class="kw">c</span>(VOTEF,PARTY,EDUC,GENDER), <span class="dt">sort=</span><span class="dv">1</span>, <span class="dt">pals=</span><span class="kw">list</span>(<span class="st">&quot;BrBG&quot;</span>))</span></code></pre></div>
<div class="figure"><span id="fig:Figure5-14"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-14-1.png" alt="Tableplot of election data with subjects without any missing responses (complete cases)." width="960" />
<p class="caption">
Figure 5.14: Tableplot of election data with subjects without any missing responses (complete cases).
</p>
</div>
<p>Focusing on the party affiliation and voting results, the appropriate
analysis is with
an Independence test because a single random sample was obtained from the population.
The total sample size for the complete responses was <span class="math inline">\(N=\)</span> 1,149
(out of the original 1,785 subjects). Because this is an Independence test,
the mosaic plot is the appropriate display of the results,
which was provided in Figure <a href="chapter5.html#fig:Figure5-5">5.5</a>.</p>
<div class="sourceCode" id="cb433"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb433-1"><a href="chapter5.html#cb433-1"></a>electable &lt;-<span class="st"> </span><span class="kw">tally</span>(<span class="op">~</span>PARTY<span class="op">+</span>VOTEF, <span class="dt">data=</span>election2)</span>
<span id="cb433-2"><a href="chapter5.html#cb433-2"></a>electable</span></code></pre></div>
<pre><code>##      VOTEF
## PARTY Gore Bush Other
##     1  238    6     2
##     2  151   18     1
##     3  113   31    13
##     4   37   36    11
##     5   21  124    12
##     6   20  121     2
##     7    3  188     1</code></pre>
<p>There is a potential for bias in some polls because of the methods used
to find and contact people. As
U.S. residents have transitioned from land-lines to cell phones, the early
adopting cell phone users were often excluded from political polling. These
policies are being reconsidered to adapt to the decline in residential phone
lines and most polling organizations now include cell phone numbers in their
list of potential respondents. This study may have some bias regarding who was
considered as part of the population of interest and who was actually found
that was willing to respond to their questions. We don’t have much information
here but biases arising from unobtainable members of populations are a potential
issue in many studies, especially when questions tend toward more sensitive
topics. We can make inferences here to people that were willing to respond to
the request to answer the survey but should be cautious in extending it to all
Americans or even voters in the year 2000. When we say “population” below, this
nuanced discussion is what we mean. Because the political party is not randomly
assigned to the subjects, we cannot make causal inferences for political
affiliation causing different voting patterns<a href="#fn92" class="footnote-ref" id="fnref92"><sup>92</sup></a>.</p>
<p>Here are our 6+ steps applied to this example:</p>
<ol start="0" style="list-style-type: decimal">
<li><p>The desired RQ is about assessing the relationship between part affiliation and vote choice, but this is constrained by the large rate of non-response in this data set. This is an Independence test and so the tableplot and mosaic plot are good visualizations to consider and the <span class="math inline">\(X^2\)</span>-statistic will be used.</p></li>
<li><p><strong>Hypotheses:</strong></p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: There is no relationship between the party affiliation
(7 levels) and voting results (<em>Bush</em>, <em>Gore</em>, <em>Other</em>) in the
population.</p></li>
<li><p><span class="math inline">\(H_A\)</span>: There is a relationship between the party affiliation
(7 levels) and voting results (<em>Bush</em>, <em>Gore</em>, <em>Other</em>) in the
population.</p></li>
</ul>
<p></p></li>
<li><p><strong>Plot the data and assess validity conditions:</strong>
</p>
<ul>
<li><p>Independence:</p>
<ul>
<li>There is no indication of an issue with this assumption since each subject is
measured only once in the table. No other information suggests
a potential issue since a random sample was taken from presumably
a large national population and we have no information that could suggest dependencies among observations.</li>
</ul></li>
<li><p>All expected cell counts larger than 5 to use the parametric
<span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution to find p-values:</p>
<ul>
<li>We need to generate a table of expected cell counts to be
able to check this condition:</li>
</ul>
<div class="sourceCode" id="cb435"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb435-1"><a href="chapter5.html#cb435-1"></a><span class="kw">chisq.test</span>(electable)<span class="op">$</span>expected</span></code></pre></div>
<pre><code>## Warning in chisq.test(electable): Chi-squared approximation may be incorrect</code></pre>
<pre><code>##      VOTEF
## PARTY      Gore      Bush    Other
##     1 124.81984 112.18799 8.992167
##     2  86.25762  77.52829 6.214099
##     3  79.66144  71.59965 5.738903
##     4  42.62141  38.30809 3.070496
##     5  79.66144  71.59965 5.738903
##     6  72.55788  65.21497 5.227154
##     7  97.42037  87.56136 7.018277</code></pre>
<ul>
<li><p>When we request the expected cell counts, R tries to help
us with a warning message if the expected cell counts might
be small, as in this situation.</p></li>
<li><p>There is one expected cell count below 5 for <code>Party</code> = 4
who voted <em>Other</em> with an expected cell count of 3.07, so
the condition is violated and the permutation approach
should be used to obtain more trustworthy p-values. The
conditions are met for performing a permutation test.</p></li>
</ul></li>
</ul></li>
<li><p><strong>Calculate the test statistic and p-value:</strong></p>
<ul>
<li>The test statistic is best calculated by the <code>chisq.test</code> function since there
are 21 cells and many potential places for a calculation error if
performed by hand.</li>
</ul>
<div class="sourceCode" id="cb438"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb438-1"><a href="chapter5.html#cb438-1"></a><span class="kw">chisq.test</span>(electable)</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  electable
## X-squared = 762.81, df = 12, p-value &lt; 2.2e-16</code></pre>
<ul>
<li><p>The observed <span class="math inline">\(X^2\)</span> statistic is 762.81.</p></li>
<li><p>The parametric p-value is &lt; 2.2e-16 from the R output which
would be reported as &lt; 0.0001. This was based on a
<span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution with <span class="math inline">\((7-1)*(3-1) = 12\)</span>
degrees of freedom displayed in Figure <a href="chapter5.html#fig:Figure5-15">5.15</a>.
Note that the observed test statistic of 762.81 was off the plot
to the right which reflects how little area is to the right of
that value in the distribution.</p></li>
</ul>
<div class="figure"><span id="fig:Figure5-15"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-15-1.png" alt="Plot of $\boldsymbol{\chi^2}$-distribution with 12 degrees of freedom." width="288" />
<p class="caption">
Figure 5.15: Plot of <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution with 12 degrees of freedom.
</p>
</div>
<ul>
<li>If you want to repeat this calculation directly you get a similarly
tiny value that R reports as 1.5e-155. Again, reporting less than
0.0001 is just fine.</li>
</ul>
<p></p>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb440-1"><a href="chapter5.html#cb440-1"></a><span class="kw">pchisq</span>(<span class="fl">762.81</span>, <span class="dt">df=</span><span class="dv">12</span>, <span class="dt">lower.tail=</span>F)</span></code></pre></div>
<pre><code>## [1] 1.553744e-155</code></pre>
<ul>
<li>But since the expected cell count condition is violated, we should
use permutations as implemented in the following code to provide a more trustworthy p-value:</li>
</ul>
<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb442-1"><a href="chapter5.html#cb442-1"></a>Tobs &lt;-<span class="st"> </span><span class="kw">chisq.test</span>(electable)<span class="op">$</span>statistic; Tobs</span></code></pre></div>
<pre><code>## X-squared 
##  762.8095</code></pre>
<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb444-1"><a href="chapter5.html#cb444-1"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb444-2"><a href="chapter5.html#cb444-2"></a>B &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb444-3"><a href="chapter5.html#cb444-3"></a>Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</span>
<span id="cb444-4"><a href="chapter5.html#cb444-4"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</span>
<span id="cb444-5"><a href="chapter5.html#cb444-5"></a>  Tstar[b] &lt;-<span class="st"> </span><span class="kw">chisq.test</span>(<span class="kw">tally</span>(<span class="op">~</span><span class="kw">shuffle</span>(PARTY)<span class="op">+</span>VOTEF, <span class="dt">data=</span>election2,</span>
<span id="cb444-6"><a href="chapter5.html#cb444-6"></a>                               <span class="dt">margins=</span>F))<span class="op">$</span>statistic</span>
<span id="cb444-7"><a href="chapter5.html#cb444-7"></a>}</span>
<span id="cb444-8"><a href="chapter5.html#cb444-8"></a><span class="kw">pdata</span>(Tstar, Tobs, <span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb446-1"><a href="chapter5.html#cb446-1"></a><span class="kw">hist</span>(Tstar)</span>
<span id="cb446-2"><a href="chapter5.html#cb446-2"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb446-3"><a href="chapter5.html#cb446-3"></a><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</span>
<span id="cb446-4"><a href="chapter5.html#cb446-4"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure5-16"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-16-1.png" alt="Permutation distribution of $X^2$ for the election data. Observed value of 763 not displayed." width="576" />
<p class="caption">
Figure 5.16: Permutation distribution of <span class="math inline">\(X^2\)</span> for the election data. Observed value of 763 not displayed.
</p>
</div>
<ul>
<li>The last results tells us that there were no permuted data sets
that produced larger <span class="math inline">\(X^2\text{&#39;s}\)</span> than the observed <span class="math inline">\(X^2\)</span> in 1,000
permutations, so we report that the <strong>p-value was less than 0.001</strong>
using the permutation approach. The permutation distribution in
Figure <a href="chapter5.html#fig:Figure5-16">5.16</a> contains no results over 40, so the
observed configuration was really far from the null
hypothesis of no relationship between party status and voting.</li>
</ul></li>
</ol>
<div style="page-break-after: always;"></div>
<ol start="4" style="list-style-type: decimal">
<li><p><strong>Conclusion:</strong></p>
<ul>
<li>There is strong evidence against the null hypothesis of no relationship between party affiliation and voting
results in the population (<span class="math inline">\(X^2\)</span>=762.81, p-value&lt;0.001), so we would conclude that there is a relationship between party affiliation and voting results.</li>
</ul></li>
<li><p><strong>Size:</strong></p>
<ul>
<li>We can add insight into the results by exploring the
standardized residuals. The numerical results are obtained using <code>chisq.test(electable)$residuals</code> and visually using <code>mosaicplot(electable, shade=T)</code> in Figure <a href="chapter5.html#fig:Figure5-17">5.17</a>. The standardized residuals show some clear sources of the differences from the results expected if there were no relationship present. The largest
contributions are found in the highest democrat category (<code>PARTY</code> = 1)
where the standardized residual for <em>Gore</em> is 10.13 and for <em>Bush</em>
is -10.03, showing much higher than expected (under <span class="math inline">\(H_0\)</span>) counts for Gore
voters and much lower than expected (under <span class="math inline">\(H_0\)</span>) for Bush.
Similar results in the opposite direction are found in the strong
republicans (<code>PARTY</code> = 7). Note how the brightest shade of blue in
Figure <a href="chapter5.html#fig:Figure5-17">5.17</a> shows up for much higher than expected
results and the brighter red for results in the other direction,
where observed counts were
much lower than expected. When there are many large standardized residuals, it
is OK to focus on the largest results but remember that some of the
intermediate deviations, or lack thereof, could also be interesting. For
example, the Gore voters from <code>PARTY</code> = 3 had a standardized residual
of 3.75 but the <code>PARTY</code> = 5 voters for Bush had a standardized residual
of 6.17. So maybe Gore didn’t have
as strong of support from his center-leaning supporters as Bush was able to
obtain from the same voters on the other side of the middle? Exploring the relative proportion of each vertical bar in the response categories is also interesting to see the proportions of each level of party affiliation and how they voted. A political
scientist would easily obtain many more (useful) theories based on this combination of results.</li>
</ul></li>
</ol>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb447"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb447-1"><a href="chapter5.html#cb447-1"></a><span class="kw">chisq.test</span>(electable)<span class="op">$</span>residuals <span class="co">#(Obs - expected)/sqrt(expected)</span></span></code></pre></div>
<pre><code>##      VOTEF
## PARTY        Gore        Bush       Other
##     1  10.1304439 -10.0254117  -2.3317373
##     2   6.9709179  -6.7607252  -2.0916557
##     3   3.7352759  -4.7980730   3.0310127
##     4  -0.8610559  -0.3729136   4.5252413
##     5  -6.5724708   6.1926811   2.6135809
##     6  -6.1701472   6.9078679  -1.4115200
##     7  -9.5662296  10.7335798  -2.2717310</code></pre>

<div class="figure"><span id="fig:Figure5-17"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-17-1.png" alt="Mosaic plot with shading based on standardized residuals for the election data." width="480" />
<p class="caption">
Figure 5.17: Mosaic plot with shading based on standardized residuals for the election data.
</p>
</div>
<div class="sourceCode" id="cb449"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb449-1"><a href="chapter5.html#cb449-1"></a><span class="co">#Adds information on the size of the residuals</span></span>
<span id="cb449-2"><a href="chapter5.html#cb449-2"></a><span class="kw">mosaicplot</span>(electable, <span class="dt">shade=</span>T) </span></code></pre></div>
<ol start="6" style="list-style-type: decimal">
<li><p><strong>Scope of inference:</strong></p>
<ul>
<li>The results are not causal since no random assignment was present but they do apply to the population
of voters in the 2000 election that were able to be contacted by those
running the poll and who would be willing to answer all the questions and actually voted.</li>
</ul></li>
</ol>
</div>
<div id="section5-10" class="section level2">
<h2><span class="header-section-number">5.10</span> Is cheating and lying related in students?</h2>
<p>A study of student behavior was performed at a university with a survey of
<span class="math inline">\(N=319\)</span> undergraduate students (<code>cheating</code> data set from the <code>poLCA</code>
package originally published by <span class="citation">Dayton (<a href="#ref-Dayton1998" role="doc-biblioref">1998</a>)</span>).

They were asked to answer
four questions about their various
academic frauds that involved cheating and lying. Specifically, they were
asked if they had ever lied to avoid taking an exam (<code>LIEEXAM</code> with 1
for no and 2 for yes), if they had lied to avoid handing in a term paper
on time (<code>LIEPAPER</code> with 2 for yes), if they had purchased a term paper
to hand in as their own or obtained a copy of an exam prior to taking the
exam (<code>FRAUD</code> with 2 for yes), and if they had copied answers during an
exam from someone near them (<code>COPYEXAM</code> with 2 for yes). Additionally,
their <code>GPA</code>s were obtained and put into categories: (&lt;2.99, 3.0 to 3.25,
3.26 to 3.50, 3.51 to 3.75, and 3.76 to 4.0). These categories were coded
from 1 to 5, respectively. Again, the code starts with making sure the
variables are treated categorically by applying the <code>factor</code> function.
</p>

<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb450-1"><a href="chapter5.html#cb450-1"></a><span class="kw">library</span>(poLCA)</span>
<span id="cb450-2"><a href="chapter5.html#cb450-2"></a><span class="kw">data</span>(cheating) <span class="co">#Survey of students</span></span>
<span id="cb450-3"><a href="chapter5.html#cb450-3"></a>cheating &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(cheating)</span>
<span id="cb450-4"><a href="chapter5.html#cb450-4"></a>cheating<span class="op">$</span>LIEEXAM &lt;-<span class="st"> </span><span class="kw">factor</span>(cheating<span class="op">$</span>LIEEXAM)</span>
<span id="cb450-5"><a href="chapter5.html#cb450-5"></a>cheating<span class="op">$</span>LIEPAPER &lt;-<span class="st"> </span><span class="kw">factor</span>(cheating<span class="op">$</span>LIEPAPER)</span>
<span id="cb450-6"><a href="chapter5.html#cb450-6"></a>cheating<span class="op">$</span>FRAUD &lt;-<span class="st"> </span><span class="kw">factor</span>(cheating<span class="op">$</span>FRAUD)</span>
<span id="cb450-7"><a href="chapter5.html#cb450-7"></a>cheating<span class="op">$</span>COPYEXAM &lt;-<span class="st"> </span><span class="kw">factor</span>(cheating<span class="op">$</span>COPYEXAM)</span>
<span id="cb450-8"><a href="chapter5.html#cb450-8"></a>cheating<span class="op">$</span>GPA &lt;-<span class="st"> </span><span class="kw">factor</span>(cheating<span class="op">$</span>GPA)</span>
<span id="cb450-9"><a href="chapter5.html#cb450-9"></a><span class="kw">tableplot</span>(cheating, <span class="dt">sort=</span>GPA,<span class="dt">pals=</span><span class="kw">list</span>(<span class="st">&quot;BrBG&quot;</span>))</span></code></pre></div>
<div class="figure"><span id="fig:Figure5-18"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-18-1.png" alt="Tableplot of initial cheating and lying data set. Note that a few GPAs were missing in the data set." width="480" />
<p class="caption">
Figure 5.18: Tableplot of initial cheating and lying data set. Note that a few GPAs were missing in the data set.
</p>
</div>
<p>We can explore some interesting questions about the relationships between
these variables. The
tableplot in Figure <a href="chapter5.html#fig:Figure5-18">5.18</a> again helps us to get a general
idea of the data set
and to assess some complicated aspects of the relationships between variables.
For example, the rates of different unethical behaviors seem to decrease with
higher GPA students (but do not completely disappear!). This data set also has
a few missing GPAs that we would want to carefully consider – which sorts of
students might not be willing to reveal their GPAs? It ends up that these
students did not <em>admit</em> to any of the unethical behaviors… Note that we
used the <code>sort=GPA</code> option in the <code>tableplot</code> function to sort the
responses based on <code>GPA</code> to see how <code>GPA</code> might relate to patterns of
unethical behavior.</p>
<p>While the relationship between GPA and presence/absence of the different
behaviors is of interest, we want to explore the types of behaviors.
It is possible to group
the lying behaviors as being a different type (less extreme?) of unethical behavior than
obtaining an exam prior to taking it, buying a paper, or copying someone else’s
answers. We want to explore whether there is some sort of relationship between
the lying and copying behaviors – are those that engage in one type of behavior
more likely to do the other? Or are they independent of each other? This is a
hard story to elicit from the previous plot because there are so many variables
involved.</p>

<div class="figure"><span id="fig:Figure5-19"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-19-1.png" alt="Tableplot of new variables liar` andcopier`` that allow exploration of relationships between different types of lying and cheating behaviors." width="960" />
<p class="caption">
Figure 5.19: Tableplot of new variables <code>liar` and</code>copier`` that allow exploration of relationships between different types of lying and cheating behaviors.
</p>
</div>
<p>To simplify the results, combining the two groups of variables
into the four possible combinations on
each has the potential to simplify the results – or at least allow exploration
of additional research questions. The <code>interaction</code> function is used to create two new variables that have four levels that are combinations of the different options from none to both of each type (<em>copier</em> and <em>liar</em>). In the tableplot in
Figure <a href="chapter5.html#fig:Figure5-19">5.19</a>, you can see
the four categories for each, starting with no bad behavior of either type
(which is fortunately the most popular response on both variables!). For each
variable, there are students who admitted to one of the two violations and some
that did both. The <code>liar</code> variable has categories of <em>None</em>, <em>ExamLie</em>,
<em>PaperLie</em>, and <em>LieBoth</em>. The <code>copier</code> variable has categories of <em>None</em>,
<em>PaperCheat</em>, <em>ExamCheat</em>, and <em>PaperExamCheat</em> (for doing both). The last
category for <code>copier</code> seems to mostly occur at
the top of the plot which is where the students who had lied to get out of things
reside, so maybe there is a relationship between those two types of behaviors?
On the other hand, for the students who have never lied, quite a few had
cheated on exams. The contingency table can help us dig further into the
hypotheses related to the Chi-square test of Independence that is appropriate
in this situation. </p>
<div class="sourceCode" id="cb451"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb451-1"><a href="chapter5.html#cb451-1"></a>cheating<span class="op">$</span>liar &lt;-<span class="st"> </span><span class="kw">interaction</span>(cheating<span class="op">$</span>LIEEXAM, cheating<span class="op">$</span>LIEPAPER)</span>
<span id="cb451-2"><a href="chapter5.html#cb451-2"></a><span class="kw">levels</span>(cheating<span class="op">$</span>liar) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;None&quot;</span>,<span class="st">&quot;ExamLie&quot;</span>,<span class="st">&quot;PaperLie&quot;</span>,<span class="st">&quot;LieBoth&quot;</span>)</span>
<span id="cb451-3"><a href="chapter5.html#cb451-3"></a></span>
<span id="cb451-4"><a href="chapter5.html#cb451-4"></a>cheating<span class="op">$</span>copier &lt;-<span class="st"> </span><span class="kw">interaction</span>(cheating<span class="op">$</span>FRAUD, cheating<span class="op">$</span>COPYEXAM)</span>
<span id="cb451-5"><a href="chapter5.html#cb451-5"></a><span class="kw">levels</span>(cheating<span class="op">$</span>copier) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;None&quot;</span>,<span class="st">&quot;PaperCheat&quot;</span>,<span class="st">&quot;ExamCheat&quot;</span>,<span class="st">&quot;PaperExamCheat&quot;</span>)</span>
<span id="cb451-6"><a href="chapter5.html#cb451-6"></a><span class="kw">tableplot</span>(cheating, <span class="dt">sort=</span>liar, <span class="dt">select=</span><span class="kw">c</span>(liar,copier),<span class="dt">pals=</span><span class="kw">list</span>(<span class="st">&quot;BrBG&quot;</span>))</span></code></pre></div>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb452-1"><a href="chapter5.html#cb452-1"></a>cheatlietable &lt;-<span class="st"> </span><span class="kw">tally</span>(<span class="op">~</span>liar<span class="op">+</span>copier, <span class="dt">data=</span>cheating)</span>
<span id="cb452-2"><a href="chapter5.html#cb452-2"></a>cheatlietable</span></code></pre></div>
<pre><code>##           copier
## liar       None PaperCheat ExamCheat PaperExamCheat
##   None      207          7        46              5
##   ExamLie    10          1         3              2
##   PaperLie   13          1         4              2
##   LieBoth    11          1         4              2</code></pre>
<p>Unfortunately for our statistic, there were very few responses in some combinations of
categories even with <span class="math inline">\(N=319\)</span>. For example, there was only one response
each in the combinations for students that copied on papers and lied
to get out of exams, papers, and both. Some other categories were pretty small
as well in the groups that only had one behavior present. To get a higher
number of counts in the combinations, we combined the single behavior only levels
into “<em>either</em>” categories and left the <em>none</em> and <em>both</em> categories for each
variable. This creates two new variables called <code>liar2</code> and <code>copier2</code>
(tableplot in Figure <a href="chapter5.html#fig:Figure5-20">5.20</a>). The code to create these
variables and make the plot is below which employs the <code>levels</code> function to assign the same label to two different levels from the original list. </p>

<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb454-1"><a href="chapter5.html#cb454-1"></a><span class="co">#Collapse the middle categories of each variable</span></span>
<span id="cb454-2"><a href="chapter5.html#cb454-2"></a>cheating<span class="op">$</span>liar2 &lt;-<span class="st"> </span>cheating<span class="op">$</span>liar</span>
<span id="cb454-3"><a href="chapter5.html#cb454-3"></a><span class="kw">levels</span>(cheating<span class="op">$</span>liar2) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;None&quot;</span>,<span class="st">&quot;ExamorPaper&quot;</span>,<span class="st">&quot;ExamorPaper&quot;</span>,<span class="st">&quot;LieBoth&quot;</span>)</span>
<span id="cb454-4"><a href="chapter5.html#cb454-4"></a>cheating<span class="op">$</span>copier2 &lt;-<span class="st"> </span>cheating<span class="op">$</span>copier</span>
<span id="cb454-5"><a href="chapter5.html#cb454-5"></a><span class="kw">levels</span>(cheating<span class="op">$</span>copier2) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;None&quot;</span>,<span class="st">&quot;ExamorPaper&quot;</span>,<span class="st">&quot;ExamorPaper&quot;</span>,<span class="st">&quot;CopyBoth&quot;</span>)</span>
<span id="cb454-6"><a href="chapter5.html#cb454-6"></a>cheatlietable &lt;-<span class="st"> </span><span class="kw">tally</span>(<span class="op">~</span>liar2<span class="op">+</span>copier2, <span class="dt">data=</span>cheating)</span>
<span id="cb454-7"><a href="chapter5.html#cb454-7"></a>cheatlietable</span></code></pre></div>
<pre><code>##              copier2
## liar2         None ExamorPaper CopyBoth
##   None         207          53        5
##   ExamorPaper   23           9        4
##   LieBoth       11           5        2</code></pre>
<div class="sourceCode" id="cb456"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb456-1"><a href="chapter5.html#cb456-1"></a><span class="kw">tableplot</span>(cheating, <span class="dt">sort=</span>liar2, <span class="dt">select=</span><span class="kw">c</span>(liar2,copier2),<span class="dt">pals=</span><span class="kw">list</span>(<span class="st">&quot;BrBG&quot;</span>))</span></code></pre></div>
<div class="figure"><span id="fig:Figure5-20"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-20-1.png" alt="Tableplot of lying and copying variables after combining categories." width="960" />
<p class="caption">
Figure 5.20: Tableplot of lying and copying variables after combining categories.
</p>
</div>
<p>This <span class="math inline">\(3\times 3\)</span> table is more manageable and has few really small
cells so we will proceed with the 6+ steps of hypothesis
testing applied to these data using the Independence testing methods (again a
single sample was taken from the population so that is the appropriate procedure to employ):</p>
<ol start="0" style="list-style-type: decimal">
<li><p>The RQ is about relationships between lying to instructors and cheating and these questions, after some work and simplifications, allow us to address a version of that RQ even though it might not be the one that we started with. The tableplots help to visualize the results and the <span class="math inline">\(X^2\)</span>-statistic will be used to do the hypothesis test.</p></li>
<li><p><strong>Hypotheses:</strong></p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: Lying and copying behavior are independent in the population
of students at this university.</p></li>
<li><p><span class="math inline">\(H_A\)</span>: Lying and copying behavior are dependent in the population
of students at this university.</p></li>
</ul></li>
<li><p><strong>Validity conditions:</strong></p>
<ul>
<li><p>Independence:</p>
<ul>
<li>There is no indication of a violation of this assumption since each subject
is measured only once in the table. No other information
suggests a potential issue but we don’t have much
information on how these subjects were obtained. What happens if we had sampled from students in different sections of a multi-section course and one of the sections had recently had a cheating scandal that impacted many students in that section?</li>
</ul></li>
<li><p>All expected cell counts larger than 5 (required to use
<span class="math inline">\(\chi^2\)</span>-distribution to find p-values):</p>
<ul>
<li>We need to generate a table of expected cell counts to
check this condition:</li>
</ul>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb457-1"><a href="chapter5.html#cb457-1"></a><span class="kw">chisq.test</span>(cheatlietable)<span class="op">$</span>expected</span></code></pre></div>
<pre><code>##              copier2
## liar2              None ExamorPaper  CopyBoth
##   None        200.20376   55.658307 9.1379310
##   ExamorPaper  27.19749    7.561129 1.2413793
##   LieBoth      13.59875    3.780564 0.6206897</code></pre>
<ul>
<li><p>When we request the expected cell counts, there is a
warning message (not shown).</p></li>
<li><p>There are three expected cell counts below 5,
so the condition is violated and a permutation approach
should be used to obtain more trustworthy p-values.</p></li>
</ul></li>
</ul></li>
<li><p><strong>Calculate the test statistic and p-value:</strong></p>
<ul>
<li>Use <code>chisq.test</code> to obtain the test statistic, although this table is small enough to do
by hand if you want the practice – see if you can find a similar answer
to what the function provides:</li>
</ul>
<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb459-1"><a href="chapter5.html#cb459-1"></a><span class="kw">chisq.test</span>(cheatlietable)</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  cheatlietable
## X-squared = 13.238, df = 4, p-value = 0.01017</code></pre>
<ul>
<li><p>The <span class="math inline">\(X^2\)</span> statistic is 13.24.</p></li>
<li><p>The parametric p-value is 0.0102 from the R output. This was based
on a <span class="math inline">\(\chi^2\)</span>-distribution with <span class="math inline">\((3-1)*(3-1) = 4\)</span> degrees of freedom
that is displayed in Figure <a href="chapter5.html#fig:Figure5-21">5.21</a>. Remember that this
isn’t quite the right distribution for the test statistic since our
expected cell count condition was violated.</p></li>
</ul>
<div class="figure"><span id="fig:Figure5-21"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-21-1.png" alt="Plot of $\boldsymbol{\chi^2}$-distribution with 4 degrees of freedom." width="336" />
<p class="caption">
Figure 5.21: Plot of <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution with 4 degrees of freedom.
</p>
</div>
<ul>
<li>If you want to repeat the p-value calculation directly:</li>
</ul>
<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb461-1"><a href="chapter5.html#cb461-1"></a><span class="kw">pchisq</span>(<span class="fl">13.2384</span>, <span class="dt">df=</span><span class="dv">4</span>, <span class="dt">lower.tail=</span>F)</span></code></pre></div>
<pre><code>## [1] 0.01016781</code></pre>
<ul>
<li>But since the expected cell condition is violated, we
should use permutations as implemented in the following
code with the number of permutations increased to 10,000 to help
get a better estimate of the p-value since it is
possibly close to 0.05:</li>
</ul>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb463-1"><a href="chapter5.html#cb463-1"></a>Tobs &lt;-<span class="st"> </span><span class="kw">chisq.test</span>(<span class="kw">tally</span>(<span class="op">~</span>liar2<span class="op">+</span>copier2, <span class="dt">data=</span>cheating))<span class="op">$</span>statistic</span>
<span id="cb463-2"><a href="chapter5.html#cb463-2"></a>Tobs</span></code></pre></div>
<pre><code>## X-squared 
##  13.23844</code></pre>
<div class="sourceCode" id="cb465"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb465-1"><a href="chapter5.html#cb465-1"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb465-2"><a href="chapter5.html#cb465-2"></a>B &lt;-<span class="st"> </span><span class="dv">10000</span> <span class="co"># Now performing 10,000 permutations</span></span>
<span id="cb465-3"><a href="chapter5.html#cb465-3"></a>Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>,<span class="dt">nrow=</span>B)</span>
<span id="cb465-4"><a href="chapter5.html#cb465-4"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</span>
<span id="cb465-5"><a href="chapter5.html#cb465-5"></a>  Tstar[b] &lt;-<span class="st"> </span><span class="kw">chisq.test</span>(<span class="kw">tally</span>(<span class="op">~</span><span class="kw">shuffle</span>(liar2)<span class="op">+</span>copier2,</span>
<span id="cb465-6"><a href="chapter5.html#cb465-6"></a>                               <span class="dt">data=</span>cheating))<span class="op">$</span>statistic</span>
<span id="cb465-7"><a href="chapter5.html#cb465-7"></a>}</span>
<span id="cb465-8"><a href="chapter5.html#cb465-8"></a><span class="kw">pdata</span>(Tstar, Tobs, <span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>## [1] 0.0174</code></pre>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb467-1"><a href="chapter5.html#cb467-1"></a><span class="kw">hist</span>(Tstar)</span>
<span id="cb467-2"><a href="chapter5.html#cb467-2"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb467-3"><a href="chapter5.html#cb467-3"></a><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</span>
<span id="cb467-4"><a href="chapter5.html#cb467-4"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure5-22"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-22-1.png" alt="Plot of permutation distributions for cheat/lie results with observed value of 13.24 (bold, vertical line)." width="480" />
<p class="caption">
Figure 5.22: Plot of permutation distributions for cheat/lie results with observed value of 13.24 (bold, vertical line).
</p>
</div>
<ul>
<li>There were 174 of <span class="math inline">\(B\)</span>=10,000 permuted data
sets that produced as large or larger
<span class="math inline">\(X^{2*}\text{&#39;s}\)</span> than the observed as displayed in
Figure <a href="chapter5.html#fig:Figure5-22">5.22</a>, so we report that the p-value was
0.0174 using the permutation approach, which was slightly larger than the
result provided by the parametric method.</li>
</ul></li>
<li><p><strong>Conclusion:</strong></p>
<ul>
<li>There is strong evidence against the null hypothesis of no relationship between lying and copying behavior in the population
of students (<span class="math inline">\(X^2\)</span>-statistic=13.24, permutation p-value of 0.0174), so conclude that there is a relationship between lying and copying behavior at the university in the population of students studied.</li>
</ul></li>
<li><p><strong>Size:</strong></p>
<ul>
<li>The standardized residuals can help us more fully understand this result – the mosaic plot only had one cell shaded and so wasn’t needed here.</li>
</ul>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb468-1"><a href="chapter5.html#cb468-1"></a><span class="kw">chisq.test</span>(cheatlietable)<span class="op">$</span>residuals</span></code></pre></div>
<pre><code>##              copier2
## liar2               None ExamorPaper   CopyBoth
##   None         0.4803220  -0.3563200 -1.3688609
##   ExamorPaper -0.8048695   0.5232734  2.4759378
##   LieBoth     -0.7047165   0.6271633  1.7507524</code></pre>
<ul>
<li>There is really only one large standardized residual for the <em>ExamorPaper</em> liars and the <em>CopyBoth</em> copiers, with a much larger observed value than expected of 2.48. The only other medium-sized standardized residuals came from the <em>CopyBoth</em> copiers column with fewer than expected students in the <em>None</em> category and more than expected in the <em>LieBoth</em> type of lying category. So we are seeing more than expected that lied somehow and copied – we can say this suggests that the students who lie tend to copy too!</li>
</ul></li>
<li><p><strong>Scope of inference:</strong></p>
<ul>
<li>There is no causal inference possible here since
neither variable was randomly assigned (really neither is
explanatory or response here either) but we can extend the
inferences to the population of students that these were selected
from that would be willing to reveal their GPA (see initial
discussion related to some differences in students that wouldn’t
answer that question).</li>
</ul></li>
</ol>
</div>
<div id="section5-11" class="section level2">
<h2><span class="header-section-number">5.11</span> Analyzing a stratified random sample of California schools</h2>

<div class="figure"><span id="fig:Figure5-23"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-23-1.png" alt="Pirate-plot of the API growth scores by level of school in the stype variable (coded E for elementary, M for Middle, and H for High school)." width="576" />
<p class="caption">
Figure 5.23: Pirate-plot of the API growth scores by level of school in the <code>stype</code> variable (coded E for elementary, M for Middle, and H for High school).
</p>
</div>
<p>In recent decades, there has been a push for quantification of school performance
and tying financial punishment and rewards to growth in these metrics both for schools and for teachers. One
example is the API (Academic Performance Index) in California that is based
mainly on student scores on standardized tests. It ranges between 200 and 1000
and year to year changes are of interest to assess “performance” of schools –
calculated as one year minus the previous year (negative “growth” is also
possible!). Suppose that a researcher is interested in whether the growth
metric might differ between different levels of schools. Maybe it is easier or
harder for elementary, middle, or high schools to attain growth? The researcher
has a list of most of the schools in the state of each level that are using a
database that the researcher has access to. In order to assess this question,
the researcher takes a <strong><em>stratified random sample</em></strong><a href="#fn93" class="footnote-ref" id="fnref93"><sup>93</sup></a>, selecting
<span class="math inline">\(n_{\text{elementary}}=100\)</span> schools from the population of 4421 elementary
schools, <span class="math inline">\(n_{\text{middle}}=50\)</span> from the population of 1018 middle schools,
and <span class="math inline">\(n_{\text{high}}=50\)</span> from the population of 755 high
schools. These data are available in the <code>survey</code> package <span class="citation">(Lumley <a href="#ref-R-survey" role="doc-biblioref">2019</a>)</span>
and the <code>api</code> data object that loads both <code>apipop</code> (population) and <code>apistrat</code> (stratified random sample) data sets.

The <code>growth</code> (change!) in
API scores for the schools between 1999 and
2000 (taken as the year 2000 score minus 1999 score) is used as the response
variable. The pirate-plot of the growth scores are displayed in Figure
<a href="chapter5.html#fig:Figure5-23">5.23</a>. They suggest some differences in the growth
rates among the different levels. There are also a few schools flagged as being possible outliers.</p>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb470-1"><a href="chapter5.html#cb470-1"></a><span class="kw">library</span>(survey)</span>
<span id="cb470-2"><a href="chapter5.html#cb470-2"></a><span class="kw">data</span>(api)</span>
<span id="cb470-3"><a href="chapter5.html#cb470-3"></a>apistrat &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(apistrat)</span>
<span id="cb470-4"><a href="chapter5.html#cb470-4"></a>apipop &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(apipop)</span>
<span id="cb470-5"><a href="chapter5.html#cb470-5"></a><span class="kw">tally</span>(<span class="op">~</span>stype, <span class="dt">data=</span>apipop) <span class="co">#Population counts</span></span></code></pre></div>
<pre><code>## stype
##    E    H    M 
## 4421  755 1018</code></pre>
<div class="sourceCode" id="cb472"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb472-1"><a href="chapter5.html#cb472-1"></a><span class="kw">tally</span>(<span class="op">~</span>stype, <span class="dt">data=</span>apistrat) <span class="co">#Sample counts</span></span></code></pre></div>
<pre><code>## stype
##   E   H   M 
## 100  50  50</code></pre>
<div class="sourceCode" id="cb474"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb474-1"><a href="chapter5.html#cb474-1"></a><span class="kw">pirateplot</span>(growth<span class="op">~</span>stype, <span class="dt">data=</span>apistrat, <span class="dt">inf.method=</span><span class="st">&quot;ci&quot;</span>, <span class="dt">inf.disp=</span><span class="st">&quot;line&quot;</span>)</span></code></pre></div>
<p>The One-Way ANOVA <span class="math inline">\(F\)</span>-test, provided below, suggests
strong evidence against the null hypothesis of no difference in the true mean growth scores among the different
types of schools (<span class="math inline">\(F(2,197)=23.56,\text{ p-value}&lt;0.0001\)</span>). But the
residuals from this model displayed in the QQ-Plot in
Figure <a href="chapter5.html#fig:Figure5-24">5.24</a> contain a slightly long right tail and short left tail,
suggesting a right skewed
distribution for the residuals. In a high-stakes situation such as this,
reporting results with violations of the assumptions probably would not be
desirable, so another approach is needed. The permutation methods would be
justified here but there is another “simpler” option available using our new
Chi-square analysis methods.</p>

<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb475-1"><a href="chapter5.html#cb475-1"></a>m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(growth<span class="op">~</span>stype, <span class="dt">data=</span>apistrat)</span>
<span id="cb475-2"><a href="chapter5.html#cb475-2"></a><span class="kw">library</span>(car)</span>
<span id="cb475-3"><a href="chapter5.html#cb475-3"></a><span class="kw">Anova</span>(m1)</span></code></pre></div>
<pre><code>## Anova Table (Type II tests)
## 
## Response: growth
##           Sum Sq  Df F value    Pr(&gt;F)
## stype      30370   2  23.563 6.685e-10
## Residuals 126957 197</code></pre>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb477-1"><a href="chapter5.html#cb477-1"></a><span class="kw">plot</span>(m1, <span class="dt">which=</span><span class="dv">2</span>, <span class="dt">pch=</span><span class="dv">16</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure5-24"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-24-1.png" alt="QQ-plot of standardized residuals from the One-Way ANOVA linear model." width="528" />
<p class="caption">
Figure 5.24: QQ-plot of standardized residuals from the One-Way ANOVA linear model.
</p>
</div>
<p>One way to get around the normality assumption is to use a method
that does not assume the responses
follow a normal distribution. If we <strong><em>bin</em></strong> or cut the quantitative response
variable into a set of ordered categories and apply a Chi-square test, we can
proceed without concern about the lack of normality in the residuals of the
ANOVA model.


To create these bins, a simple idea would be to use the quartiles
to generate the response variable categories, binning the quantitative
responses into groups for the lowest 25%, second 25%, third 25%, and highest
25% by splitting the data at <span class="math inline">\(Q_1\)</span>, the Median, and <span class="math inline">\(Q_3\)</span>. In R, the
<code>cut</code> function is available to turn a
quantitative variable into a categorical variable. First, we can use the
information from <code>favstats</code> to find the cut-points:</p>
<div class="sourceCode" id="cb478"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb478-1"><a href="chapter5.html#cb478-1"></a><span class="kw">favstats</span>(<span class="op">~</span>growth, <span class="dt">data=</span>apistrat)</span></code></pre></div>
<pre><code>##  min   Q1 median Q3 max   mean      sd   n missing
##  -47 6.75     25 48 133 27.995 28.1174 200       0</code></pre>
<p>The <code>cut</code> function can provide the binned variable if it is provided
with the end-points of the desired intervals to
create new categories with those names in a new variable called
<code>growthcut</code>. </p>
<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb480-1"><a href="chapter5.html#cb480-1"></a>apistrat<span class="op">$</span>growthcut &lt;-<span class="st"> </span><span class="kw">cut</span>(apistrat<span class="op">$</span>growth, <span class="dt">breaks=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">47</span>,<span class="fl">6.75</span>,<span class="dv">25</span>,<span class="dv">48</span>,<span class="dv">133</span>),</span>
<span id="cb480-2"><a href="chapter5.html#cb480-2"></a>                          <span class="dt">include.lowest=</span>T)</span></code></pre></div>
<!-- \newpage -->
<div class="sourceCode" id="cb481"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb481-1"><a href="chapter5.html#cb481-1"></a><span class="kw">tally</span>(<span class="op">~</span>growthcut, <span class="dt">data=</span>apistrat)</span></code></pre></div>
<pre><code>## growthcut
## [-47,6.75]  (6.75,25]    (25,48]   (48,133] 
##         50         52         49         49</code></pre>
<p>Now that we have a categorical response variable, we need to decide which
sort of Chi-square
analysis to perform. The sampling design determines the correct analysis as
always in these situations. The stratified random sample involved samples from
each of the three populations so a Homogeneity test should be employed. In
these situations, the stacked bar chart provides the appropriate summary of the
data. It also shows us the labels of the categories that the <code>cut</code> function
created in the new <code>growthcut</code> variable:</p>

<div class="figure"><span id="fig:Figure5-25"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-25-1.png" alt="Stacked bar chart of the growth category responses by level of school." width="480" />
<p class="caption">
Figure 5.25: Stacked bar chart of the growth category responses by level of school.
</p>
</div>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb483-1"><a href="chapter5.html#cb483-1"></a><span class="kw">plot</span>(growthcut<span class="op">~</span>stype, <span class="dt">data=</span>apistrat)</span></code></pre></div>
<p>Figure <a href="chapter5.html#fig:Figure5-25">5.25</a> suggests that the distributions of growth
scores may not be the same across the levels of the
schools with many more high growth <em>Elementary</em> schools than in either
the <em>Middle</em> or <em>High</em> school groups (the “high” growth category
is labeled as (48, 133] providing the interval of growth scores placed in this
category). Similarly, the proportion of the low or negative growth (category of
(-47.6, 6.75] for “growth” between -47.6 and 6.75) is least frequently occurring in <em>Elementary</em>
schools and most frequent in the <em>High</em> schools. Statisticians often work
across many disciplines and so may not always have the subject area knowledge
to know why these differences exist (just like you might not), but an education
researcher could take this sort of information – because it is a useful summary
of interesting school-level data – and generate further insights into why
growth in the API metric may or may not be a good or fair measure of school
performance.</p>
<p>Of course, we want to consider whether these results can extend to the
population of all California schools. The homogeneity hypotheses for
assessing the growth rate categories across the types of schools would be:</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: There is no difference in the distribution of growth categories
across the three levels of schools in the population of California schools.</p></li>
<li><p><span class="math inline">\(H_A\)</span>: There is some difference in the distribution of growth categories
across the three levels of schools in the population of California schools.</p></li>
</ul>
<p>There might be an issue with the independence assumption in that schools
within the same district might
be more similar to one another and different between one another. Sometimes
districts are accounted for in education research to account for differences in
policies and demographics among the districts. We could explore this issue by
finding district-level average growth rates and exploring whether those vary
systematically but this is beyond the scope of the current exploration.</p>
<p>Checking the expected cell counts gives insight into the assumption for
using the <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution to find the p-value:</p>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb484"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb484-1"><a href="chapter5.html#cb484-1"></a>growthtable &lt;-<span class="st"> </span><span class="kw">tally</span>(<span class="op">~</span>stype<span class="op">+</span>growthcut, <span class="dt">data=</span>apistrat)</span>
<span id="cb484-2"><a href="chapter5.html#cb484-2"></a>growthtable</span></code></pre></div>
<pre><code>##      growthcut
## stype [-47,6.75] (6.75,25] (25,48] (48,133]
##     E         14        22      27       37
##     H         24        18       5        3
##     M         12        12      17        9</code></pre>
<div class="sourceCode" id="cb486"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb486-1"><a href="chapter5.html#cb486-1"></a><span class="kw">chisq.test</span>(growthtable)<span class="op">$</span>expected </span></code></pre></div>
<pre><code>##      growthcut
## stype [-47,6.75] (6.75,25] (25,48] (48,133]
##     E       25.0        26   24.50    24.50
##     H       12.5        13   12.25    12.25
##     M       12.5        13   12.25    12.25</code></pre>
<p>The smallest expected count is 12.25, occurring in four different cells,
so we can use the parametric approach.</p>
<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb488-1"><a href="chapter5.html#cb488-1"></a><span class="kw">chisq.test</span>(growthtable) </span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  growthtable
## X-squared = 38.668, df = 6, p-value = 8.315e-07</code></pre>
<p>The observed test statistic is <span class="math inline">\(X^2=38.67\)</span> and, based on a
<span class="math inline">\(\boldsymbol{\chi^2}(6)\)</span> distribution, the p-value is 0.0000008. This p-value
suggests that there is very strong evidence against the null hypothesis of no difference in the distribution of API growth of schools among
<em>Elementary</em>, <em>Middle</em> and <em>High School</em> in the population of schools in
California between 1999 and 2000, and we can conclude that there is some difference in the population (California
schools). Because the schools were randomly selected from all the California
schools we can make valid inferences to all the schools but because the level of schools, obviously, cannot be randomly
assigned, we cannot say that level of school causes these differences.</p>
<p>The standardized residuals can enhance this interpretation, displayed in
Figure <a href="chapter5.html#fig:Figure5-26">5.26</a>. The <em>Elementary</em> schools have fewer low/negative growth schools and more
high growth schools than expected under the null hypothesis. The <em>High</em>
schools have more low growth and fewer higher growth (growth over 25 points) schools
than expected if there were no difference in patterns of response across
the school levels. The <em>Middle</em> school results were closer to the results
expected if there were no differences across the school levels.</p>
<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb490-1"><a href="chapter5.html#cb490-1"></a><span class="kw">chisq.test</span>(growthtable)<span class="op">$</span>residuals</span></code></pre></div>
<pre><code>##      growthcut
## stype [-47,6.75]  (6.75,25]    (25,48]   (48,133]
##     E -2.2000000 -0.7844645  0.5050763  2.5253814
##     H  3.2526912  1.3867505 -2.0714286 -2.6428571
##     M -0.1414214 -0.2773501  1.3571429 -0.9285714</code></pre>

<div class="figure"><span id="fig:Figure5-26"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-26-1.png" alt="Mosaic plot of the API Growth rate categories versus level of the school with shading for size of standardized residuals." width="576" />
<p class="caption">
Figure 5.26: Mosaic plot of the API Growth rate categories versus level of the school with shading for size of standardized residuals.
</p>
</div>
<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb492-1"><a href="chapter5.html#cb492-1"></a><span class="kw">mosaicplot</span>(growthcut<span class="op">~</span>stype,<span class="dt">data=</span>apistrat,<span class="dt">shade=</span>T)</span></code></pre></div>
<p>The binning of quantitative variables is not a first step in analyses –
the quantitative
version is almost always preferable. However, this analysis avoided the
violation of the normality assumption that was somewhat problematic for the
ANOVA and still provided useful inferences to the differences in the types of
schools. When one goes from a quantitative to categorical version of a
variable, one loses information (the specific details of the quantitative
responses within each level created) and this almost always will result in a
loss of statistical power of the procedure.

In this situation, the p-value from
the ANOVA was of the order <span class="math inline">\(10^{-10}\)</span> while the Chi-square test had a
p-value of order <span class="math inline">\(10^{-7}\)</span>. This larger p-value is typical of the loss of
power in going to a categorical response when more information was available.
In many cases, there are no options but to use contingency table analyses. This
example shows that there might be some situations where “going categorical”
could be an acceptable method for handing situations where an assumption is
violated.</p>
<!-- \newpage -->
</div>
<div id="section5-12" class="section level2">
<h2><span class="header-section-number">5.12</span> Chapter summary</h2>
<p>Chi-square tests can be generally used to perform two types of tests,
the Independence and Homogeneity
tests. The appropriate analysis is determined based on the data collection
methodology. The parametric Chi-square distribution for which these tests are
named is appropriate when the expected cell counts are large enough (related to
having a large enough overall sample). When the expected cell count condition is violated, the
permutation approach can provide valuable inferences in these situations in
most situations.</p>
<p>Data displays of the stacked bar chart (Homogeneity) and mosaic plots
(Independence) provide a visual
summary of the results that can also be found in contingency tables. You should
have learned how to calculate the <span class="math inline">\(X^2\)</span> (X-squared) test statistic
based on first finding the expected cell counts. Under certain assumptions, it
will follow a Chi-Square distribution with <span class="math inline">\((R-1)(C-1)\)</span> degrees of freedom. When
those assumptions are not met, it is better to use a permutation approach to
find p-values. Either way, the same statistic is used to test either kind of
hypothesis, independence or homogeneity. After assessing evidence against the null hypothesis, it is interesting to see which cells in the table
contributed to the deviations from the null hypothesis. The standardized
residuals provide that information. Graphing them in a mosaic plot makes for a
fun display to identify the large residuals and often allows you to better
understand the results. This should tie back into the original data display (tableplot, stacked bar chart or mosaic plot) and
contingency table where you identified initial patterns and help to tell the story of the results.</p>
<!-- \newpage -->
</div>
<div id="section5-13" class="section level2">
<h2><span class="header-section-number">5.13</span> Summary of important R commands</h2>
<p>The main components of R code used in this chapter follow with components
to modify in lighter and/or ALL CAPS text where <code>y</code> is a response variable and <code>x</code> is a predictor
are easily identified:</p>
<ul>
<li><p><strong><font color='red'>TABLENAME</font> <code>&lt;-</code> tally(~<font color='red'>x</font> +
<font color='red'>y</font>, data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li><p>This function requires that the <code>mosaic</code> package has been loaded.</p></li>
<li><p>This provides a table of the counts in the variable called
<code>TABLENAME</code>.</p></li>
<li><p><code>margins=T</code> is used if want to display row, column, and
table totals. </p></li>
</ul></li>
<li><p><strong>plot(<font color='red'>y</font>~
<font color='red'>x</font>, data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li>Makes a stacked bar chart useful for homogeneity test situations.
</li>
</ul></li>
<li><p><strong>mosaicplot(<font color='red'>TABLENAME</font>)</strong></p>
<ul>
<li>Makes a mosaic plot useful for finding patterns in the table
in independence test situations.
</li>
</ul></li>
<li><p><strong>tableplot(data=<font color='red'>DATASETNAME</font>, sortCol=<font color='red'>VARIABLENAME</font>,pals=list(“BrBG”))</strong></p>
<ul>
<li><p>Makes a tableplot sorted by <font color='red'>VARIABLENAME</font>, requires that the <code>tabplot</code> and <code>RColorBrewer</code> packages have been loaded.</p></li>
<li><p>The <code>pals=list("BrBG")</code> option provides a color-blind friendly color palette, although other options are possible, such as <code>pals=list("RdBu")</code>.
</p></li>
</ul></li>
<li><p><strong>chisq.test(<font color='red'>TABLENAME</font>)</strong></p>
<ul>
<li>Provides <span class="math inline">\(X^2\)</span> and p-values based on the
<span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution with <span class="math inline">\((R-1)(C-1)\)</span> degrees of
freedom. </li>
</ul></li>
<li><p><strong>chisq.test(<font color='red'>TABLENAME</font>)$expected</strong></p>
<ul>
<li>Provides expected cell counts.</li>
</ul></li>
<li><p><strong>pchisq(<font color='red'>X-SQUARED</font>,
df=(<font color='red'>R</font> - 1)<code>*</code>(<font color='red'>C</font> - 1), lower.tail=F)</strong></p>
<ul>
<li><p>Provides p-value from <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution with
<span class="math inline">\((R-1)(C-1)\)</span> degrees of freedom for observed test statistic.</p></li>
<li><p>See Section <a href="chapter5.html#section5-5">5.5</a> for code related to finding a
permutation-based p-value. </p></li>
</ul></li>
<li><p><strong>chisq.test(<font color='red'>TABLENAME</font>)$residuals^2</strong></p>
<ul>
<li>Provides <span class="math inline">\(X^2\)</span> contributions from each cell in table.</li>
</ul></li>
<li><p><strong>chisq.test(<font color='red'>TABLENAME</font>)$residuals</strong></p>
<ul>
<li>Provides standardized residuals.</li>
</ul></li>
<li><p><strong>mosaicplot(<font color='red'>TABLENAME</font>, shade=T)</strong></p>
<ul>
<li>Provides a mosaic plot with shading based on standardized residuals.</li>
</ul></li>
</ul>
<div style="page-break-after: always;"></div>
</div>
<div id="section5-14" class="section level2">
<h2><span class="header-section-number">5.14</span> Practice problems</h2>
<p>5.1. <strong>Determine type of Chi-Square test</strong> Determine which type of test is appropriate in each situation –
<strong><em>Independence</em></strong> or <strong><em>Homogeneity</em></strong>?</p>
<p>5.1.1. Concerns over diseases being transmitted between birds and humans
have led to many areas developing
monitoring plans for the birds that are in their regions. The duck pond on
campus at MSU-Bozeman is a bit like a night club for the birds that pass
through Bozeman.</p>
<ol style="list-style-type: lower-roman">
<li><p>Suppose that a researcher randomly
samples 20 ducks at the duck pond on campus on 4 different
occasions and records the number ducks that are healthy and number
that are sick on each day. The variables in this study are the day of
measurement and sick/healthy.</p></li>
<li><p>In another monitoring study, a researcher goes to a wetland area
and collects a random sample from all birds
present on a single day, classifies them by type of bird (ducks,
swans, etc.) and then assesses whether each is sick or healthy.
The variables in this study are type of bird and sick/healthy.</p></li>
</ol>
<p>5.1.2. Psychologists performed an experiment on 48 male bank supervisors
attending a management institute to investigate biases against women
in personnel decisions. The supervisors were
asked to make a decision on whether to promote a hypothetical applicant based
on a personnel file. For half of them, the application file described
a female candidate; for the others it described a male.</p>
<p>5.1.3. Researchers collected data on death
penalty sentencing in Georgia. For 243 crimes, they categorized the crime by
severity from 1 to 6 with Category 1 comprising barroom brawls, liquor-induced
arguments, lovers’ quarrels, and similar crimes and Category 6 including the
most vicious, cruel, cold-blooded, unprovoked crimes. They also recorded the
perpetrator’s race. They wanted to know if there was a relationship between
race and type of crime.</p>
<p>5.1.4. Epidemiologists want to see if
Vitamin C helped people with colds. They would like to give some patients
Vitamin C and some a placebo then compare the two groups. However, they are
worried that the placebo might not be working. Since vitamin C has such a
distinct taste, they are worried the participants will know which group they are
in. To test if the placebo was working, they collected 200 subjects and
randomly assigned half to take a placebo and the other half to take Vitamin C.
30 minutes later, they asked the subjects which supplement they received
(hoping that the patients would not know which group they were assigned to).</p>
<p>5.1.5. Is zodiac sign related to GPA?
300 randomly selected students from MSU were asked their birthday and their
current GPA. GPA was then categorized as &lt; 1.50 = F, 1.51-2.50 = D,
2.51 - 3.25 = C, 3.26-3.75 = B, 3.76-4.0 = A and their birthday was used
to find their zodiac sign.</p>
<p>5.1.6. In 1935, the statistician R. A.
Fisher famously had a colleague claim that she could distinguish whether milk
or tea was added to a cup first. Fisher presented her, in a random order, 4
cups that were filled with milk first and 4 cups that were filled with tea first.</p>
<p>5.1.7. Researchers wanted to see if
people from Rural and Urban areas aged differently. They contacted 200 people
from Rural areas and 200 people from Urban areas and asked the participants
their age (&lt;40, 41-50, 51-60, &gt;60).</p>
<p>5.2. <strong>Data is/are analysis</strong> The <a href="https://fivethirtyeight.com/">FiveThirtyEight Blog</a>
often shows up with interesting data summaries
that have general public appeal. Their staff includes a bunch of quants with
various backgrounds. When starting their blog, they had to decide on the data
is/are question that we introduced in Section <a href="chapter2.html#section2-1">2.1</a>. To help them
think about this, they collected a nationally representative sample
that contained three questions about this. Based on their survey, they
concluded that</p>

<div class="figure"><span id="fig:Figure5-27"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-27-1.png" alt="Tableplot of data from “data-is-vs-data-are” survey, sorted by “CareAbout” responses." width="960" />
<p class="caption">
Figure 5.27: Tableplot of data from “data-is-vs-data-are” survey, sorted by “CareAbout” responses.
</p>
</div>
<blockquote>
<p>Relevant to the
<a href="http://fivethirtyeight.com/datalab/data-is-vs-data-are/">interests of FiveThirtyEight</a> in particular, we also asked whether people preferred using “data” as a
singular or plural noun. To those who prefer the plural, I’ll put this in
your terms: The data are pretty conclusive that the vast majority of
respondents think we should say “data is.” The singular crowd won by a 58
percentage-point margin, with 79 percent of respondents liking “data is” to 21
percent preferring “data are.” But only half of respondents had put any
thought to the usage prior to our survey, so it seems that it’s not a
pressing issue for most.</p>
</blockquote>
<p>This came from a survey that contained questions about <em>which is the
correct usage</em>, (<code>isare</code>), <em>have you thought about this issue</em>
(<code>thoughtabout</code>) with levels Yes/No, and <em>do you care about this issue</em>
(<code>careabout</code>) with four levels from <em>Not at all</em> to <em>A lot</em>. The following
code loads their data set after missing responses were removed, does a
little re-ordering of factor levels to help make the results easier to
understand, and makes a tableplot (Figure <a href="chapter5.html#fig:Figure5-27">5.27</a>) to get a general sense of the results
including information on the respondents’ gender, age, income, and education.</p>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb493-1"><a href="chapter5.html#cb493-1"></a><span class="kw">library</span>(readr)</span>
<span id="cb493-2"><a href="chapter5.html#cb493-2"></a>csd &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/csd.csv&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb494-1"><a href="chapter5.html#cb494-1"></a><span class="kw">library</span>(tabplot)</span>
<span id="cb494-2"><a href="chapter5.html#cb494-2"></a><span class="co">#Need to make it explicit that these are factor variables</span></span>
<span id="cb494-3"><a href="chapter5.html#cb494-3"></a>csd<span class="op">$</span>careabout &lt;-<span class="st"> </span><span class="kw">factor</span>(csd<span class="op">$</span>careabout) </span>
<span id="cb494-4"><a href="chapter5.html#cb494-4"></a><span class="co">#Reorders factor levels to be in &quot;correct&quot; order</span></span>
<span id="cb494-5"><a href="chapter5.html#cb494-5"></a>csd<span class="op">$</span>careabout &lt;-<span class="st"> </span><span class="kw">factor</span>(csd<span class="op">$</span>careabout,</span>
<span id="cb494-6"><a href="chapter5.html#cb494-6"></a>                    <span class="dt">levels=</span><span class="kw">levels</span>(csd<span class="op">$</span>careabout)[<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">2</span>)]) </span>
<span id="cb494-7"><a href="chapter5.html#cb494-7"></a>csd<span class="op">$</span>Education &lt;-<span class="st"> </span><span class="kw">factor</span>(csd<span class="op">$</span>Education)</span>
<span id="cb494-8"><a href="chapter5.html#cb494-8"></a>csd<span class="op">$</span>Education &lt;-<span class="st"> </span><span class="kw">factor</span>(csd<span class="op">$</span>Education,</span>
<span id="cb494-9"><a href="chapter5.html#cb494-9"></a>                    <span class="dt">levels=</span><span class="kw">levels</span>(csd<span class="op">$</span>Education)[<span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">2</span>)])</span>
<span id="cb494-10"><a href="chapter5.html#cb494-10"></a>csd<span class="op">$</span>Household.Income &lt;-<span class="st"> </span><span class="kw">factor</span>(csd<span class="op">$</span>Household.Income)</span>
<span id="cb494-11"><a href="chapter5.html#cb494-11"></a>csd<span class="op">$</span>Household.Income &lt;-<span class="st"> </span><span class="kw">factor</span>(csd<span class="op">$</span>Household.Income,</span>
<span id="cb494-12"><a href="chapter5.html#cb494-12"></a>                    <span class="dt">levels=</span><span class="kw">levels</span>(csd<span class="op">$</span>Household.Income)[<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">2</span>,<span class="dv">3</span>)])</span>
<span id="cb494-13"><a href="chapter5.html#cb494-13"></a><span class="co">#Sorts plot by careabout responses</span></span>
<span id="cb494-14"><a href="chapter5.html#cb494-14"></a><span class="kw">tableplot</span>(csd[,<span class="kw">c</span>(<span class="st">&quot;isare&quot;</span>,<span class="st">&quot;careabout&quot;</span>,<span class="st">&quot;thoughtabout&quot;</span>,<span class="st">&quot;Gender&quot;</span>,</span>
<span id="cb494-15"><a href="chapter5.html#cb494-15"></a>                 <span class="st">&quot;Age&quot;</span>,<span class="st">&quot;Household.Income&quot;</span>,<span class="st">&quot;Education&quot;</span>)], <span class="dt">sortCol=</span>careabout,</span>
<span id="cb494-16"><a href="chapter5.html#cb494-16"></a>          <span class="dt">pals=</span><span class="kw">list</span>(<span class="st">&quot;BrBG&quot;</span>)) </span></code></pre></div>
<p>5.2.1. If we are interested in the variables <code>isare</code> and <code>careabout</code>,
what sort of test should we perform?</p>
<p>5.2.2. Make the appropriate plot of the results for the table relating
those two variables relative to your answer to 5.8.</p>
<p>5.2.3. Generate the contingency table
and find the expected cell counts, first “by hand” and then check them using
the output. Is the parametric procedure appropriate here? Why or why not?</p>
<p>5.2.4. Report the value of the test
statistic, its distribution under the null, the parametric p-value, and write a
decision and conclusion, making sure to address scope of inference.</p>
<p>5.2.5. Make a mosaic plot with the
standardized residuals and discuss the results. Specifically, in what way do
the is/are preferences move away from the null hypothesis for people that care
more about this?</p>
<table>
<tbody>
<tr class="odd">
<td align="left">We might be fighting a losing battle on “data is a plural word”,</td>
</tr>
<tr class="even">
<td align="left">but since we are in the group that cares a lot about this, we are going to</td>
</tr>
<tr class="odd">
<td align="left">keep trying…</td>
</tr>
</tbody>
</table>

<div class="figure"><span id="fig:Figure5-28"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-28-1.png" alt="Stacked bar chart of the close calls/not (overtakes less than or equal to 100 cm or not) by outfit." width="480" />
<p class="caption">
Figure 5.28: Stacked bar chart of the close calls/not (overtakes less than or equal to 100 cm or not) by outfit.
</p>
</div>
<p>5.3. <strong>Overtake close calls by outfit analysis</strong> We can revisit the car overtake passing distance data from Chapter <a href="chapter3.html#chapter3">3</a> and to focus in on the “close calls”. The following code uses the <code>ifelse</code> function to create the close call/not response variable. It works to create a two-category variable where the first category (<em>close</em>) is encountered when the condition is true (<code>dd$Distance&lt;=100</code>, so the passing distance was less than or equal to 100 cm) from the “if” part of the function (<em>if Distance is less than or equal to 100 cm, then “close”</em>) and the “else” is the second category (when the <code>Distance</code> was over 100 cm) and gets the category of <em>notclose</em>. The <code>factor</code> function is applied to the results from <code>ifelse</code> to make this a categorical variable for later use. Some useful code and a stacked bar chart in Figure <a href="chapter5.html#fig:Figure5-28">5.28</a> is provided.</p>
<div class="sourceCode" id="cb495"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb495-1"><a href="chapter5.html#cb495-1"></a>dd &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/Walker2014_mod.csv&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb496"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb496-1"><a href="chapter5.html#cb496-1"></a>dd<span class="op">$</span>Condition &lt;-<span class="st"> </span><span class="kw">factor</span>(dd<span class="op">$</span>Condition)</span>
<span id="cb496-2"><a href="chapter5.html#cb496-2"></a>dd<span class="op">$</span>Condition2 &lt;-<span class="st"> </span><span class="kw">with</span>(dd, <span class="kw">reorder</span>(Condition, Distance, mean))</span>
<span id="cb496-3"><a href="chapter5.html#cb496-3"></a>dd<span class="op">$</span>Close &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">ifelse</span>(dd<span class="op">$</span>Distance<span class="op">&lt;=</span><span class="dv">100</span>, <span class="st">&quot;close&quot;</span>, <span class="st">&quot;notclose&quot;</span>))</span>
<span id="cb496-4"><a href="chapter5.html#cb496-4"></a></span>
<span id="cb496-5"><a href="chapter5.html#cb496-5"></a><span class="kw">plot</span>(Close <span class="op">~</span><span class="st"> </span>Condition2, <span class="dt">data=</span>dd)</span></code></pre></div>
<div class="sourceCode" id="cb497"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb497-1"><a href="chapter5.html#cb497-1"></a>table1 &lt;-<span class="st"> </span><span class="kw">tally</span>(Close <span class="op">~</span><span class="st"> </span>Condition2, <span class="dt">data=</span>dd)</span>
<span id="cb497-2"><a href="chapter5.html#cb497-2"></a></span>
<span id="cb497-3"><a href="chapter5.html#cb497-3"></a><span class="kw">chisq.test</span>(table1)</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  table1
## X-squared = 30.861, df = 6, p-value = 2.695e-05</code></pre>
<p>5.3.1. This is a Homogeneity test situation. Why?</p>
<p>5.3.2. Perform the 6+ steps of the hypothesis test using the provided results.</p>
<p>5.3.3. Explain how these results are consistent with the One-Way ANOVA test but also address a different research question.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Dayton1998">
<p>Dayton, C. Mitchell. 1998. <em>Latent Class Scaling Analysis</em>. Thousand Oaks, CA: SAGE Publications.</p>
</div>
<div id="ref-R-poLCA">
<p>Linzer, Drew, and Jeffrey Lewis. 2014. <em>PoLCA: Polytomous Variable Latent Class Analysis</em>. <a href="https://CRAN.R-project.org/package=poLCA">https://CRAN.R-project.org/package=poLCA</a>.</p>
</div>
<div id="ref-Linzer2011">
<p>Linzer, Drew, and Jeffrey Lewis. 2011. “PoLCA: An R Package for Polytomous Variable Latent Class Analysis.” <em>Journal of Statistical Software</em> 42 (10): 1–29.</p>
</div>
<div id="ref-R-survey">
<p>Lumley, Thomas. 2019. <em>Survey: Analysis of Complex Survey Samples</em>. <a href="https://CRAN.R-project.org/package=survey">https://CRAN.R-project.org/package=survey</a>.</p>
</div>
<div id="ref-R-vcd">
<p>Meyer, David, Achim Zeileis, and Kurt Hornik. 2017. <em>Vcd: Visualizing Categorical Data</em>. <a href="https://CRAN.R-project.org/package=vcd">https://CRAN.R-project.org/package=vcd</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="86">
<li id="fn86"><p>While randomization is typically useful in trying to “equalize” the composition of groups, a possible randomization of subjects to the groups is to put all the males into the treatment group. Sometimes we add additional constraints to randomization of subjects to treatments to guarantee that we don’t get stuck with an unusual and highly unlikely assignment like that. It is important at least to check the demographics of different treatment groups to see if anything odd occurred.<a href="chapter5.html#fnref86" class="footnote-back">↩︎</a></p></li>
<li id="fn89"><p>Note that in smaller data sets to get results as discussed here, use the <code>correct=F</code> option. If you get output that contains “<code>...with Yate's continuity correction</code>”, a slightly modified version of this test is being used.<a href="chapter5.html#fnref89" class="footnote-back">↩︎</a></p></li>
<li id="fn91"><p>Doctors are faced with this exact dilemma – with little more training than you have now in statistics, they read a result like this in a paper and used to be encouraged to focus on the p-value to decide about treatment recommendations. Would you recommend the treatment here just based on the small p-value? Would having Figure <a href="chapter5.html#fig:Figure5-11">5.11</a> to go with the small p-value help you make a more educated decision? Now the recommendations are starting to move past just focusing on the p-values and thinking about the practical importance and size of the differences. The potential benefits of a treatment need to be balanced with risks of complications too, but that takes us back into discussing having multiple analyses in the same study (treatment improvement, complications/not, etc.).<a href="chapter5.html#fnref91" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter4.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter6.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Greenwood_Book.pdf"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
