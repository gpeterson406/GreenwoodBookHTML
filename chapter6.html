<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Correlation and Simple Linear Regression | Intermediate Statistics with R</title>
  <meta name="description" content="Chapter 6 Correlation and Simple Linear Regression | Intermediate Statistics with R" />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Correlation and Simple Linear Regression | Intermediate Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="gpeterson406/Greenwood_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Correlation and Simple Linear Regression | Intermediate Statistics with R" />
  
  
  

<meta name="author" content="Mark C Greenwood" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter5.html"/>
<link rel="next" href="chapter7.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="font-awesome.min.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intermediate Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#section1-1"><i class="fa fa-check"></i><b>1.1</b> Overview of methods</a></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#section1-2"><i class="fa fa-check"></i><b>1.2</b> Getting started in R</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#section1-3"><i class="fa fa-check"></i><b>1.3</b> Basic summary statistics, histograms, and boxplots using R</a></li>
<li class="chapter" data-level="1.4" data-path="chapter1.html"><a href="chapter1.html#section1-4"><i class="fa fa-check"></i><b>1.4</b> Chapter summary</a></li>
<li class="chapter" data-level="1.5" data-path="chapter1.html"><a href="chapter1.html#section1-5"><i class="fa fa-check"></i><b>1.5</b> Summary of important R code</a></li>
<li class="chapter" data-level="1.6" data-path="chapter1.html"><a href="chapter1.html#section1-6"><i class="fa fa-check"></i><b>1.6</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> (R)e-Introduction to statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#section2-1"><i class="fa fa-check"></i><b>2.1</b> Histograms, boxplots, and density curves</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#section2-2"><i class="fa fa-check"></i><b>2.2</b> Pirate-plots</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#section2-3"><i class="fa fa-check"></i><b>2.3</b> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#section2-4"><i class="fa fa-check"></i><b>2.4</b> Permutation testing for the two sample mean situation</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#section2-5"><i class="fa fa-check"></i><b>2.5</b> Hypothesis testing (general)</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#section2-6"><i class="fa fa-check"></i><b>2.6</b> Connecting randomization (nonparametric) and parametric tests</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#section2-7"><i class="fa fa-check"></i><b>2.7</b> Second example of permutation tests</a></li>
<li class="chapter" data-level="2.8" data-path="chapter2.html"><a href="chapter2.html#section2-8"><i class="fa fa-check"></i><b>2.8</b> Reproducibility Crisis: Moving beyond p &lt; 0.05, publication bias, and multiple testing issues</a></li>
<li class="chapter" data-level="2.9" data-path="chapter2.html"><a href="chapter2.html#section2-9"><i class="fa fa-check"></i><b>2.9</b> Confidence intervals and bootstrapping</a></li>
<li class="chapter" data-level="2.10" data-path="chapter2.html"><a href="chapter2.html#section2-10"><i class="fa fa-check"></i><b>2.10</b> Bootstrap confidence intervals for difference in GPAs</a></li>
<li class="chapter" data-level="2.11" data-path="chapter2.html"><a href="chapter2.html#section2-11"><i class="fa fa-check"></i><b>2.11</b> Chapter summary</a></li>
<li class="chapter" data-level="2.12" data-path="chapter2.html"><a href="chapter2.html#section2-12"><i class="fa fa-check"></i><b>2.12</b> Summary of important R code</a></li>
<li class="chapter" data-level="2.13" data-path="chapter2.html"><a href="chapter2.html#section2-13"><i class="fa fa-check"></i><b>2.13</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> One-Way ANOVA</a><ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#section3-1"><i class="fa fa-check"></i><b>3.1</b> Situation</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#section3-2"><i class="fa fa-check"></i><b>3.2</b> Linear model for One-Way ANOVA (cell means and reference-coding)</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#section3-3"><i class="fa fa-check"></i><b>3.3</b> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#section3-4"><i class="fa fa-check"></i><b>3.4</b> ANOVA model diagnostics including QQ-plots</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#section3-5"><i class="fa fa-check"></i><b>3.5</b> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#section3-6"><i class="fa fa-check"></i><b>3.6</b> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li class="chapter" data-level="3.7" data-path="chapter3.html"><a href="chapter3.html#section3-7"><i class="fa fa-check"></i><b>3.7</b> Pair-wise comparisons for the Overtake data</a></li>
<li class="chapter" data-level="3.8" data-path="chapter3.html"><a href="chapter3.html#section3-8"><i class="fa fa-check"></i><b>3.8</b> Chapter summary</a></li>
<li class="chapter" data-level="3.9" data-path="chapter3.html"><a href="chapter3.html#section3-9"><i class="fa fa-check"></i><b>3.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="3.10" data-path="chapter3.html"><a href="chapter3.html#section3-10"><i class="fa fa-check"></i><b>3.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Two-Way ANOVA</a><ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#section4-1"><i class="fa fa-check"></i><b>4.1</b> Situation</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#section4-2"><i class="fa fa-check"></i><b>4.2</b> Designing a two-way experiment and visualizing results</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#section4-3"><i class="fa fa-check"></i><b>4.3</b> Two-Way ANOVA models and hypothesis tests</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#section4-4"><i class="fa fa-check"></i><b>4.4</b> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#section4-5"><i class="fa fa-check"></i><b>4.5</b> Observational study example: The Psychology of Debt</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#section4-6"><i class="fa fa-check"></i><b>4.6</b> Pushing Two-Way ANOVA to the limit: Un-replicated designs and Estimability</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#section4-7"><i class="fa fa-check"></i><b>4.7</b> Chapter summary</a></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#section4-8"><i class="fa fa-check"></i><b>4.8</b> Summary of important R code</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#section4-9"><i class="fa fa-check"></i><b>4.9</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Chi-square tests</a><ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#section5-1"><i class="fa fa-check"></i><b>5.1</b> Situation, contingency tables, and tableplots</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#section5-2"><i class="fa fa-check"></i><b>5.2</b> Homogeneity test hypotheses</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#section5-3"><i class="fa fa-check"></i><b>5.3</b> Independence test hypotheses</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#section5-4"><i class="fa fa-check"></i><b>5.4</b> Models for R by C tables</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#section5-5"><i class="fa fa-check"></i><b>5.5</b> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#section5-6"><i class="fa fa-check"></i><b>5.6</b> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#section5-7"><i class="fa fa-check"></i><b>5.7</b> Examining residuals for the source of differences</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#section5-8"><i class="fa fa-check"></i><b>5.8</b> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#section5-9"><i class="fa fa-check"></i><b>5.9</b> Political party and voting results: Complete analysis</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#section5-10"><i class="fa fa-check"></i><b>5.10</b> Is cheating and lying related in students?</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#section5-11"><i class="fa fa-check"></i><b>5.11</b> Analyzing a stratified random sample of California schools</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#section5-12"><i class="fa fa-check"></i><b>5.12</b> Chapter summary</a></li>
<li class="chapter" data-level="5.13" data-path="chapter5.html"><a href="chapter5.html#section5-13"><i class="fa fa-check"></i><b>5.13</b> Summary of important R commands</a></li>
<li class="chapter" data-level="5.14" data-path="chapter5.html"><a href="chapter5.html#section5-14"><i class="fa fa-check"></i><b>5.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Correlation and Simple Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#section6-1"><i class="fa fa-check"></i><b>6.1</b> Relationships between two quantitative variables</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#section6-2"><i class="fa fa-check"></i><b>6.2</b> Estimating the correlation coefficient</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#section6-3"><i class="fa fa-check"></i><b>6.3</b> Relationships between variables by groups</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#section6-4"><i class="fa fa-check"></i><b>6.4</b> Inference for the correlation coefficient</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#section6-5"><i class="fa fa-check"></i><b>6.5</b> Are tree diameters related to tree heights?</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#section6-6"><i class="fa fa-check"></i><b>6.6</b> Describing relationships with a regression model</a></li>
<li class="chapter" data-level="6.7" data-path="chapter6.html"><a href="chapter6.html#section6-7"><i class="fa fa-check"></i><b>6.7</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.8" data-path="chapter6.html"><a href="chapter6.html#section6-8"><i class="fa fa-check"></i><b>6.8</b> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li class="chapter" data-level="6.9" data-path="chapter6.html"><a href="chapter6.html#section6-9"><i class="fa fa-check"></i><b>6.9</b> Outliers: leverage and influence</a></li>
<li class="chapter" data-level="6.10" data-path="chapter6.html"><a href="chapter6.html#section6-10"><i class="fa fa-check"></i><b>6.10</b> Residual diagnostics – setting the stage for inference</a></li>
<li class="chapter" data-level="6.11" data-path="chapter6.html"><a href="chapter6.html#section6-11"><i class="fa fa-check"></i><b>6.11</b> Old Faithful discharge and waiting times</a></li>
<li class="chapter" data-level="6.12" data-path="chapter6.html"><a href="chapter6.html#section6-12"><i class="fa fa-check"></i><b>6.12</b> Chapter summary</a></li>
<li class="chapter" data-level="6.13" data-path="chapter6.html"><a href="chapter6.html#section6-13"><i class="fa fa-check"></i><b>6.13</b> Summary of important R code</a></li>
<li class="chapter" data-level="6.14" data-path="chapter6.html"><a href="chapter6.html#section6-14"><i class="fa fa-check"></i><b>6.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Simple linear regression inference</a><ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#section7-1"><i class="fa fa-check"></i><b>7.1</b> Model</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#section7-2"><i class="fa fa-check"></i><b>7.2</b> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#section7-3"><i class="fa fa-check"></i><b>7.3</b> Bozeman temperature trend</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#section7-4"><i class="fa fa-check"></i><b>7.4</b> Randomization-based inferences for the slope coefficient</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#section7-5"><i class="fa fa-check"></i><b>7.5</b> Transformations part I: Linearizing relationships</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#section7-6"><i class="fa fa-check"></i><b>7.6</b> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#section7-7"><i class="fa fa-check"></i><b>7.7</b> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li class="chapter" data-level="7.8" data-path="chapter7.html"><a href="chapter7.html#section7-8"><i class="fa fa-check"></i><b>7.8</b> Chapter summary</a></li>
<li class="chapter" data-level="7.9" data-path="chapter7.html"><a href="chapter7.html#section7-9"><i class="fa fa-check"></i><b>7.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="7.10" data-path="chapter7.html"><a href="chapter7.html#section7-10"><i class="fa fa-check"></i><b>7.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#section8-1"><i class="fa fa-check"></i><b>8.1</b> Going from SLR to MLR</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#section8-2"><i class="fa fa-check"></i><b>8.2</b> Validity conditions in MLR</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#section8-3"><i class="fa fa-check"></i><b>8.3</b> Interpretation of MLR terms</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#section8-4"><i class="fa fa-check"></i><b>8.4</b> Comparing multiple regression models</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#section8-5"><i class="fa fa-check"></i><b>8.5</b> General recommendations for MLR interpretations and VIFs</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#section8-6"><i class="fa fa-check"></i><b>8.6</b> MLR inference: Parameter inferences using the t-distribution</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#section8-7"><i class="fa fa-check"></i><b>8.7</b> Overall F-test in multiple linear regression</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#section8-8"><i class="fa fa-check"></i><b>8.8</b> Case study: First year college GPA and SATs</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#section8-9"><i class="fa fa-check"></i><b>8.9</b> Different intercepts for different groups: MLR with indicator variables</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#section8-10"><i class="fa fa-check"></i><b>8.10</b> Additive MLR with more than two groups: Headache example</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#section8-11"><i class="fa fa-check"></i><b>8.11</b> Different slopes and different intercepts</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#section8-12"><i class="fa fa-check"></i><b>8.12</b> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#section8-13"><i class="fa fa-check"></i><b>8.13</b> AICs for model selection</a></li>
<li class="chapter" data-level="8.14" data-path="chapter8.html"><a href="chapter8.html#section8-14"><i class="fa fa-check"></i><b>8.14</b> Case study: Forced expiratory volume model selection using AICs</a></li>
<li class="chapter" data-level="8.15" data-path="chapter8.html"><a href="chapter8.html#section8-15"><i class="fa fa-check"></i><b>8.15</b> Chapter summary</a></li>
<li class="chapter" data-level="8.16" data-path="chapter8.html"><a href="chapter8.html#section8-16"><i class="fa fa-check"></i><b>8.16</b> Summary of important R code</a></li>
<li class="chapter" data-level="8.17" data-path="chapter8.html"><a href="chapter8.html#section8-17"><i class="fa fa-check"></i><b>8.17</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Case studies</a><ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#section9-1"><i class="fa fa-check"></i><b>9.1</b> Overview of material covered</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#section9-2"><i class="fa fa-check"></i><b>9.2</b> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#section9-3"><i class="fa fa-check"></i><b>9.3</b> Ants learn to rely on more informative attributes during decision-making</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#section9-4"><i class="fa fa-check"></i><b>9.4</b> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#section9-5"><i class="fa fa-check"></i><b>9.5</b> What do didgeridoos really do about sleepiness?</a></li>
<li class="chapter" data-level="9.6" data-path="chapter9.html"><a href="chapter9.html#section9-6"><i class="fa fa-check"></i><b>9.6</b> General summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Intermediate Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter6" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Correlation and Simple Linear Regression</h1>
<div id="section6-1" class="section level2">
<h2><span class="header-section-number">6.1</span> Relationships between two quantitative variables</h2>
<p>The independence test in Chapter <a href="chapter5.html#chapter5">5</a> provided a technique for assessing evidence of a
relationship between two categorical variables. The terms <strong><em>relationship</em></strong> and <strong><em>association</em></strong>
are synonyms that, in statistics, imply that particular values on one variable tend to occur more often with
some other values of the other variable or that knowing
something about the level of one variable provides information about the
patterns of values on the other variable. These terms are not specific to the
“form” of the relationship – any pattern (strong or weak, negative or positive,
easily described or complicated) satisfy the definition. There are two other
aspects to using these terms in a statistical context. First, they are not
directional – an association between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is the same as saying there is an
association between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>. Second, they are not causal unless the levels of
one of the variables are randomly assigned in an experimental context. We add
to this terminology the idea of correlation between variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.
<strong><em>Correlation</em></strong>, in most statistical contexts, is a measure of the specific type
of relationship between the variables: the <strong>linear relationship between two
quantitative variables</strong><a href="#fn94" class="footnote-ref" id="fnref94"><sup>94</sup></a>.
So as we start to review these ideas from your previous statistics course,
remember that associations and relationships are more general than correlations
and it is possible to have no correlation where there is a strong relationship
between variables. “Correlation” is used colloquially as a synonym for
relationship but we will work to reserve it for its more specialized usage here
to refer specifically to the linear relationship.  </p>
<p>Assessing and then modeling relationships between quantitative variables drives
the rest of the chapters,
so we should get started with some motivating examples to start to think about
what relationships between quantitative variables “look like”… To motivate
these methods, we will start with a study of the effects of beer consumption on
blood alcohol levels (<em>BAC</em>, in grams of alcohol per deciliter of blood). A group
of <span class="math inline">\(n=16\)</span> student volunteers at The Ohio State University drank a
randomly assigned number of beers<a href="#fn95" class="footnote-ref" id="fnref95"><sup>95</sup></a>.
Thirty minutes later, a police officer measured their <em>BAC</em>. Your instincts, especially
as well-educated college students with some chemistry knowledge, should inform
you about the direction of this relationship – that there is a <strong><em>positive
relationship</em></strong> between <code>Beers</code> and <code>BAC</code>. In other words, <strong>higher values of
one variable are associated with higher values of the other</strong>. Similarly,
lower values of one are associated with lower values of the other.
In fact there are online calculators that tell you how much your <em>BAC</em> increases
for each extra beer consumed (for example:
<a href="http://www.craftbeer.com/beer-studies/blood-alcohol-content-calculator" class="uri">http://www.craftbeer.com/beer-studies/blood-alcohol-content-calculator</a>
if you plug in 1 beer). The increase
in <span class="math inline">\(y\)</span> (<code>BAC</code>) for a 1 unit increase in <span class="math inline">\(x\)</span> (here, 1 more beer) is an example of a
<strong><em>slope coefficient</em></strong> that is applicable if the relationship between the
variables is linear and something that will be
fundamental in what is called a <strong><em>simple linear regression model</em></strong>.


In a
simple linear regression model (simple means that there is only one explanatory
variable) the slope is the expected change in the mean response for a one unit
increase in the explanatory variable. You could also use the <em>BAC</em> calculator and
the models that we are going to develop to pick a total number of beers you
will consume and get a predicted <em>BAC</em>, which employs the entire equation we will estimate.</p>
<p>Before we get to the specifics of this model and how we measure correlation, we
should graphically explore the relationship between <code>Beers</code> and <code>BAC</code> in a scatterplot.
Figure <a href="chapter6.html#fig:Figure6-1">6.1</a> shows a <strong><em>scatterplot</em></strong> of the results that display
the expected positive relationship. Scatterplots display the response pairs for
the two quantitative variables with the
explanatory variable on the <span class="math inline">\(x\)</span>-axis and the response variable on the <span class="math inline">\(y\)</span>-axis. The
relationship between <code>Beers</code> and <code>BAC</code> appears to be relatively linear but
there is possibly more variability than one might expect. For example, for
students consuming 5 beers, their <em>BAC</em>s range from 0.05 to 0.10. If you look
at the online <em>BAC</em> calculators, you will see that other factors such as weight,
sex, and beer percent alcohol can impact
the results. We might also be interested in previous alcohol consumption. In
Chapter <a href="chapter8.html#chapter8">8</a>, we will learn how to estimate the relationship between
<code>Beers</code> and <code>BAC</code> after correcting or controlling for those “other variables” using
<strong><em>multiple linear regression</em></strong>, where we incorporate more than one
quantitative explanatory variable into the linear model (somewhat like in the
2-Way ANOVA).


Some of this variability might be hard or impossible to explain
regardless of the other variables available and is considered unexplained variation
and goes into the residual errors in our models, just like in the ANOVA models.
To make scatterplots as in Figure <a href="chapter6.html#fig:Figure6-1">6.1</a>, you can simply<a href="#fn96" class="footnote-ref" id="fnref96"><sup>96</sup></a> use
<code>plot(y~x, data=...)</code>.</p>
<div class="sourceCode" id="cb499"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb499-1"><a href="chapter6.html#cb499-1"></a><span class="kw">library</span>(readr)</span>
<span id="cb499-2"><a href="chapter6.html#cb499-2"></a>BB &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/beersbac.csv&quot;</span>)</span></code></pre></div>

<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb500-1"><a href="chapter6.html#cb500-1"></a><span class="kw">plot</span>(BAC<span class="op">~</span>Beers, <span class="dt">data=</span>BB, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">col=</span><span class="dv">30</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure6-1"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-1-1.png" alt="Scatterplot of Beers consumed versus BAC." width="576" />
<p class="caption">
Figure 6.1: Scatterplot of <em>Beers</em> consumed versus <em>BAC</em>.
</p>
</div>
<!-- \newpage -->
<p>There are a few general things to look for in scatterplots:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Assess the</strong> <span class="math inline">\(\underline{\textbf{direction of the relationship}}\)</span> – is it
positive or negative?</p></li>
<li><p><strong>Consider the</strong> <span class="math inline">\(\underline{\textbf{strength of the relationship}}\)</span>.
The general idea of assessing strength visually is about how hard or easy it is
to see the pattern. If it is hard to see a pattern, then it is weak. If it is easy to
see, then it is strong.</p></li>
<li><p><strong>Consider the</strong> <span class="math inline">\(\underline{\textbf{linearity of the relationship}}\)</span>. Does it
appear to curve or does it follow a relatively straight line? Curving relationships are
called <strong><em>curvilinear</em></strong> or <strong><em>nonlinear</em></strong> and can be strong or
weak just like linear relationships – it is all about how tightly the
points follow the pattern you identify.</p></li>
<li><p><strong>Check for</strong> <span class="math inline">\(\underline{\textbf{unusual observations -- outliers}}\)</span> – by looking
for points that don’t follow the overall pattern. Being large in <span class="math inline">\(x\)</span> or <span class="math inline">\(y\)</span>
doesn’t mean that the point is an outlier. Being unusual relative to the overall
pattern makes a point an outlier in this setting.</p></li>
<li><p><strong>Check for</strong> <span class="math inline">\(\underline{\textbf{changing variability}}\)</span> in one variable based
on values of the other variable. This will tie into a constant variance assumption
later in the regression models.</p></li>
<li><p><strong>Finally, look for</strong> <span class="math inline">\(\underline{\textbf{distinct groups}}\)</span> in the scatterplot.
This might suggest that observations from two populations, say males and females,
were combined but the relationship between the two quantitative variables
might be different for the two groups.</p></li>
</ol>
<p>Going back to Figure <a href="chapter6.html#fig:Figure6-1">6.1</a> it appears that there is a
moderately strong linear
relationship between <code>Beers</code> and <code>BAC</code> – not weak but with some variability
around what appears to be a fairly clear to see straight-line relationship. There might even be a
hint of a nonlinear relationship in the higher beer values. There are no clear
outliers because the observation at 9 beers seems to be following the overall
pattern fairly closely. There is little evidence of non-constant variance
mainly because of the limited size of the data set – we’ll check this with
better plots later. And there are no clearly distinct groups in this plot, possibly
because the # of beers was randomly assigned. These data have one more
interesting feature to be noted – that subjects managed to consume 8 or 9
beers. This seems to be a large number. I have never been able to trace this
data set to the original study so it is hard to know if (1) they had this study
approved by a human subjects research review board to make sure it was “safe”,
(2) every subject in the study was able to consume their randomly assigned
amount, and (3) whether subjects were asked to show up to the study with <em>BAC</em>s
of 0. We also don’t know the exact alcohol concentration of the beer consumed or
volume. So while this is a fun example to start these methods with, a better version of this data set would be nice…</p>
<p>In making scatterplots, there is always a choice of a variable for the
<span class="math inline">\(x\)</span>-axis and the <span class="math inline">\(y\)</span>-axis. It is our
convention to put explanatory or independent variables (the ones used to
explain or predict the responses) on the <span class="math inline">\(x\)</span>-axis. In studies where the subjects are
randomly assigned to levels of a variable, this is very clearly an explanatory
variable, and we can go as far as making causal inferences with it. In
observational studies, it can be less clear which variable explains which. In
these cases, make the most reasonable choice based on the observed variables but
remember that, when the direction of relationship is unclear, you could have
switched the axes and thus the implication of which variable is explanatory.</p>
</div>
<div id="section6-2" class="section level2">
<h2><span class="header-section-number">6.2</span> Estimating the correlation coefficient</h2>
<p>In terms of quantifying relationships between variables, we start with
the correlation coefficient, a
measure that is the same regardless of your choice of variables as
explanatory or response. We measure the strength and direction of
linear relationships between two quantitative variables using
<strong><em>Pearson’s r</em></strong> or <strong><em>Pearson’s Product Moment Correlation Coefficient</em></strong>.
For those who really like acronyms, Wikipedia even suggests calling it
the PPMCC. However,
its use is so ubiquitous that the lower case <strong><em>r</em></strong> or just “correlation
coefficient” are often sufficient to identify that you have used the PPMCC.
Some of the extra distinctions arise because there are other ways of measuring
correlations in other situations (for example between two categorical
variables), but we will not consider them here.</p>
<!-- \newpage -->
<p>The correlation coefficient, <strong><em>r</em></strong>, is calculated as</p>
<p><span class="math display">\[r=\frac{1}{n-1}\sum^n_{i=1}\left(\frac{x_i-\bar{x}}{s_x}\right)
\left(\frac{y_i-\bar{y}}{s_y}\right),\]</span></p>
<p>where <span class="math inline">\(s_x\)</span> and <span class="math inline">\(s_y\)</span> are the standard deviations of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. This
formula can also be written as</p>
<p><span class="math display">\[r=\frac{1}{n-1}\sum^n_{i=1}z_{x_i}z_{y_i}\]</span></p>
<p>where <span class="math inline">\(z_{x_i}\)</span> is the z-score (observation minus mean divided by
standard deviation) for the <span class="math inline">\(i^{th}\)</span> observation on <span class="math inline">\(x\)</span> and <span class="math inline">\(z_{y_i}\)</span>
is the z-score for the <span class="math inline">\(i^{th}\)</span> observation on <span class="math inline">\(y\)</span>. We won’t directly
use this formula, but its contents inform the behavior of <strong><em>r</em></strong>.
First, because it is a sum divided by (<span class="math inline">\(n-1\)</span>) it is a bit like
an average – it combines information across all observations and, like the
mean, is sensitive to outliers. Second, it is a dimension-less measure, meaning
that it has no units attached to it. It is based on z-scores which have units
of standard deviations of <span class="math inline">\(x\)</span> or <span class="math inline">\(y\)</span> so the original units of measurement are
cancelled out going into this calculation. This also means that changing the
original units of measurement, say from Fahrenheit to Celsius or from miles to
km for one or the other variable will have no impact on the correlation. Less
obviously, the formula guarantees that <strong><em>r</em></strong> is between -1 and 1. It will
attain -1 for a perfect negative linear relationship, 1 for a perfect positive
linear relationship, and 0 for no linear relationship. We are being careful
here to say <strong><em>linear relationship</em></strong> because you can have a strong nonlinear
relationship with a correlation of 0. For example, consider
Figure <a href="chapter6.html#fig:Figure6-2">6.2</a>.</p>

<div class="figure"><span id="fig:Figure6-2"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-2-1.png" alt="Scatterplot of an amusing (and strong) relationship that has \(r=0\)." width="576" />
<p class="caption">
Figure 6.2: Scatterplot of an amusing (and strong) relationship that has <span class="math inline">\(r=0\)</span>.
</p>
</div>
<p>There are some conditions for trusting the results that the
correlation coefficient provides:</p>
<ol style="list-style-type: decimal">
<li><p>Two quantitative variables measured.</p>
<ul>
<li>This might seem silly, but categorical variables can be coded
numerically and a meaningless correlation can be estimated if you
are not careful what you correlate.</li>
</ul></li>
<li><p>The relationship between the variables is relatively linear.</p>
<ul>
<li>If the relationship is nonlinear, the correlation is meaningless since it only measures linear relationships
and can be misleading if applied to a nonlinear relationship.</li>
</ul></li>
<li><p>There should be no outliers. </p>
<ul>
<li><p>The correlation is very sensitive (technically <strong><em>not resistant</em></strong>)
to the impacts of certain types of outliers and you should generally
avoid reporting the correlation when they are present.
</p></li>
<li><p>One option in the presence of outliers is to report the correlation
with and without outliers to see how they influence the estimated
correlation.</p></li>
</ul></li>
</ol>
<p>The correlation coefficient is dimensionless but larger magnitude values
(closer to -1 OR 1) mean stronger linear relationships. A rough interpretation
scale based on experiences working with correlations follows, but this varies
between fields and types of research and variables measured. It depends on the
levels of correlation researchers become used to obtaining, so can even vary
within fields. Use this scale until you develop your own experience:</p>
<ul>
<li><p><span class="math inline">\(\left|\boldsymbol{r}\right|&lt;0.3\)</span>: weak linear relationship,</p></li>
<li><p><span class="math inline">\(0.3 &lt; \left|\boldsymbol{r}\right|&lt;0.7\)</span>: moderate linear relationship,</p></li>
<li><p><span class="math inline">\(0.7 &lt; \left|\boldsymbol{r}\right|&lt;0.9\)</span>: strong linear relationship, and</p></li>
<li><p><span class="math inline">\(0.9 &lt; \left|\boldsymbol{r}\right|&lt;1.0\)</span>: very strong linear relationship.</p></li>
</ul>
<p>And again note that this scale only relates to the <strong>linear</strong> aspect of
the relationship between the variables.</p>
<p>When we have linear relationships between two quantitative variables,
<span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, we can obtain estimated correlations from the <code>cor</code>
function either using <code>y~x</code> or by running the <code>cor</code> function<a href="#fn97" class="footnote-ref" id="fnref97"><sup>97</sup></a> on the entire data set. When you run the <code>cor</code>
function on a data set it produces a <strong><em>correlation matrix</em></strong> which
contains a matrix of correlations where you can triangulate the
variables being correlated by the row and column names, noting
that the correlation between a variable and itself is 1. A matrix of
correlations is useful for comparing more than two variables, discussed below.  </p>
<div class="sourceCode" id="cb501"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb501-1"><a href="chapter6.html#cb501-1"></a><span class="kw">library</span>(mosaic)</span>
<span id="cb501-2"><a href="chapter6.html#cb501-2"></a><span class="kw">cor</span>(BAC<span class="op">~</span>Beers, <span class="dt">data=</span>BB)</span></code></pre></div>
<pre><code>## [1] 0.8943381</code></pre>
<div class="sourceCode" id="cb503"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb503-1"><a href="chapter6.html#cb503-1"></a><span class="kw">cor</span>(BB)</span></code></pre></div>
<pre><code>##           Beers       BAC
## Beers 1.0000000 0.8943381
## BAC   0.8943381 1.0000000</code></pre>
<p>Based on either version of using the function, we find that the correlation
between <code>Beers</code> and <code>BAC</code> is estimated to be 0.89. This suggests a
strong linear relationship between the
two variables. Examples are about the only way to build up enough experience to
become skillful in using the correlation coefficient. Some additional
complications arise in more complicated studies as the next example
demonstrates.</p>
<p><span class="citation">Gude et al. (<a href="#ref-Gude2009" role="doc-biblioref">2009</a>)</span> explored the relationship
between average summer
temperature (degrees F) and area burned (natural log of hectares<a href="#fn98" class="footnote-ref" id="fnref98"><sup>98</sup></a> = log(hectares)) by wildfires in Montana
from 1985 to 2007. The <strong><em>log-transformation</em></strong> is often used to reduce
the impacts of really large observations with
non-negative (strictly greater than 0) variables
(more on <strong><em>transformations</em></strong> and their impacts on regression models
in Chapter <a href="chapter7.html#chapter7">7</a>).

Based on your experiences with the wildfire “season” and before
analyzing the data, I’m sure
you would assume that summer temperature explains the area burned by wildfires.
But could it be that more fires are related to having warmer summers? That
second direction is unlikely on a state-wide scale but could apply at a
particular weather station that is near a fire. There is another option – some
other variable is affecting both variables. For example, drier summers might
be the real explanatory variable that is related to having both warm summers and lots
of fires. These variables are also being measured over time making them examples
of <strong><em>time series</em></strong>. In
this situation, if there are changes over time, they might be attributed to
climate change. So there are really three relationships to explore with the
variables measured here (remembering that the full story might require
measuring even more!): log-area burned versus temperature, temperature versus
year, and log-area burned versus year.    </p>
<p>With more than two variables, we can use the <code>cor</code> function on all the
variables and end up getting a matrix of correlations or, simply, the
<strong><em>correlation matrix</em></strong>.  If you triangulate the row and column labels, that cell provides the correlation between that pair of variables. For example, in the first row (<code>Year</code>)
and the last column (<code>loghectares</code>), you can find that the correlation
coefficient is <strong><em>r</em></strong>=0.362. Note the symmetry in the matrix around the
diagonal of 1’s – this further illustrates that correlation between
<span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> does not depend on which variable is viewed as the “response”.
The estimated correlation
between <code>Temperature</code> and <code>Year</code> is -0.004 and the correlation between
<code>loghectares</code> (<em>log-hectares burned</em>) and <code>Temperature</code> is 0.81. So
<code>Temperature</code> has almost no linear
change over time. And there is a strong linear relationship between
<code>loghectares</code> and <code>Temperature</code>. So it appears that temperatures may
be related to log-area burned but that the trend over time in both is less
clear (at least the linear trends).</p>
<div class="sourceCode" id="cb505"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb505-1"><a href="chapter6.html#cb505-1"></a>mtfires &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/climateR2.csv&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb506-1"><a href="chapter6.html#cb506-1"></a><span class="co"># natural log transformation of area burned</span></span>
<span id="cb506-2"><a href="chapter6.html#cb506-2"></a>mtfires<span class="op">$</span>loghectares &lt;-<span class="st"> </span><span class="kw">log</span>(mtfires<span class="op">$</span>hectares) </span>
<span id="cb506-3"><a href="chapter6.html#cb506-3"></a></span>
<span id="cb506-4"><a href="chapter6.html#cb506-4"></a><span class="co">#Cuts the original hectares data so only log-scale version in tibble</span></span>
<span id="cb506-5"><a href="chapter6.html#cb506-5"></a>mtfiresR &lt;-<span class="st"> </span>mtfires[,<span class="op">-</span><span class="dv">3</span>] </span>
<span id="cb506-6"><a href="chapter6.html#cb506-6"></a><span class="kw">cor</span>(mtfiresR)</span></code></pre></div>
<pre><code>##                   Year Temperature loghectares
## Year         1.0000000  -0.0037991   0.3617789
## Temperature -0.0037991   1.0000000   0.8135947
## loghectares  0.3617789   0.8135947   1.0000000</code></pre>
<p>The correlation matrix alone is misleading – we need to explore scatterplots
to check for nonlinear
relationships, outliers, and clustering of observations that may be distorting
the numerical measure of the linear relationship.  The <code>pairs.panels</code>
function from the <code>psych</code> package <span class="citation">(Revelle <a href="#ref-R-psych" role="doc-biblioref">2019</a>)</span> combines the numerical
correlation information and scatterplots in one display.

There are
some options to turn off for the moment but it is an easy function to use to
get lots of information in one place. As in the correlation matrix, you
triangulate the variables for the pairwise relationship. The upper right
panel of Figure <a href="chapter6.html#fig:Figure6-3">6.3</a> displays a correlation of 0.36 for
<code>Year</code> and <code>loghectares</code> and the lower left panel contains the
scatterplot with <code>Year</code> on the <span class="math inline">\(x\)</span>-axis and <code>loghectares</code> on the <span class="math inline">\(y\)</span>-axis.
The correlation between <code>Year</code> and <code>Temperature</code> is really small, both
in magnitude and in display, but appears to be nonlinear (it goes down between
1985 and 1995 and then goes back up), so the correlation coefficient doesn’t
mean much here since it just measures the overall linear relationship. We might
say that this is a moderate strength (moderately “clear”) curvilinear
relationship. In terms of the underlying climate process, it suggests a
decrease in summer temperatures between 1985 and 1995 and then an increase in
the second half of the data set.</p>

<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb508-1"><a href="chapter6.html#cb508-1"></a><span class="kw">library</span>(psych) </span>
<span id="cb508-2"><a href="chapter6.html#cb508-2"></a><span class="kw">pairs.panels</span>(mtfiresR, <span class="dt">ellipses=</span>F, <span class="dt">scale=</span>T, <span class="dt">smooth=</span>F, <span class="dt">col=</span><span class="dv">0</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure6-3"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-3-1.png" alt="Scatterplot matrix of Montana fires data." width="576" />
<p class="caption">
Figure 6.3: Scatterplot matrix of Montana fires data.
</p>
</div>
<p>As one more example, the Australian Institute of Sport collected data
on 102 male and 100 female athletes that are available in the <code>ais</code>
data set from the <code>alr3</code> package (<span class="citation">Weisberg (<a href="#ref-R-alr3" role="doc-biblioref">2018</a>)</span>, <span class="citation">Weisberg (<a href="#ref-Weisberg2005" role="doc-biblioref">2005</a>)</span>).

They measured a
variety of variables including the athlete’s Hematocrit (<code>Hc</code>,
units of percentage of red blood cells in the blood), Body Fat Percentage
(<code>Bfat</code>, units of percentage of total body weight), and height (<code>Ht</code>,
units of cm). Eventually we might be interested in predicting <code>Hc</code>
based on the other variables, but for now the associations are of interest.</p>

<div class="sourceCode" id="cb509"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb509-1"><a href="chapter6.html#cb509-1"></a><span class="kw">library</span>(alr3)</span>
<span id="cb509-2"><a href="chapter6.html#cb509-2"></a><span class="kw">data</span>(ais)</span>
<span id="cb509-3"><a href="chapter6.html#cb509-3"></a><span class="kw">library</span>(tibble)</span>
<span id="cb509-4"><a href="chapter6.html#cb509-4"></a>ais &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(ais)</span>
<span id="cb509-5"><a href="chapter6.html#cb509-5"></a>aisR &lt;-<span class="st"> </span>ais[,<span class="kw">c</span>(<span class="st">&quot;Ht&quot;</span>,<span class="st">&quot;Hc&quot;</span>,<span class="st">&quot;Bfat&quot;</span>)]</span>
<span id="cb509-6"><a href="chapter6.html#cb509-6"></a><span class="kw">summary</span>(aisR)</span></code></pre></div>
<pre><code>##        Ht              Hc             Bfat       
##  Min.   :148.9   Min.   :35.90   Min.   : 5.630  
##  1st Qu.:174.0   1st Qu.:40.60   1st Qu.: 8.545  
##  Median :179.7   Median :43.50   Median :11.650  
##  Mean   :180.1   Mean   :43.09   Mean   :13.507  
##  3rd Qu.:186.2   3rd Qu.:45.58   3rd Qu.:18.080  
##  Max.   :209.4   Max.   :59.70   Max.   :35.520</code></pre>
<div class="sourceCode" id="cb511"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb511-1"><a href="chapter6.html#cb511-1"></a><span class="kw">pairs.panels</span>(aisR, <span class="dt">scale=</span>T, <span class="dt">ellipse=</span>F, <span class="dt">smooth=</span>F, <span class="dt">col=</span><span class="dv">0</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure6-4"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-4-1.png" alt="Scatterplot matrix of athlete data." width="576" />
<p class="caption">
Figure 6.4: Scatterplot matrix of athlete data.
</p>
</div>
<div class="sourceCode" id="cb512"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb512-1"><a href="chapter6.html#cb512-1"></a><span class="kw">cor</span>(aisR)</span></code></pre></div>
<pre><code>##              Ht         Hc       Bfat
## Ht    1.0000000  0.3711915 -0.1880217
## Hc    0.3711915  1.0000000 -0.5324491
## Bfat -0.1880217 -0.5324491  1.0000000</code></pre>
<p><code>Ht</code> (<em>Height</em>) and <code>Hc</code> (<em>Hematocrit</em>) have a moderate positive
relationship that may contain a slight nonlinearity. It also contains one
clear outlier for a middle height athlete (around 175 cm) with an <code>Hc</code>
of close to 60% (a result that is extremely high). One might wonder about
whether this athlete has been doping or
if that measurement involved a recording error. We should consider removing
that observation to see how our results might change without it impacting the
results. For the relationship between <code>Bfat</code> (<em>body fat</em>) and <code>Hc</code>
(<em>hematocrit</em>), that same high <code>Hc</code> value is a clear outlier. There is
also a high <code>Bfat</code> (<em>body fat</em>) athlete (35%) with a somewhat low
<code>Hc</code> value. This also might be influencing our impressions so we will
remove both “unusual” values and remake the plot. The two offending
observations were found for individuals numbered 56 and 166 in the data set:</p>
<div class="sourceCode" id="cb514"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb514-1"><a href="chapter6.html#cb514-1"></a>aisR[<span class="kw">c</span>(<span class="dv">56</span>,<span class="dv">166</span>),]</span></code></pre></div>
<pre><code>## # A tibble: 2 x 3
##      Ht    Hc  Bfat
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  180.  37.6 35.5 
## 2  175.  59.7  9.56</code></pre>
<p>We can create a reduced version of the data (<code>aisR2</code>) by removing those
two rows using <code>[-c(56, 166),]</code> and then remake the plot:</p>

<div class="sourceCode" id="cb516"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb516-1"><a href="chapter6.html#cb516-1"></a>aisR2 &lt;-<span class="st"> </span>aisR[<span class="op">-</span><span class="kw">c</span>(<span class="dv">56</span>,<span class="dv">166</span>),] <span class="co">#Removes observations in rows 56 and 166</span></span>
<span id="cb516-2"><a href="chapter6.html#cb516-2"></a><span class="kw">pairs.panels</span>(aisR2, <span class="dt">scale=</span>T, <span class="dt">ellipse=</span>F, <span class="dt">smooth=</span>F, <span class="dt">col=</span><span class="dv">0</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure6-5"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-5-1.png" alt="Scatterplot matrix of athlete data with two potential outliers removed. " width="576" />
<p class="caption">
Figure 6.5: Scatterplot matrix of athlete data with two potential outliers removed. 
</p>
</div>
<p>After removing these two unusual observations, the relationships between
the variables are more obvious (Figure <a href="chapter6.html#fig:Figure6-5">6.5</a>). There is a
moderate strength, relatively linear relationship between <em>Height</em> and
<em>Hematocrit</em>. There is almost no relationship between <em>Height</em> and
<em>Body Fat %</em> <span class="math inline">\((\boldsymbol{r}=-0.20)\)</span>. There is a negative, moderate strength,
somewhat curvilinear relationship between <em>Hematocrit</em> and <em>Body Fat %</em>
<span class="math inline">\((\boldsymbol{r}=-0.54)\)</span>. As hematocrit increases initially, the body fat
percentage decreases but at a certain level (around 45% for <code>Hc</code>), the
body fat percentage seems to
level off. Interestingly, it ended up that removing those two outliers had only
minor impacts on the estimated correlations – this will not always be the case.</p>
<p>Sometimes we want to just be able to focus on the correlations, assuming
we trust that
the correlation is a reasonable description of the results between the
variables. To make it easier to see patterns of positive and negative
correlations, we can employ a different version of the same display from
the <code>corrplot</code> package <span class="citation">(Wei and Simko <a href="#ref-R-corrplot" role="doc-biblioref">2017</a>)</span> with the <code>corrplot.mixed</code> function. 

In this case
(Figure <a href="chapter6.html#fig:Figure6-6">6.6</a>), it tells much the same story but also allows
the viewer to easily distinguish both size and direction and read off the
numerical correlations if desired. </p>

<div class="sourceCode" id="cb517"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb517-1"><a href="chapter6.html#cb517-1"></a><span class="kw">library</span>(corrplot)</span>
<span id="cb517-2"><a href="chapter6.html#cb517-2"></a><span class="kw">corrplot.mixed</span>(<span class="kw">cor</span>(aisR2), <span class="dt">upper.col=</span><span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;orange&quot;</span>),<span class="dt">lower.col=</span><span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;orange&quot;</span>))</span></code></pre></div>
<div class="figure"><span id="fig:Figure6-6"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-6-1.png" alt="Correlation plot of the athlete data with two potential outliers removed. Lighter (orange) circle for positive correlations and black for negative correlations." width="336" />
<p class="caption">
Figure 6.6: Correlation plot of the athlete data with two potential outliers removed. Lighter (orange) circle for positive correlations and black for negative correlations.
</p>
</div>
</div>
<div id="section6-3" class="section level2">
<h2><span class="header-section-number">6.3</span> Relationships between variables by groups</h2>
<p>In assessing the relationship
between variables, incorporating information from a third variable can often
enhance the information gathered by either showing that the relationship
between the first two variables is the same across levels of the other variable
or showing that it differs. When the other variable is categorical (or just can
be made categorical), it can be added to scatterplots, changing the symbols and
colors for the points based on the different groups. These techniques are
especially useful if the categorical variable corresponds to potentially
distinct groups in the responses. In the previous example, the data set was
built with male and female athletes. For some characteristics, the
relationships might be the same for both sexes but for others, there are likely
some physiological differences to consider.</p>
<p>We could continue to use the <code>plot</code> function here, but it would require
additional lines of code to add these extra features. The <code>scatterplot</code>
function from the <code>car</code> package (<span class="citation">Fox, Weisberg, and Price (<a href="#ref-R-carData" role="doc-biblioref">2019</a><a href="#ref-R-carData" role="doc-biblioref">b</a>)</span>, <span class="citation">Fox and Weisberg (<a href="#ref-Fox2011" role="doc-biblioref">2011</a>)</span>) makes it easy
to incorporate information from an
additional categorical variable. We’ll add to our regular formula idea (<code>y~x</code>)
the vertical line “|” followed by the categorical variable <code>z</code>, such as
<code>y~x|z</code>. As noted earlier, in statistics, “|” means “to condition on” or,
here, consider the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> by groups in <span class="math inline">\(z\)</span>. The other
options are mainly to make it easier to read the information in the plot…
Using this enhanced notation, Figure <a href="chapter6.html#fig:Figure6-7">6.7</a> displays the
<em>Height</em> and <em>Hematocrit</em> relationship with information on the sex of the
athletes where sex was coded 0 for males and 1 for females. </p>

<div class="figure"><span id="fig:Figure6-7"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-7-1.png" alt="Scatterplot of athlete’s height and hematocrit by sex of athletes. Males were coded as 0s and females as 1s." width="576" />
<p class="caption">
Figure 6.7: Scatterplot of athlete’s height and hematocrit by sex of athletes. Males were coded as 0s and females as 1s.
</p>
</div>
<div class="sourceCode" id="cb518"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb518-1"><a href="chapter6.html#cb518-1"></a>aisR2 &lt;-<span class="st"> </span>ais[<span class="op">-</span><span class="kw">c</span>(<span class="dv">56</span>,<span class="dv">166</span>),<span class="kw">c</span>(<span class="st">&quot;Ht&quot;</span>,<span class="st">&quot;Hc&quot;</span>,<span class="st">&quot;Bfat&quot;</span>,<span class="st">&quot;Sex&quot;</span>)]</span>
<span id="cb518-2"><a href="chapter6.html#cb518-2"></a><span class="kw">library</span>(car)</span>
<span id="cb518-3"><a href="chapter6.html#cb518-3"></a>aisR2<span class="op">$</span>Sex &lt;-<span class="st"> </span><span class="kw">factor</span>(aisR2<span class="op">$</span>Sex)</span>
<span id="cb518-4"><a href="chapter6.html#cb518-4"></a><span class="kw">scatterplot</span>(Hc<span class="op">~</span>Ht<span class="op">|</span>Sex, <span class="dt">data=</span>aisR2, <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">21</span>), <span class="dt">regLine=</span>F, <span class="dt">smooth=</span>F,</span>
<span id="cb518-5"><a href="chapter6.html#cb518-5"></a>            <span class="dt">boxplots=</span><span class="st">&quot;xy&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Scatterplot of Height vs Hematocrit by Sex&quot;</span>) </span>
<span id="cb518-6"><a href="chapter6.html#cb518-6"></a><span class="co"># pch=c(3,21) provides two different symbols that are easy to distinguish. </span></span>
<span id="cb518-7"><a href="chapter6.html#cb518-7"></a><span class="co"># Drop this option or add more numbers to the list if you have more than 2 groups.</span></span></code></pre></div>
<p>Adding the grouping information really changes the impressions of the relationship
between <em>Height</em> and <em>Hematocrit</em> – within each sex, there is little relationship
between the two variables. The overall relationship is of
moderate strength and positive but the subgroup relationships are weak at best.
The overall relationship is created by inappropriately combining two groups
that had different means in both the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> directions. Men have higher
mean heights and hematocrit values than women and putting them together in one
large group creates the misleading overall relationship<a href="#fn99" class="footnote-ref" id="fnref99"><sup>99</sup></a>.</p>
<p>To get the correlation coefficients by groups, we can subset the data set using a
logical inquiry on the <code>Sex</code> variable in the updated <code>aisR2</code> data set, using
<code>Sex==0</code> in the <code>subset</code> function to get a tibble with male subjects only and <code>Sex==1</code> for the female subjects,
then running the <code>cor</code> function on each version of the data set:</p>
<div class="sourceCode" id="cb519"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb519-1"><a href="chapter6.html#cb519-1"></a><span class="kw">cor</span>(Hc<span class="op">~</span>Ht, <span class="dt">data=</span><span class="kw">subset</span>(aisR2,Sex<span class="op">==</span><span class="dv">0</span>)) <span class="co">#Males only</span></span></code></pre></div>
<pre><code>## [1] -0.04756589</code></pre>
<div class="sourceCode" id="cb521"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb521-1"><a href="chapter6.html#cb521-1"></a><span class="kw">cor</span>(Hc<span class="op">~</span>Ht, <span class="dt">data=</span><span class="kw">subset</span>(aisR2,Sex<span class="op">==</span><span class="dv">1</span>)) <span class="co">#Females only</span></span></code></pre></div>
<pre><code>## [1] 0.02795272</code></pre>
<p>These results show that <span class="math inline">\(\boldsymbol{r}=-0.05\)</span> for <em>Height</em> and <em>Hematocrit</em>
for <em>males</em> and <span class="math inline">\(\boldsymbol{r}=0.03\)</span> for <em>females</em>. The first suggests a
very weak negative linear
relationship and the second suggests a very weak positive linear relationship.
The correlation when the two groups were combined (and group information was
ignored!) was that <span class="math inline">\(\boldsymbol{r}=0.37\)</span>. So one
conclusion here is that correlations on data sets that contain groups can be
very misleading (if the groups are ignored). It also emphasizes the importance of exploring for potential
subgroups in the data set – these two groups were not obvious in the initial
plot, but with added information the real story became clear.</p>
<p>For the <em>Body Fat</em> vs <em>Hematocrit</em> results in Figure <a href="chapter6.html#fig:Figure6-8">6.8</a>, with
an overall correlation of <span class="math inline">\(\boldsymbol{r}=-0.54\)</span>, the subgroup correlations
show weaker relationships that also appear to be in different directions
(<span class="math inline">\(\boldsymbol{r}=0.13\)</span> for men and <span class="math inline">\(\boldsymbol{r}=-0.17\)</span> for women). This
doubly reinforces the dangers of aggregating different groups and
ignoring the group information.</p>
<div class="sourceCode" id="cb523"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb523-1"><a href="chapter6.html#cb523-1"></a><span class="kw">cor</span>(Hc<span class="op">~</span>Bfat, <span class="dt">data=</span><span class="kw">subset</span>(aisR2,Sex<span class="op">==</span><span class="dv">0</span>)) <span class="co">#Males only</span></span></code></pre></div>
<pre><code>## [1] 0.1269418</code></pre>
<div class="sourceCode" id="cb525"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb525-1"><a href="chapter6.html#cb525-1"></a><span class="kw">cor</span>(Hc<span class="op">~</span>Bfat, <span class="dt">data=</span><span class="kw">subset</span>(aisR2,Sex<span class="op">==</span><span class="dv">1</span>)) <span class="co">#Females only</span></span></code></pre></div>
<pre><code>## [1] -0.1679751</code></pre>

<div class="figure"><span id="fig:Figure6-8"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-8-1.png" alt="Scatterplot of athlete’s body fat and hematocrit by sex of athletes. Males were coded as 0s and females as 1s." width="576" />
<p class="caption">
Figure 6.8: Scatterplot of athlete’s body fat and hematocrit by sex of athletes. Males were coded as 0s and females as 1s.
</p>
</div>
<!-- \newpage -->
<div class="sourceCode" id="cb527"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb527-1"><a href="chapter6.html#cb527-1"></a><span class="kw">scatterplot</span>(Hc<span class="op">~</span>Bfat<span class="op">|</span>Sex, <span class="dt">data=</span>aisR2, <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">21</span>), <span class="dt">regLine=</span>F, <span class="dt">smooth=</span>F,</span>
<span id="cb527-2"><a href="chapter6.html#cb527-2"></a>            <span class="dt">boxplots=</span><span class="st">&quot;xy&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Scatterplot of Body Fat vs Hematocrit by Sex&quot;</span>)</span></code></pre></div>
<p>One final exploration for these data involves the <em>body fat </em>and <em>height</em> relationship
displayed in Figure <a href="chapter6.html#fig:Figure6-9">6.9</a>. This relationship shows an even greater
disparity between overall and subgroup results. The overall relationship is
characterized as a weak negative relationship <span class="math inline">\((\boldsymbol{r}=-0.20)\)</span> that
is not clearly linear or nonlinear. The subgroup relationships are both clearly
positive with a stronger relationship for men that might also be nonlinear (for
the linear relationships <span class="math inline">\(\boldsymbol{r}=0.45\)</span> for women and
<span class="math inline">\(\boldsymbol{r}=0.20\)</span> for men). Especially for female athletes, those that are
taller seem to have higher body fat percentages. This might be related to the
types of sports they compete in – that would be another categorical variable
we could incorporate… Both groups also seem to demonstrate slightly more
variability in <em>Body Fat</em> associated with taller athletes (each sort of
“fans out”).</p>
<div class="sourceCode" id="cb528"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb528-1"><a href="chapter6.html#cb528-1"></a><span class="kw">cor</span>(Bfat<span class="op">~</span>Ht, <span class="dt">data=</span><span class="kw">subset</span>(aisR2,Sex<span class="op">==</span><span class="dv">0</span>)) <span class="co">#Males only</span></span></code></pre></div>
<pre><code>## [1] 0.1954609</code></pre>
<div class="sourceCode" id="cb530"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb530-1"><a href="chapter6.html#cb530-1"></a><span class="kw">cor</span>(Bfat<span class="op">~</span>Ht, <span class="dt">data=</span><span class="kw">subset</span>(aisR2,Sex<span class="op">==</span><span class="dv">1</span>)) <span class="co">#Females only</span></span></code></pre></div>
<pre><code>## [1] 0.4476962</code></pre>

<div class="figure"><span id="fig:Figure6-9"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-9-1.png" alt="Scatterplot of athlete’s body fat and height by sex." width="576" />
<p class="caption">
Figure 6.9: Scatterplot of athlete’s body fat and height by sex.
</p>
</div>
<div class="sourceCode" id="cb532"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb532-1"><a href="chapter6.html#cb532-1"></a><span class="kw">scatterplot</span>(Bfat<span class="op">~</span>Ht<span class="op">|</span>Sex, <span class="dt">data=</span>aisR2, <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">21</span>), <span class="dt">regLine=</span>F, <span class="dt">smooth=</span>F,</span>
<span id="cb532-2"><a href="chapter6.html#cb532-2"></a>            <span class="dt">boxplots=</span><span class="st">&quot;xy&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Scatterplot of Height vs Body Fat by Sex&quot;</span>)</span></code></pre></div>
<p>In each of these situations, the sex of the athletes has the potential to cause misleading
conclusions if ignored. There are two ways that this could occur – if we did
not measure it then we would have no hope to account for it OR we could have
measured it but not adjusted for it in our results, as was done initially. We
distinguish between these two situations by defining the impacts of this
additional variable as either a confounding or lurking variable:</p>
<ul>
<li><p><strong><em>Confounding variable:</em></strong> affects the response variable and is related to the
explanatory variable. The impacts of a confounding variable on the response
variable cannot be separated from the impacts of the explanatory variable.
</p></li>
<li><p><strong><em>Lurking variable:</em></strong> a potential confounding variable that is not measured
and is not considered in the interpretation of the study.
</p></li>
</ul>
<p>Lurking variables show up in studies sometimes due to lack of knowledge of the
system being studied or a lack of
resources to measure these variables. Note that there may be no satisfying resolution to the confounding variable problem but that it is better to have measured it and know about it than to have it remain a lurking variable.</p>
<p>To help think about confounding and lurking variables, consider the following
situation. On many
highways, such as Highway 93 in Montana and north into Canada, recent
construction efforts have been involved in creating safe passages for animals
by adding fencing and animal crossing structures. These structures both can
improve driver safety, save money from costs associated with animal-vehicle
collisions, and increase connectivity of animal populations. Researchers (such as <span class="citation">Clevenger and Waltho (<a href="#ref-Clevenger2005" role="doc-biblioref">2005</a>)</span>)
involved in these projects are interested in which characteristics of
underpasses lead to the most successful structures, mainly measured by rates of
animal usage (number of times they cross under the road). Crossing structures
are typically made using culverts and those tend to be cylindrical. Researchers
are interested in studying the effect of height and width of crossing structures
on animal usage. Unfortunately, all the tallest structures are also the widest
structures. If animals prefer the tall and wide structures, then there is no
way to know if it is due to the height or width of the structure since they are
confounded. If the researchers had only measured width, then they might assume
that it is the important characteristic of the structures but height could be a
lurking variable that really was the factor related to animal usage of the
structures. This is an example where it may not be possible to design a study
that prevents confounding of the two variables <em>height</em> and <em>width</em>. If the
researchers could control the height and width of the structures independently, then they
could randomly assign both variables to make sure that some narrow structures
are installed that are tall and some that are short. Additionally, they would
also want to have some wide structures that are short and some are tall. Careful design of studies can prevent confounding of variables if they are
known in advance and it is possible to control them, but in observational
studies the observed combinations of variables are uncontrollable. This is why
we need to employ additional caution in interpreting results from observational
studies. Here that would mean that even if width was found to be a predictor of animal usage, we would likely want to avoid saying that width of the structures caused differences in animal usage.</p>
</div>
<div id="section6-4" class="section level2">
<h2><span class="header-section-number">6.4</span> Inference for the correlation coefficient</h2>
<p>We used bootstrapping briefly in
Chapter <a href="chapter2.html#chapter2">2</a> to generate nonparametric confidence intervals based
on the middle
95% of the bootstrapped version of the statistic. Remember that bootstrapping
involves sampling <em>with replacement</em>
from the data set and creates a distribution centered near the statistic from
the real data set. This also mimics sampling under the alternative as opposed
to sampling under the null as in our permutation approaches. Bootstrapping is
particularly useful for making confidence intervals where the distribution of
the statistic may not follow a named distribution. This is the case for the
correlation coefficient which we will see shortly.
</p>
<p>The correlation is an interesting
summary but it is also an estimator of a population parameter called <span class="math inline">\(\rho\)</span>
(the symbol rho), which is the <strong><em>population correlation coefficient</em></strong>. When
<span class="math inline">\(\rho = 1\)</span>, we have a perfect positive linear relationship in the population;
when <span class="math inline">\(\rho=-1\)</span>, there is a perfect negative linear relationship in the
population; and when <span class="math inline">\(\rho=0\)</span>, there is no linear relationship in the
population. Therefore, to test if there is a
linear relationship between two quantitative variables, we use the null
hypothesis <span class="math inline">\(H_0: \rho=0\)</span> (tests if the true correlation, <span class="math inline">\(\rho\)</span>, is 0 – no
linear relationship). The alternative hypothesis is that there is some
(positive or negative) relationship between the variables in the population,
<span class="math inline">\(H_A: \rho \ne 0\)</span>. The distribution of the Pearson correlation coefficient
can be complicated in some situations, so we will use
bootstrapping methods to generate confidence intervals for <span class="math inline">\(\rho\)</span> based on
repeated random samples with replacement from the original data set.
 
If the <span class="math inline">\(C\%\)</span>
confidence interval contains 0, then we would find little to no evidence against the null
hypothesis since 0 is in the interval of our likely values for <span class="math inline">\(\rho\)</span>. If
the <span class="math inline">\(C\%\)</span> confidence interval does not contain 0, then we would find strong evidence against the null
hypothesis. Along with its use in testing, it is also interesting to be able to generate a confidence interval for <span class="math inline">\(\rho\)</span> to provide an interval where we are <span class="math inline">\(C\%\)</span> confident that the true parameter lies.</p>
<p>The <em>beers</em> and <em>BAC</em> example seemed to provide a strong relationship with
<span class="math inline">\(\boldsymbol{r}=0.89\)</span>. As correlations approach -1 or 1, the sampling distribution becomes
more and more skewed. This certainly shows up in the bootstrap distribution
that the following code produces (Figure <a href="chapter6.html#fig:Figure6-10">6.10</a>). Remember that
bootstrapping utilizes the <code>resample</code> function applied to the data set to
create new realizations of the data set by re-sampling
with replacement from those observations. The bold vertical line in
Figure <a href="chapter6.html#fig:Figure6-10">6.10</a> corresponds to the estimated correlation
<span class="math inline">\(\boldsymbol{r}=0.89\)</span> and the distribution contains a noticeable left skew
with a few much smaller <span class="math inline">\(T^*\text{&#39;s}\)</span> possible in bootstrap samples. The
<span class="math inline">\(C\%\)</span> confidence interval is found based
on the middle <span class="math inline">\(C\%\)</span> of the distribution or by finding the values that put
<span class="math inline">\((100-C)/2\)</span> into each tail of the distribution with the <code>qdata</code> function.</p>
<div class="sourceCode" id="cb533"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb533-1"><a href="chapter6.html#cb533-1"></a>Tobs &lt;-<span class="st"> </span><span class="kw">cor</span>(BAC<span class="op">~</span>Beers, <span class="dt">data=</span>BB); Tobs</span></code></pre></div>
<pre><code>## [1] 0.8943381</code></pre>
<div class="sourceCode" id="cb535"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb535-1"><a href="chapter6.html#cb535-1"></a><span class="kw">set.seed</span>(<span class="dv">614</span>)</span>
<span id="cb535-2"><a href="chapter6.html#cb535-2"></a>B &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb535-3"><a href="chapter6.html#cb535-3"></a>Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</span>
<span id="cb535-4"><a href="chapter6.html#cb535-4"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</span>
<span id="cb535-5"><a href="chapter6.html#cb535-5"></a>  Tstar[b] &lt;-<span class="st"> </span><span class="kw">cor</span>(BAC<span class="op">~</span>Beers, <span class="dt">data=</span><span class="kw">resample</span>(BB))</span>
<span id="cb535-6"><a href="chapter6.html#cb535-6"></a>}</span>
<span id="cb535-7"><a href="chapter6.html#cb535-7"></a>quantiles &lt;-<span class="st"> </span><span class="kw">qdata</span>(Tstar, <span class="kw">c</span>(<span class="fl">0.025</span>,<span class="fl">0.975</span>)) <span class="co">#95% Confidence Interval</span></span></code></pre></div>
<!-- \newpage -->

<div class="sourceCode" id="cb536"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb536-1"><a href="chapter6.html#cb536-1"></a>quantiles</span></code></pre></div>
<pre><code>##        quantile     p
## 2.5%  0.7633606 0.025
## 97.5% 0.9541518 0.975</code></pre>
<div class="sourceCode" id="cb538"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb538-1"><a href="chapter6.html#cb538-1"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb538-2"><a href="chapter6.html#cb538-2"></a><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">550</span>))</span>
<span id="cb538-3"><a href="chapter6.html#cb538-3"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb538-4"><a href="chapter6.html#cb538-4"></a><span class="kw">abline</span>(<span class="dt">v=</span>quantiles<span class="op">$</span>quantile, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb538-5"><a href="chapter6.html#cb538-5"></a></span>
<span id="cb538-6"><a href="chapter6.html#cb538-6"></a><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</span>
<span id="cb538-7"><a href="chapter6.html#cb538-7"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb538-8"><a href="chapter6.html#cb538-8"></a><span class="kw">abline</span>(<span class="dt">v=</span>quantiles<span class="op">$</span>quantile, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure6-10"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-10-1.png" alt="Histogram and density curve of the bootstrap distribution of the correlation coefficient with bold vertical line for observed correlation and dashed lines for bounds for the 95% bootstrap confidence interval." width="960" />
<p class="caption">
Figure 6.10: Histogram and density curve of the bootstrap distribution of the correlation coefficient with bold vertical line for observed correlation and dashed lines for bounds for the 95% bootstrap confidence interval.
</p>
</div>
<p>These results tell us that the bootstrap 95% CI is from 0.76 to 0.95 – we are 95%
confident that the true correlation between <em>Beers</em> and <em>BAC</em> in all OSU students
like those that volunteered for this study is between 0.76 and 0.95. Note that there are no units
on the correlation coefficient or in this interpretation of it.</p>
<p>We can also use this confidence interval to test for a linear relationship between
these variables.</p>
<ul>
<li><p><span class="math inline">\(\boldsymbol{H_0:\rho=0:}\)</span> <strong>There is no linear relationship between <em>Beers</em>
and <em>BAC</em> in the population.</strong></p></li>
<li><p><span class="math inline">\(\boldsymbol{H_A: \rho \ne 0:}\)</span> <strong>There is a linear relationship between
<em>Beers</em> and <em>BAC</em> in the population.</strong></p></li>
</ul>
<p>The 95% confidence level corresponds to a 5% significance level test and if the 95% CI does not contain 0, you know that the p-value would be less than 0.05 and if it does contain 0 that the p-value would be more than 0.05. The 95% CI is
from 0.76 to 0.95, which does not contain 0, so we find strong evidence<a href="#fn100" class="footnote-ref" id="fnref100"><sup>100</sup></a> against the null
hypothesis and conclude that there is
a linear relationship between <em>Beers</em> and <em>BAC</em> in OSU students. We’ll revisit this
example using the upcoming regression tools to explore the potential for more
specific conclusions about this relationship. Note that for these inferences to be
accurate, we need to be able to trust that the sample correlation is reasonable
for characterizing the relationship between these variables along with the assumptions we will discuss below.</p>
<p>In this situation with randomly assigned levels of <span class="math inline">\(x\)</span> and strong evidence against the null
hypothesis of no relationship, we can further conclude that changing beer
consumption <strong>causes</strong> changes in the <em>BAC</em>. This is a much stronger conclusion
than we can typically make based on correlation coefficients. Correlations and
scatterplots are enticing for infusing causal interpretations in non-causal
situations. Statistics teachers often repeat the mantra that <strong><em>correlation is not causation</em></strong>
and that generally applies – except when there is randomization involved in
the study. It is rarer for
researchers either to assign, or even to be able to assign, levels of
quantitative variables so correlations should be viewed as non-causal unless
the details of the study suggest otherwise.</p>
</div>
<div id="section6-5" class="section level2">
<h2><span class="header-section-number">6.5</span> Are tree diameters related to tree heights?</h2>
<p>In a study at the Upper Flat Creek
study area in the University of Idaho Experimental Forest, a random sample of
<span class="math inline">\(n=336\)</span> trees was selected from the forest, with measurements recorded on Douglas
Fir, Grand Fir, Western Red
Cedar, and Western Larch trees. The data set called <code>ufc</code> is available from the
<code>spuRs</code> package <span class="citation">(Jones et al. <a href="#ref-R-spuRs" role="doc-biblioref">2018</a>)</span> and
contains <code>dbh.cm</code> (tree diameter at 1.37 m from the ground, measured in cm) and
<code>height.m</code> (tree height in meters).

The relationship displayed in
Figure <a href="chapter6.html#fig:Figure6-11">6.11</a> is positive,
moderately strong with some curvature and increasing variability as the
diameter increases. There do not appear to be groups in the data set but since
this contains four different types of trees, we would want to revisit this plot
by type of tree.</p>

<div class="sourceCode" id="cb539"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb539-1"><a href="chapter6.html#cb539-1"></a><span class="kw">library</span>(spuRs) <span class="co">#install.packages(&quot;spuRs&quot;)</span></span>
<span id="cb539-2"><a href="chapter6.html#cb539-2"></a><span class="kw">data</span>(ufc)</span>
<span id="cb539-3"><a href="chapter6.html#cb539-3"></a>ufc &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(ufc)</span>
<span id="cb539-4"><a href="chapter6.html#cb539-4"></a><span class="kw">scatterplot</span>(height.m<span class="op">~</span>dbh.cm, <span class="dt">data=</span>ufc, <span class="dt">smooth=</span>F, <span class="dt">regLine=</span>T, <span class="dt">pch=</span><span class="dv">16</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure6-11"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-11-1.png" alt="Scatterplot of tree heights (m) vs tree diameters (cm)." width="576" />
<p class="caption">
Figure 6.11: Scatterplot of tree heights (m) vs tree diameters (cm).
</p>
</div>
<p>Of particular interest is an observation with a diameter around 58 cm and a height
of less than 5 m. Observing a tree with a diameter around 60 cm is not unusual
in the data set, but none of the other trees with this diameter had heights
under 15 m. It ends up that the likely outlier is in observation number 168 and
because it is so unusual it likely corresponds to either a damaged tree or a
recording error.</p>
<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb540-1"><a href="chapter6.html#cb540-1"></a>ufc[<span class="dv">168</span>,]</span></code></pre></div>
<pre><code>## # A tibble: 1 x 5
##    plot  tree species dbh.cm height.m
##   &lt;int&gt; &lt;int&gt; &lt;fct&gt;    &lt;dbl&gt;    &lt;dbl&gt;
## 1    67     6 WL        57.5      3.4</code></pre>
<p>With the outlier in the data set, the correlation is 0.77 and without it, the
correlation increases to 0.79. The removal does not create a big change because
the data set is relatively large and the <em>diameter</em> value is close to the mean of the
<span class="math inline">\(x\text{&#39;s}\)</span><a href="#fn101" class="footnote-ref" id="fnref101"><sup>101</sup></a> but it has some impact on the
strength of the correlation.</p>
<div class="sourceCode" id="cb542"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb542-1"><a href="chapter6.html#cb542-1"></a><span class="kw">cor</span>(dbh.cm<span class="op">~</span>height.m, <span class="dt">data=</span>ufc)</span></code></pre></div>
<pre><code>## [1] 0.7699552</code></pre>
<div class="sourceCode" id="cb544"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb544-1"><a href="chapter6.html#cb544-1"></a><span class="kw">cor</span>(dbh.cm<span class="op">~</span>height.m, <span class="dt">data=</span>ufc[<span class="op">-</span><span class="dv">168</span>,])</span></code></pre></div>
<pre><code>## [1] 0.7912053</code></pre>
<p>With the outlier included, the bootstrap 95% confidence interval goes from 0.702 to
0.820 – we are 95% confident that the true correlation between <em>diameter</em> and <em>height</em>
in the population of trees is between 0.708 and 0.819. When
the outlier is dropped from the data set, the 95% bootstrap CI is 0.753 to 0.826,
which shifts the lower endpoint of the interval up, reducing the width of the
interval from 0.111 to 0.073. In other words, the uncertainty regarding the
value of the population correlation coefficient is reduced. The reason to
remove the observation is that it is unusual based on the observed pattern,
which implies an error in data collection or sampling from a population other
than the one used for the other observations and, if the removal is justified,
it helps us refine our inferences for the population parameter. But measuring
the linear relationship in these data where there is a clear curve violates one of
our assumptions of using these methods – we’ll see some other ways of detecting
this issue in Section <a href="chapter6.html#section6-10">6.10</a> and we’ll try to “fix” this example using
transformations in the Chapter <a href="chapter7.html#chapter7">7</a>.</p>

<div class="sourceCode" id="cb546"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb546-1"><a href="chapter6.html#cb546-1"></a>Tobs &lt;-<span class="st"> </span><span class="kw">cor</span>(dbh.cm<span class="op">~</span>height.m, <span class="dt">data=</span>ufc); Tobs</span></code></pre></div>
<pre><code>## [1] 0.7699552</code></pre>
<div class="sourceCode" id="cb548"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb548-1"><a href="chapter6.html#cb548-1"></a><span class="kw">set.seed</span>(<span class="dv">208</span>)</span>
<span id="cb548-2"><a href="chapter6.html#cb548-2"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))</span>
<span id="cb548-3"><a href="chapter6.html#cb548-3"></a>B &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb548-4"><a href="chapter6.html#cb548-4"></a>Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</span>
<span id="cb548-5"><a href="chapter6.html#cb548-5"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</span>
<span id="cb548-6"><a href="chapter6.html#cb548-6"></a>  Tstar[b] &lt;-<span class="st"> </span><span class="kw">cor</span>(dbh.cm<span class="op">~</span>height.m, <span class="dt">data=</span><span class="kw">resample</span>(ufc))</span>
<span id="cb548-7"><a href="chapter6.html#cb548-7"></a>}</span>
<span id="cb548-8"><a href="chapter6.html#cb548-8"></a>quantiles &lt;-<span class="st"> </span><span class="kw">qdata</span>(Tstar, <span class="kw">c</span>(.<span class="dv">025</span>,.<span class="dv">975</span>)) <span class="co">#95% Confidence Interval</span></span>
<span id="cb548-9"><a href="chapter6.html#cb548-9"></a>quantiles</span></code></pre></div>
<pre><code>##        quantile     p
## 2.5%  0.7075771 0.025
## 97.5% 0.8190283 0.975</code></pre>
<div class="sourceCode" id="cb550"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb550-1"><a href="chapter6.html#cb550-1"></a><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="fl">0.6</span>,<span class="fl">0.9</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">275</span>),</span>
<span id="cb550-2"><a href="chapter6.html#cb550-2"></a>     <span class="dt">main=</span><span class="st">&quot;Bootstrap distribution of correlation with all data&quot;</span>)</span>
<span id="cb550-3"><a href="chapter6.html#cb550-3"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb550-4"><a href="chapter6.html#cb550-4"></a><span class="kw">abline</span>(<span class="dt">v=</span>quantiles<span class="op">$</span>quantile, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb550-5"><a href="chapter6.html#cb550-5"></a></span>
<span id="cb550-6"><a href="chapter6.html#cb550-6"></a>Tobs &lt;-<span class="st"> </span><span class="kw">cor</span>(dbh.cm<span class="op">~</span>height.m, <span class="dt">data=</span>ufc[<span class="op">-</span><span class="dv">168</span>,]); Tobs</span></code></pre></div>
<pre><code>## [1] 0.7912053</code></pre>
<div class="sourceCode" id="cb552"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb552-1"><a href="chapter6.html#cb552-1"></a>Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</span>
<span id="cb552-2"><a href="chapter6.html#cb552-2"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</span>
<span id="cb552-3"><a href="chapter6.html#cb552-3"></a>  Tstar[b] &lt;-<span class="st"> </span><span class="kw">cor</span>(dbh.cm<span class="op">~</span>height.m, <span class="dt">data=</span><span class="kw">resample</span>(ufc[<span class="op">-</span><span class="dv">168</span>,]))</span>
<span id="cb552-4"><a href="chapter6.html#cb552-4"></a>}</span>
<span id="cb552-5"><a href="chapter6.html#cb552-5"></a>quantiles &lt;-<span class="st"> </span><span class="kw">qdata</span>(Tstar, <span class="kw">c</span>(.<span class="dv">025</span>,.<span class="dv">975</span>)) <span class="co">#95% Confidence Interval</span></span>
<span id="cb552-6"><a href="chapter6.html#cb552-6"></a>quantiles</span></code></pre></div>
<pre><code>##        quantile     p
## 2.5%  0.7532338 0.025
## 97.5% 0.8259416 0.975</code></pre>
<div class="sourceCode" id="cb554"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb554-1"><a href="chapter6.html#cb554-1"></a><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="fl">0.6</span>,<span class="fl">0.9</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">275</span>),</span>
<span id="cb554-2"><a href="chapter6.html#cb554-2"></a>     <span class="dt">main=</span> <span class="st">&quot;Bootstrap distribution of correlation without outlier&quot;</span>)</span>
<span id="cb554-3"><a href="chapter6.html#cb554-3"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb554-4"><a href="chapter6.html#cb554-4"></a><span class="kw">abline</span>(<span class="dt">v=</span>quantiles<span class="op">$</span>quantile, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure6-12"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-12-1.png" alt="Bootstrap distributions of the correlation coefficient for the full data set (top) and without potential outlier included (bottom) with observed correlation (bold line) and bounds for the 95% confidence interval (dashed lines). Notice the change in spread of the bootstrap distributions as well as the different centers." width="960" />
<p class="caption">
Figure 6.12: Bootstrap distributions of the correlation coefficient for the full data set (top) and without potential outlier included (bottom) with observed correlation (bold line) and bounds for the 95% confidence interval (dashed lines). Notice the change in spread of the bootstrap distributions as well as the different centers.
</p>
</div>
<div style="page-break-after: always;"></div>
</div>
<div id="section6-6" class="section level2">
<h2><span class="header-section-number">6.6</span> Describing relationships with a regression model</h2>
<p>When the relationship appears to
be relatively linear, it makes sense to estimate and then interpret a line to
represent the relationship between the variables. This line is called a
<strong><em>regression line</em></strong> and involves finding a line that best fits (explains
variation in) the response variable for the
given values of the explanatory variable.

For regression, it matters which
variable you choose for <span class="math inline">\(x\)</span> and which you choose for <span class="math inline">\(y\)</span> – for correlation
it did not matter. This regression line describes the “effect” of <span class="math inline">\(x\)</span> on
<span class="math inline">\(y\)</span> and also provides an equation for predicting values of <span class="math inline">\(y\)</span> for given
values of <span class="math inline">\(x\)</span>. The <em>Beers</em> and <em>BAC</em> data provide a nice example to start
our exploration of regression models. The beer consumption is a clear
explanatory variable,
detectable in the story because (1) it was randomly assigned to subjects and
(2) basic science supports beer consumption amount being an explanatory
variable for <em>BAC</em>. In some situations, this will not be so clear, but look for
random assignment or scientific logic to guide your choices of variables as
explanatory or response<a href="#fn102" class="footnote-ref" id="fnref102"><sup>102</sup></a>. Regression lines are actually provided by
default in the <code>scatterplot</code> function with the <code>reg.line=T</code> option or just
omitting <code>reg.line=F</code> from the previous versions of the code since it is a
default option to provide the lines.</p>

<div class="sourceCode" id="cb555"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb555-1"><a href="chapter6.html#cb555-1"></a><span class="kw">scatterplot</span>(BAC<span class="op">~</span>Beers, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,.<span class="dv">2</span>), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">9</span>), <span class="dt">data=</span>BB, <span class="dt">pch=</span><span class="dv">16</span>,</span>
<span id="cb555-2"><a href="chapter6.html#cb555-2"></a>            <span class="dt">boxplot=</span>F, <span class="dt">main=</span><span class="st">&quot;Scatterplot with regression line&quot;</span>,</span>
<span id="cb555-3"><a href="chapter6.html#cb555-3"></a>            <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">smooth=</span>F)</span>
<span id="cb555-4"><a href="chapter6.html#cb555-4"></a><span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">1</span><span class="op">:</span><span class="dv">9</span>, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>)</span>
<span id="cb555-5"><a href="chapter6.html#cb555-5"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">c</span>(<span class="fl">0.05914</span>,<span class="fl">0.0771</span>), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure6-13"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-13-1.png" alt="Scatterplot with estimated regression line for the Beers and BAC data. Horizontal dashed lines for the predicted BAC for 4 and 5 beers consumed." width="576" />
<p class="caption">
Figure 6.13: Scatterplot with estimated regression line for the <em>Beers</em> and <em>BAC</em> data. Horizontal dashed lines for the predicted BAC for 4 and 5 beers consumed.
</p>
</div>
<p>The equation for a line is <span class="math inline">\(y=a+bx\)</span>, or maybe <span class="math inline">\(y=mx+b\)</span>. In the version
<span class="math inline">\(mx+b\)</span> you learned that <span class="math inline">\(m\)</span> is a slope coefficient that relates a
change in <span class="math inline">\(x\)</span> to changes in <span class="math inline">\(y\)</span> and that <span class="math inline">\(b\)</span> is a <span class="math inline">\(y\)</span>-intercept (the
value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x\)</span> is 0). In Figure <a href="chapter6.html#fig:Figure6-13">6.13</a>, two extra
horizontal lines are added to help you see the defining characteristics of the line. The
slope, whatever letter you use, is the change in <span class="math inline">\(y\)</span> for a one-unit
increase in <span class="math inline">\(x\)</span>. Here, the slope is the change in <code>BAC</code> for a 1 beer
increase in <code>Beers</code>, such as the change from 4 to 5 beers. The
<span class="math inline">\(y\)</span>-values (blue, dashed lines) for <code>Beers</code> = 4 and 5 go from 0.059 to
0.077. This means that for a 1 beer increase (+1 unit change in <span class="math inline">\(x\)</span>), the
<code>BAC</code> goes up by <span class="math inline">\(0.077-0.059=0.018\)</span> (+0.018 unit change in <span class="math inline">\(y\)</span>).
We can also try to find the <span class="math inline">\(y\)</span>-intercept on the graph by looking for the
<code>BAC</code> level for 0 <code>Beers</code> consumed. The <span class="math inline">\(y\)</span>-value (<code>BAC</code>) ends up
being around -0.01 if you extend the regression line to <code>Beers</code>=0.
You might assume that the <code>BAC</code> should be 0 for <code>Beers</code>=0 but the
researchers did not observe any students at 0 <code>Beers</code>, so we don’t
really know what the <code>BAC</code> might be at this value. We have to
use our line to <strong><em>predict</em></strong> this value. This ends up providing a
prediction below 0 – an impossible value for <em>BAC</em>. If the
<span class="math inline">\(y\)</span>-intercept were positive, it would suggest that the students
has a <em>BAC</em> over 0 even without drinking. </p>
<p>The numbers reported were very
accurate because we weren’t using the plot alone to generate the
values – we
were using a linear model to estimate the equation to
describe the
relationship between <code>Beers</code> and <code>BAC</code>. In statistics, we estimate
“<span class="math inline">\(m\)</span>” and “<span class="math inline">\(b\)</span>”. We also write the equation starting with the <span class="math inline">\(y\)</span>-intercept
and use slightly different notation that allows us to extend to more
complicated models with more variables.


Specifically, the estimated
regression equation is <span class="math inline">\(\widehat{y} = b_0 + b_1x\)</span>, where</p>
<ul>
<li><p><span class="math inline">\(\widehat{y}\)</span> is the estimated value of <span class="math inline">\(y\)</span> for a given <span class="math inline">\(x\)</span>,</p></li>
<li><p><span class="math inline">\(b_0\)</span> is the estimated <span class="math inline">\(y\)</span>-intercept (predicted value of <span class="math inline">\(y\)</span> when
<span class="math inline">\(x\)</span> is 0),</p></li>
<li><p><span class="math inline">\(b_1\)</span> is the estimated slope coefficient, and</p></li>
<li><p><span class="math inline">\(x\)</span> is the explanatory variable.</p></li>
</ul>
<p>One of the differences between when you learned equations in algebra
classes and our
situation is that the line is not a perfect description of the relationship
between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> – it is an “on average” description and will usually
leave differences between the line and the observations, which we call
residuals <span class="math inline">\((e = y-\widehat{y})\)</span>.

We worked with residuals in the ANOVA<a href="#fn103" class="footnote-ref" id="fnref103"><sup>103</sup></a>
material. The residuals describe the vertical distance in the scatterplot between
our model (regression line) and the actual observed data point. The lack of a
perfect fit of the line to the observations distinguishes statistical equations
from those you learned in math classes. The equations work the same, but we
have to modify interpretations of the coefficients to reflect this.</p>
<p>We also tie this estimated model to a theoretical or <strong><em>population
regression model</em></strong>:
</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1x_i+\varepsilon_i\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(y_i\)</span> is the observed response for the <span class="math inline">\(i^{th}\)</span> observation,</p></li>
<li><p><span class="math inline">\(x_i\)</span> is the observed value of the explanatory variable for the
<span class="math inline">\(i^{th}\)</span> observation,</p></li>
<li><p><span class="math inline">\(\beta_0 + \beta_1x_i\)</span> is the true mean function evaluated at <span class="math inline">\(x_i\)</span>,</p></li>
<li><p><span class="math inline">\(\beta_0\)</span> is the true (or population) <span class="math inline">\(y\)</span>-intercept,</p></li>
<li><p><span class="math inline">\(\beta_1\)</span> is the true (or population) slope coefficient, and</p></li>
<li><p>the deviations, <span class="math inline">\(\varepsilon_i\)</span>, are assumed to be independent and
normally distributed with mean 0 and standard deviation <span class="math inline">\(\sigma\)</span> or,
more compactly, <span class="math inline">\(\varepsilon_i \sim N(0,\sigma^2)\)</span>.</p></li>
</ul>
<p>This presents another version of the linear model from Chapters <a href="chapter2.html#chapter2">2</a>, <a href="chapter3.html#chapter3">3</a>,
and <a href="chapter4.html#chapter4">4</a>, now with a
quantitative explanatory variable instead of categorical explanatory variable(s). This chapter
focuses mostly on the estimated regression coefficients, but remember that we
are doing statistics and our desire is to make inferences to a larger population.
So, estimated coefficients, <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, are approximations to
theoretical coefficients, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. In other words,
<span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>
are the statistics that try to estimate the true population parameters
<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, respectively.</p>
<p>To get estimated regression coefficients, we use the <code>lm</code> function
and our standard <code>lm(y~x, data=...)</code> setup.

This is the same function
used to estimate our ANOVA models and much of this
will look familiar. In fact, the ties between ANOVA and regression are
deep and fundamental but not the topic of this section. For the <em>Beers</em>
and <em>BAC</em> example, the <strong><em>estimated regression coefficients</em></strong> can be
found from:</p>

<div class="sourceCode" id="cb556"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb556-1"><a href="chapter6.html#cb556-1"></a>m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(BAC<span class="op">~</span>Beers, <span class="dt">data=</span>BB)</span>
<span id="cb556-2"><a href="chapter6.html#cb556-2"></a>m1</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = BAC ~ Beers, data = BB)
## 
## Coefficients:
## (Intercept)        Beers  
##    -0.01270      0.01796</code></pre>
<p>More often, we will extract these from the <strong><em>coefficient table</em></strong> produced
by a model <code>summary</code>: </p>

<div class="sourceCode" id="cb558"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb558-1"><a href="chapter6.html#cb558-1"></a><span class="kw">summary</span>(m1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = BAC ~ Beers, data = BB)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.027118 -0.017350  0.001773  0.008623  0.041027 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -0.012701   0.012638  -1.005    0.332
## Beers        0.017964   0.002402   7.480 2.97e-06
## 
## Residual standard error: 0.02044 on 14 degrees of freedom
## Multiple R-squared:  0.7998, Adjusted R-squared:  0.7855 
## F-statistic: 55.94 on 1 and 14 DF,  p-value: 2.969e-06</code></pre>
<p>From either version of the output, you can find the estimated <span class="math inline">\(y\)</span>-intercept
in the <code>(Intercept)</code> part of the output and the slope coefficient in the
<code>Beers</code> part of the output. So <span class="math inline">\(b_0 = -0.0127\)</span>, <span class="math inline">\(b_1=0.01796\)</span>, and
the <strong><em>estimated regression equation</em></strong> is</p>
<p><span class="math display">\[\widehat{\text{BAC}}_i = -0.0127 + 0.01796\cdot\text{Beers}_i.\]</span></p>
<p>This is the equation that was plotted in Figure <a href="chapter6.html#fig:Figure6-13">6.13</a>.
In writing out the equation, it is
good to replace <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> with the variable names to make the predictor
and response variables clear. <strong>If you prefer to write all equations with</strong>
<span class="math inline">\(\boldsymbol{x}\)</span> <strong>and</strong> <span class="math inline">\(\boldsymbol{y}\)</span><strong>, you need to define</strong>
<span class="math inline">\(\boldsymbol{x}\)</span> <strong>and</strong> <span class="math inline">\(\boldsymbol{y}\)</span> <strong>or else these equations are not
clearly defined.</strong></p>
<p>There is a general interpretation
for the slope coefficient that you will need to master. In general, we
interpret the slope coefficient as: </p>
<ul>
<li><strong>Slope interpretation (general):</strong> For a 1 <strong>[<em>unit of X</em>]</strong> increase in
<strong><em>X</em></strong>, we expect, <em>on average</em>, a <span class="math inline">\(\boldsymbol{b_1}\)</span> <strong>[<em>unit of Y</em>]</strong> change
in <strong><em>Y</em></strong>.</li>
</ul>

<div class="figure" style="text-align: center"><span id="fig:Figure6-14"></span>
<img src="chapter6_files/image047.png" alt="Diagram of interpretation of slope coefficients." width="100%" />
<p class="caption">
Figure 6.14: Diagram of interpretation of slope coefficients.
</p>
</div>
<p>Figure <a href="chapter6.html#fig:Figure6-14">6.14</a> can help you think about the
different sorts of slope coefficients we might need to interpret, both
providing changes in the response variable for 1 unit increases in the
predictor variable.</p>
<p>Applied to this problem, for each additional 1 beer consumed, we expect
a 0.018 gram per dL change in the <em>BAC</em> <em>on average</em>. Using “change” in
the interpretation for what happened in the response
allows you to use the same template for the interpretation even with
negative slopes –
be careful about saying “decrease” when the slope is negative as you can create
a double-negative and end up implying an increase… Note also that you need to
carefully incorporate the units of <span class="math inline">\(x\)</span> and the units of <span class="math inline">\(y\)</span> to make the
interpretation clear. For example, if the change in <em>BAC</em> for 1 beer increase
is 0.018, then we could also modify the size of the change in <span class="math inline">\(x\)</span> to be a 10 beer increase and then the estimated
change in <em>BAC</em> is <span class="math inline">\(10*0.018 = 0.18\)</span> g/dL. Both are correct as long as you are
clear about the change in <span class="math inline">\(x\)</span> you are talking
about. Typically, we will just use the units used in the original variables and
only change the scale of “change in <span class="math inline">\(x\)</span>” when it provides an interpretation we
are particularly interested in.</p>
<p>Similarly, the general interpretation for a <span class="math inline">\(y\)</span>-intercept is:
</p>
<ul>
<li><strong><span class="math inline">\(Y\)</span>-intercept interpretation (general):</strong> For <strong><em>X</em></strong>= 0 <strong>[<em>units of X</em>]</strong>,
we expect, on average, <span class="math inline">\(\boldsymbol{b_0}\)</span> <strong>[<em>units of Y</em>]</strong> <strong>in</strong> <strong><em>Y</em></strong>.</li>
</ul>
<p>Again, applied to the <em>BAC</em> data set: For 0 beers for <em>Beers</em> consumed,
we expect, on
average, -0.012 g/dL <em>BAC</em>. The <span class="math inline">\(y\)</span>-intercept interpretation is often less
interesting than the slope interpretation but can be interesting in some
situations. Here, it is predicting average <em>BAC</em> for <code>Beers</code>=0, which
is a value outside the scope of the <span class="math inline">\(x\text{&#39;s}\)</span> (<em>Beers</em> was observed
between 1 and 9). Prediction outside the scope of the predictor values is
called <strong><em>extrapolation</em></strong>. Extrapolation is dangerous at best and
misleading at worst. That said, if you are asked to
interpret the <span class="math inline">\(y\)</span>-intercept you should still interpret it, but it is also good to
note if it is outside of the region where we had observations on the
explanatory variable. Another example is useful for practicing how to do these
interpretations.</p>
<p>In the Australian Athlete data, we
saw a weak negative relationship between <em>Body Fat</em> (% body weight that
is fat) and <em>Hematocrit</em> (% red blood cells in the blood). The scatterplot
in Figure <a href="chapter6.html#fig:Figure6-15">6.15</a> shows just the
results for the female athletes along with the regression line which has a
negative slope coefficient. The estimated regression coefficients are found
using the <code>lm</code> function:
</p>
<div class="sourceCode" id="cb560"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb560-1"><a href="chapter6.html#cb560-1"></a>aisR2 &lt;-<span class="st"> </span>ais[<span class="op">-</span><span class="kw">c</span>(<span class="dv">56</span>,<span class="dv">166</span>), <span class="kw">c</span>(<span class="st">&quot;Ht&quot;</span>,<span class="st">&quot;Hc&quot;</span>,<span class="st">&quot;Bfat&quot;</span>,<span class="st">&quot;Sex&quot;</span>)]</span>
<span id="cb560-2"><a href="chapter6.html#cb560-2"></a>m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Hc<span class="op">~</span>Bfat, <span class="dt">data=</span><span class="kw">subset</span>(aisR2,Sex<span class="op">==</span><span class="dv">1</span>)) <span class="co">#Results for Females </span></span></code></pre></div>
<div style="page-break-after: always;"></div>

<div class="sourceCode" id="cb561"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb561-1"><a href="chapter6.html#cb561-1"></a><span class="kw">summary</span>(m2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Hc ~ Bfat, data = subset(aisR2, Sex == 1))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.2399 -2.2132 -0.1061  1.8917  6.6453 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 42.01378    0.93269  45.046   &lt;2e-16
## Bfat        -0.08504    0.05067  -1.678   0.0965
## 
## Residual standard error: 2.598 on 97 degrees of freedom
## Multiple R-squared:  0.02822,    Adjusted R-squared:  0.0182 
## F-statistic: 2.816 on 1 and 97 DF,  p-value: 0.09653</code></pre>
<div class="sourceCode" id="cb563"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb563-1"><a href="chapter6.html#cb563-1"></a><span class="kw">scatterplot</span>(Hc<span class="op">~</span>Bfat, <span class="dt">data=</span><span class="kw">subset</span>(aisR2,Sex<span class="op">==</span><span class="dv">1</span>), <span class="dt">smooth=</span>F, <span class="dt">pch=</span><span class="dv">16</span>,</span>
<span id="cb563-2"><a href="chapter6.html#cb563-2"></a>            <span class="dt">main=</span><span class="st">&quot;Scatterplot of Body Fat vs Hematocrit for Female Athletes&quot;</span>,</span>
<span id="cb563-3"><a href="chapter6.html#cb563-3"></a>            <span class="dt">ylab=</span><span class="st">&quot;Hc (% blood)&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Body fat (% weight)&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure6-15"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-15-1.png" alt="Scatterplot of Hematocrit versus Body Fat for female athletes." width="576" />
<p class="caption">
Figure 6.15: Scatterplot of Hematocrit versus Body Fat for female athletes.
</p>
</div>
<p>Based on these results, the estimated regression equation is
<span class="math inline">\(\widehat{\text{Hc}}_i = 42.014 - 0.085\cdot\text{BodyFat}_i\)</span> with <span class="math inline">\(b_0 = 42.014\)</span>
and <span class="math inline">\(b_1 = 0.085\)</span>. The slope coefficient interpretation is: For a one
percent increase in body fat, we expect, on average, a -0.085% (blood) change
in Hematocrit for Australian female athletes. For the <span class="math inline">\(y\)</span>-intercept, the
interpretation is: For a 0% body fat female athlete, we expect a Hematocrit of
42.014% on average. Again, this <span class="math inline">\(y\)</span>-intercept involves extrapolation to a region
of <span class="math inline">\(x\)</span>’s that we did not observed. None of the athletes had body fat below 5% so we don’t know what would happen to
the hematocrit of an athlete that had no body fat except that it probably would not
continue to follow a linear relationship.</p>
</div>
<div id="section6-7" class="section level2">
<h2><span class="header-section-number">6.7</span> Least Squares Estimation</h2>
<p>The previous results used the <code>lm</code> function as a “black box” to generate
the estimated coefficients.

The lines produced probably look reasonable but you
could imagine drawing other lines that might look equally plausible. Because we
are interested in explaining variation in the response variable, we want a
model that in some sense minimizes the residuals <span class="math inline">\((e_i=y_i-\widehat{y}_i)\)</span>
and explains the responses as well as possible, in other words has
<span class="math inline">\(y_i-\widehat{y}_i\)</span> as small as possible.


We can’t just add these <span class="math inline">\(e_i\)</span>’s up because
it would always be 0 (remember why we use the variance to measure
spread from introductory statistics?). We use a similar technique in
regression, we find the regression line that minimizes the squared residuals
<span class="math inline">\(e^2_i=(y_i-\widehat{y}_i)^2\)</span> over all the observations, minimizing the
<strong><em>Sum of Squared Residuals</em></strong><span class="math inline">\(\boldsymbol{=\Sigma e^2_i}\)</span>.
Finding the estimated regression coefficients that minimize the sum of squared
residuals is called <strong><em>least squares estimation</em></strong> and provides us a
reasonable method for finding the “best” estimated regression line of all
the possible choices.</p>
<p>For the <em>Beers</em> vs <em>BAC</em> data, Figure <a href="chapter6.html#fig:Figure6-16">6.16</a> shows the
result of a search for the optimal slope coefficient between values of 0 and
0.03. The plot shows how the sum of
the squared residuals was minimized for the value that <code>lm</code> returned at
0.018. The main point of this is that if any other slope coefficient was tried,
it did not do as good <strong>on the least squares criterion</strong> as the least squares
estimates.</p>

<div class="figure"><span id="fig:Figure6-16"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-16-1.png" alt="Plot of sum of squared residuals vs possible slope coefficients for Beers vs BAC data, with vertical line for the least squares estimate that minimizes the sum of squared residuals." width="384" />
<p class="caption">
Figure 6.16: Plot of sum of squared residuals vs possible slope coefficients for <em>Beers</em> vs <em>BAC</em> data, with vertical line for the least squares estimate that minimizes the sum of squared residuals.
</p>
</div>
<p>Sometimes it is helpful to have a
go at finding the estimates yourself. If you install and load the <code>tigerstats</code>
<span class="citation">(Robinson and White <a href="#ref-R-tigerstats" role="doc-biblioref">2016</a>)</span> and <code>manipulate</code> <span class="citation">(Allaire <a href="#ref-R-manipulate" role="doc-biblioref">2014</a>)</span> packages in RStudio
and then run <code>FindRegLine()</code>, you get
a chance to try to find the optimal slope and intercept for a fake data set.


Click on the “sprocket” icon in the upper left of the plot and you will see
something like Figure <a href="chapter6.html#fig:Figure6-17">6.17</a>. This interaction can help you see
how the residuals
are being measuring in the <span class="math inline">\(y\)</span>-direction and appreciate that <code>lm</code> takes care of
this for us.
</p>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb564"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb564-1"><a href="chapter6.html#cb564-1"></a><span class="op">&gt;</span><span class="st"> </span><span class="kw">library</span>(tigerstats)</span>
<span id="cb564-2"><a href="chapter6.html#cb564-2"></a><span class="op">&gt;</span><span class="st"> </span><span class="kw">library</span>(manipulate)</span>
<span id="cb564-3"><a href="chapter6.html#cb564-3"></a><span class="op">&gt;</span><span class="st"> </span><span class="kw">FindRegLine</span>()</span>
<span id="cb564-4"><a href="chapter6.html#cb564-4"></a></span>
<span id="cb564-5"><a href="chapter6.html#cb564-5"></a>Equation of the regression line is<span class="op">:</span></span>
<span id="cb564-6"><a href="chapter6.html#cb564-6"></a>y =<span class="st"> </span><span class="fl">4.34</span> <span class="op">+</span><span class="st"> </span><span class="fl">-0.02</span>x</span>
<span id="cb564-7"><a href="chapter6.html#cb564-7"></a></span>
<span id="cb564-8"><a href="chapter6.html#cb564-8"></a>Your final score is <span class="fl">13143.99</span></span>
<span id="cb564-9"><a href="chapter6.html#cb564-9"></a>Thanks <span class="cf">for</span> playing<span class="op">!</span></span></code></pre></div>

<div class="figure" style="text-align: center"><span id="fig:Figure6-17"></span>
<img src="chapter6_files/image063.png" alt="Results of running FindRegLine() where I didn’t quite find the least squares line. The correct line is the bold (red) line and produced a smaller sum of squared residuals than the guessed thinner (black) line." width="100%" />
<p class="caption">
Figure 6.17: Results of running <code>FindRegLine()</code> where I didn’t quite find the least squares line. The correct line is the bold (red) line and produced a smaller sum of squared residuals than the guessed thinner (black) line.
</p>
</div>
<p>It ends up that the least squares
criterion does not require a search across coefficients or trial and error –
there are some “simple” equations available for calculating the estimates of
the <span class="math inline">\(y\)</span>-intercept and slope:</p>
<p><span class="math display">\[b_1 = \frac{\Sigma_i(x_i-\bar{x})(y_i-\bar{y})}{\Sigma_i(x_i-\bar{x})^2}
=r\frac{s_y}{s_x} \text{ and } b_0 = \bar{y} - b_1\bar{x}.\]</span></p>
<p>You will never
need to use these equations but they do inform some properties of the
regression line. The slope coefficient, <span class="math inline">\(b_1\)</span>, is based on
the variability in <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> and the correlation between them. If
<span class="math inline">\(\boldsymbol{r}=0\)</span>, then the slope coefficient will
also be 0. The intercept is a function of the means of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> and
what the estimated slope coefficient is. <strong>If the slope coefficient,
<span class="math inline">\(\boldsymbol{b_1}\)</span>, is 0, then</strong> <span class="math inline">\(\boldsymbol{b_0=\bar{y}}\)</span> (which is just the
mean of the response variable for all observed values of <span class="math inline">\(x\)</span> – this is
a very boring model!). The slope is 0 when the correlation is 0. So when
there is no linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> (<span class="math inline">\(r=0\)</span>), the least
squares regression line is a horizontal line with height <span class="math inline">\(\bar{y}\)</span>, and
the line produces the same fitted values for all <span class="math inline">\(x\)</span> values. You can also
think about this as when there is no relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, the
best prediction of <span class="math inline">\(y\)</span> is the mean of the <span class="math inline">\(y\)</span>-values and it doesn’t change
based on the values of <span class="math inline">\(x\)</span>. It is less obvious in these equations, but they
also imply that <strong>the regression line ALWAYS goes through the point</strong>
<span class="math inline">\(\boldsymbol{(\bar{x},\bar{y}).}\)</span> It provides a sort of anchor point
for all regression lines.</p>
<p>For one more example, we can
revisit the Montana wildfire areas burned (log-hectares) and the average summer
temperature (degrees F), which had <span class="math inline">\(\boldsymbol{r}=0.81\)</span>. The interpretations of the
different parts of the regression model follow the least squares estimation
provided by <code>lm</code>:
</p>
<div class="sourceCode" id="cb565"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb565-1"><a href="chapter6.html#cb565-1"></a>fire1 &lt;-<span class="st"> </span><span class="kw">lm</span>(loghectares<span class="op">~</span>Temperature, <span class="dt">data=</span>mtfires)</span>
<span id="cb565-2"><a href="chapter6.html#cb565-2"></a><span class="kw">summary</span>(fire1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = loghectares ~ Temperature, data = mtfires)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.0822 -0.9549  0.1210  1.0007  2.4728 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -69.7845    12.3132  -5.667 1.26e-05
## Temperature   1.3884     0.2165   6.412 2.35e-06
## 
## Residual standard error: 1.476 on 21 degrees of freedom
## Multiple R-squared:  0.6619, Adjusted R-squared:  0.6458 
## F-statistic: 41.12 on 1 and 21 DF,  p-value: 2.347e-06</code></pre>
<ul>
<li><p>Regression Equation (Completely Specified):</p>
<ul>
<li><p>Estimated model: <span class="math inline">\(\widehat{\text{log(Ha)}} = -69.78 + 1.39\cdot\text{Temp}\)</span></p></li>
<li><p>Or <span class="math inline">\(\widehat{y} = -69.78 + 1.39x\)</span> with <strong>Y=log(Ha) and X=Temperature</strong></p></li>
</ul></li>
<li><p>Response Variable: Yearly <em>log</em> Hectares burned by wildfires</p></li>
<li><p>Explanatory Variable: Average Summer Temperature</p></li>
<li><p>Estimated <span class="math inline">\(y\)</span>-Intercept (<span class="math inline">\(b_0\)</span>): -69.78</p></li>
<li><p>Estimated slope (<span class="math inline">\(b_1\)</span>): 1.39</p></li>
<li><p>Slope Interpretation: For a 1 degree Fahrenheit increase in
Average Summer Temperature we would expect, <strong>on average</strong>, a 1.39 log(Hectares)
<span class="math inline">\(\underline{change}\)</span> in log(Hectares) burned in Montana.</p></li>
<li><p><span class="math inline">\(Y\)</span>-intercept Interpretation: If temperature were 0 degrees F, we would
expect -69.78 log(Hectares) burned <strong>on average</strong> in Montana.</p></li>
</ul>
<p>One other use of regression equations is for prediction. It is a trivial
exercise (or maybe not – we’ll see when you try it!) to plug an <span class="math inline">\(x\)</span>-value of
interest into the regression equation and get an estimate for <span class="math inline">\(y\)</span> at that <span class="math inline">\(x\)</span>.
Basically, the regression lines displayed in the scatterplots show the
predictions from the regression line across the range of <span class="math inline">\(x\text{&#39;s}\)</span>. 
Formally, <strong><em>prediction</em></strong> involves estimating the response for a particular
value of <span class="math inline">\(x\)</span>. We know that it won’t be perfect but it is our best guess.
Suppose that we are interested in
predicting the log-area burned for a summer that had an average temperature of
<span class="math inline">\(59^\circ\text{F}\)</span>. If we plug <span class="math inline">\(59^\circ\text{F}\)</span> into the regression
equation, <span class="math inline">\(\widehat{\text{log(Ha)}} = -69.78 + 1.39\bullet \text{Temp}\)</span>,
we get</p>
<p><span class="math display">\[\begin{array}{rl} \\ \require{cancel} \widehat{\log(\text{Ha})}&amp;= -69.78\text{ log-hectares }+ 1.39\text{ log-hectares}/^\circ \text{F}\bullet 59^\circ\text{F} \\&amp;= -69.78\text{ log-hectares } +1.39\text{ log-hectares}/\cancel{^\circ \text{F}}\bullet 59\cancel{^\circ \text{F}} \\&amp;= 12.23 \text{ log-hectares} \\ \end{array}\]</span></p>
<p>We did not observe any summers at exactly <span class="math inline">\(x=59\)</span> but did observe some
nearby and this result seems relatively reasonable.</p>
<p>Now suppose someone asks you to use this equation for predicting
<span class="math inline">\(\text{Temperature} = 65^\circ F\)</span>. We can run that through the equation:
<span class="math inline">\(-69.78 + 1.39*65 = 20.57\)</span> log-hectares. But can we trust this prediction? We did
not observe any summers over 60 degrees F so we are now predicting outside the
scope of our observations – performing <strong><em>extrapolation</em></strong>.  Having a scatterplot in
hand helps us to assess the range of values where we can reasonably use the
equation – here between 54 and 60 degrees F seems reasonable.</p>

<div class="sourceCode" id="cb567"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb567-1"><a href="chapter6.html#cb567-1"></a><span class="kw">scatterplot</span>(loghectares<span class="op">~</span>Temperature, <span class="dt">data=</span>mtfires, <span class="dt">regLine=</span>T, <span class="dt">smooth=</span>F, <span class="dt">spread=</span>F, <span class="dt">pch=</span><span class="dv">16</span>,</span>
<span id="cb567-2"><a href="chapter6.html#cb567-2"></a>            <span class="dt">main=</span><span class="st">&quot;Scatterplot with regression line for Area burned vs Temperature&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure6-18"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-18-1.png" alt="Scatterplot of log-hectares burned versus temperature with estimated regression line." width="576" />
<p class="caption">
Figure 6.18: Scatterplot of log-hectares burned versus temperature with estimated regression line.
</p>
</div>
</div>
<div id="section6-8" class="section level2">
<h2><span class="header-section-number">6.8</span> Measuring the strength of regressions: R<sup>2</sup></h2>
<p>At the beginning of the chapter,
we used the correlation coefficient to measure the strength and direction of
the linear relationship. The regression line provides an even more detailed
description of the direction of the linear relationship than the correlation
provided; in regression we addressed the question of “for a unit change in <span class="math inline">\(x\)</span>,
what sort of change in <span class="math inline">\(y\)</span> do we expect, on average?” whereas the
correlation just addressed whether the relationship was positive or negative.
However, the <strong>regression line tells us nothing about the strength of the
relationship</strong>. Consider the three scatterplots in Figure <a href="chapter6.html#fig:Figure6-19">6.19</a>:
the left panel is the original <em>BAC</em> data and the two right
panels have fake data that generated exactly the same estimated regression model with a
weaker (middle panel) and then a stronger (right panel) linear relationship
between <code>Beers</code> and <code>BAC</code>. This suggests that the regression
line is a useful but incomplete characterization of relationships between
variables – we need a measure of strength of the relationship to go with the
equation.</p>
<p>We could use the correlation coefficient, <strong><em>r</em></strong>, again to characterize
strength but it is somewhat redundant to report a measure that contains
direction information. It also will not extend to multiple
regression models where we have more than one predictor variable in the same
model.</p>
<p>In regression models, we use the <strong><em>coefficient of determination</em></strong>
(symbol: <strong>R<sup>2</sup></strong>) to accompany our regression line and describe
the strength of the relationship. It can either be
scaled between 0 and 1 or 0 to 100% and has “units” of the proportion or
percentage of the variation in <span class="math inline">\(y\)</span> that is explained by the model that
includes <span class="math inline">\(x\)</span> (and later more than one <span class="math inline">\(x\)</span>). For example, an <strong>R<sup>2</sup></strong>
of 0% corresponds to explaining 0% of the variation in the response with our
model and <span class="math inline">\(\boldsymbol{R^2} = 100\%\)</span> means that all the variation in the
response was explained by the model. In between, it provides a nice summary
of how much of the total variability in the response we can account for
with our model
including <span class="math inline">\(x\)</span> (and, in Chapter <a href="chapter8.html#chapter8">8</a>, including multiple
predictor variables).  </p>

<div class="figure"><span id="fig:Figure6-19"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-19-1.png" alt="Three scatterplots with the same estimated regression line." width="576" />
<p class="caption">
Figure 6.19: Three scatterplots with the same estimated regression line.
</p>
</div>
<p>The <strong>R<sup>2</sup></strong> is calculated using the sums of squares we encountered in the
ANOVA methods.

We once again have some total amount of variability that is
attributed to the variation based on the model fit, here we call it
<span class="math inline">\(\text{SS}_\text{regression}\)</span>, and the residual variability, still
<span class="math inline">\(\text{SS}_\text{error}=\Sigma(y-\widehat{y})^2\)</span>. The <span class="math inline">\(\text{SS}_\text{regression}\)</span>
is most easily calculated as
<span class="math inline">\(\text{SS}_\text{regression} = \text{SS}_\text{Total} - \text{SS}_\text{error}\)</span>,
the difference between the total
variability and the variability not explained by the model under consideration.
Using these quantities, we calculate the portion of the total variability that
the model explains as</p>
<p><span class="math display">\[\boldsymbol{R^2}=\frac{\text{SS}_\text{regression}}{\text{SS}_\text{Total}}
=1 - \frac{\text{SS}_\text{error}}{\text{SS}_\text{Total}}.\]</span></p>
<p>It also ends up that the
coefficient of determination for models with one predictor is the correlation
coefficient (<strong><em>r</em></strong>) squared (<span class="math inline">\(\boldsymbol{R^2} = \boldsymbol{r^2}\)</span>). So we can
quickly find coefficients of determination if we know correlations in simple
linear regression models. In the real <em>Beers</em> and <em>BAC</em> data, <strong><em>r</em></strong>=0.8943.
So <span class="math inline">\(\boldsymbol{R^2} = 0.79998\)</span> or approximately 0.80. So 80% of the variation in
<em>BAC</em> is explained by <em>Beer</em> consumption. That leaves 20% of the variation in
the responses to be unexplained by our model. In this case much of the
unexplained variation is likely attributable to
differences in physical characteristics (that were not measured) but the
statistical model places that unexplained variation into the category of
“random errors”. We don’t actually have to find <strong><em>r</em></strong> to get coefficients of
determination – the result is part of the regular summary of a regression
model that we have not discussed.
We repeat the full <code>lm</code> model summary below – note that a number is reported for the “<code>Multiple R-squared</code>” in the second to last line of the output. It is reported as a
proportion and it is your choice whether you want to report
and interpret it as a proportion or percentage, just make that clear in how you
discuss it.</p>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb568"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb568-1"><a href="chapter6.html#cb568-1"></a>m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(BAC<span class="op">~</span>Beers, <span class="dt">data=</span>BB)</span>
<span id="cb568-2"><a href="chapter6.html#cb568-2"></a><span class="kw">summary</span>(m1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = BAC ~ Beers, data = BB)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.027118 -0.017350  0.001773  0.008623  0.041027 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -0.012701   0.012638  -1.005    0.332
## Beers        0.017964   0.002402   7.480 2.97e-06
## 
## Residual standard error: 0.02044 on 14 degrees of freedom
## Multiple R-squared:  0.7998, Adjusted R-squared:  0.7855 
## F-statistic: 55.94 on 1 and 14 DF,  p-value: 2.969e-06</code></pre>
<p>In this output, be careful because there is another related quantity called
<strong><em>Adjusted</em></strong> <strong>R-squared</strong> that we will discuss later. This other quantity
is not a measure of the strength of the
relationship but will be useful.</p>
<p>We could also revisit the ANOVA
table for this model to verify the source of the <strong>R<sup>2</sup></strong> of 0.80
based on <span class="math inline">\(\text{SS}_\text{regression}= 0.02337\)</span> and
<span class="math inline">\(\text{SS}_\text{Total} = 0.02337+0.00585\)</span>. This provides 0.80 from
<span class="math inline">\(0.02337/0.02922\)</span>.</p>
<div class="sourceCode" id="cb570"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb570-1"><a href="chapter6.html#cb570-1"></a><span class="kw">anova</span>(m1)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: BAC
##           Df    Sum Sq   Mean Sq F value    Pr(&gt;F)
## Beers      1 0.0233753 0.0233753  55.944 2.969e-06
## Residuals 14 0.0058497 0.0004178</code></pre>
<div class="sourceCode" id="cb572"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb572-1"><a href="chapter6.html#cb572-1"></a>SStotal &lt;-<span class="st"> </span><span class="fl">0.0233753</span> <span class="op">+</span><span class="st"> </span><span class="fl">0.0058497</span></span>
<span id="cb572-2"><a href="chapter6.html#cb572-2"></a>SSregression &lt;-<span class="st"> </span><span class="fl">0.0233753</span></span>
<span id="cb572-3"><a href="chapter6.html#cb572-3"></a>SSregression<span class="op">/</span>SStotal</span></code></pre></div>
<pre><code>## [1] 0.7998392</code></pre>
<p>In Figure <a href="chapter6.html#fig:Figure6-19">6.19</a>, there are three
examples with the same regression model, but different strengths of
relationships. In the real data set <span class="math inline">\(\boldsymbol{R^2} = 80\%\)</span>. For the first
fake data set (middle panel), the <strong>R<sup>2</sup></strong> drops to <span class="math inline">\(13.8\%\)</span> and for
the second fake data set (right panel), <strong>R<sup>2</sup></strong> is <span class="math inline">\(97.3\%\)</span>. As a
summary, <strong>R<sup>2</sup></strong> provides a natural scale to understand “how good”
each model is at explaining the responses. We can revisit some of our previous
models to get a little more practice with using this summary of strength or quality
of regression models.</p>
<p>For the Montana fire data, <span class="math inline">\(\boldsymbol{R^2}=66.2\%\)</span>. So the proportion of
variation of log-area burned that is explained by average summer temperature
is 0.662. This is “good” but also
leaves quite a bit of unexplained variation in the responses. There is a long
list of reasons why this explanatory variable leaves a lot of variation in the
response unexplained. Note that we were careful about using the scaling of the
response variable (log(area burned)) in the interpretation – this is because we
would get a much different answer if area burned vs temperature was considered.</p>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb574"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb574-1"><a href="chapter6.html#cb574-1"></a>fire1 &lt;-<span class="st"> </span><span class="kw">lm</span>(loghectares<span class="op">~</span>Temperature, <span class="dt">data=</span>mtfires)</span>
<span id="cb574-2"><a href="chapter6.html#cb574-2"></a><span class="kw">summary</span>(fire1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = loghectares ~ Temperature, data = mtfires)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.0822 -0.9549  0.1210  1.0007  2.4728 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -69.7845    12.3132  -5.667 1.26e-05
## Temperature   1.3884     0.2165   6.412 2.35e-06
## 
## Residual standard error: 1.476 on 21 degrees of freedom
## Multiple R-squared:  0.6619, Adjusted R-squared:  0.6458 
## F-statistic: 41.12 on 1 and 21 DF,  p-value: 2.347e-06</code></pre>
<p>For the model for female Australian athletes that used <em>Body fat</em> to
explain <em>Hematocrit</em>, the estimated regression model was
<span class="math inline">\(\widehat{\text{Hc}}_i = 42.014 - 0.085\cdot\text{BodyFat}_i\)</span> and
<span class="math inline">\(\boldsymbol{r} = -0.168\)</span>. The coefficient of determination is
<span class="math inline">\(\boldsymbol{R^2} = (-0.168)^2 = 0.0282\)</span>. So <em>body fat</em> explains 2.8% of
the variation in <em>Hematocrit</em> in these women. That is not a very good
regression model with over 97% of the variation in <em>Hematocrit</em> unexplained
by this model. The scatterplot showed a fairly weak relationship but this
provides numerical and interpretable information that drives that point home.</p>
<div class="sourceCode" id="cb576"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb576-1"><a href="chapter6.html#cb576-1"></a>m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Hc<span class="op">~</span>Bfat, <span class="dt">data=</span><span class="kw">subset</span>(aisR2,Sex<span class="op">==</span><span class="dv">1</span>)) <span class="co">#Results for Females</span></span>
<span id="cb576-2"><a href="chapter6.html#cb576-2"></a><span class="kw">summary</span>(m2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Hc ~ Bfat, data = subset(aisR2, Sex == 1))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.2399 -2.2132 -0.1061  1.8917  6.6453 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 42.01378    0.93269  45.046   &lt;2e-16
## Bfat        -0.08504    0.05067  -1.678   0.0965
## 
## Residual standard error: 2.598 on 97 degrees of freedom
## Multiple R-squared:  0.02822,    Adjusted R-squared:  0.0182 
## F-statistic: 2.816 on 1 and 97 DF,  p-value: 0.09653</code></pre>
<div style="page-break-after: always;"></div>
</div>
<div id="section6-9" class="section level2">
<h2><span class="header-section-number">6.9</span> Outliers: leverage and influence</h2>
<p>In the review of correlation, we
loosely considered the impacts of outliers on the correlation.  We removed
unusual points to see both the visual changes (in the scatterplot) as well as
changes in the correlation coefficient in Figures <a href="chapter6.html#fig:Figure6-4">6.4</a>
and <a href="chapter6.html#fig:Figure6-5">6.5</a>. In this section,
we formalize these ideas in the context of impacts of unusual points on our
regression equation. In regression, it is possible for a single point to have a
big impact on the overall regression results but it is also possible to have a
clear outlier that has little impact on the results. We call an observation
<strong><em>influential</em></strong> if its removal causes a “big” change in the regression line,
specifically in terms of impacting the
slope coefficient.  Points that are on the edges of the <span class="math inline">\(x\text{&#39;s}\)</span>
(far from the mean of the <span class="math inline">\(x\text{&#39;s}\)</span>) have the potential for more impact
on the line as we will see in some examples shortly.</p>
<p>You can think of the regression line being balanced at <span class="math inline">\(\bar{x}\)</span> and the
further from that location a point is, the more a single point can move
the line. We can measure the distance of
points from <span class="math inline">\(\bar{x}\)</span> to quantify each observation’s potential for impact
on the line using what is called the <strong><em>leverage</em></strong> of a point.  Leverage
is a positive numerical measure with larger values corresponding to more
leverage. The scale changes depending on the sample size (<span class="math inline">\(n\)</span>) and the
complexity of the model so all that matters is which observations have
more or less relative leverage in a particular data set. The observations
with <span class="math inline">\(x\)</span>-values
that provide higher leverage have increased potential to influence the
estimated regression line. Along with measuring the leverage, we can also
measure the influence that each point has on the regression line using
<strong><em>Cook’s Distance</em></strong> or <strong><em>Cook’s D</em></strong>.  It also is a positive
measure with higher values suggesting more influence. The rule of thumb is that
Cook’s D values over 1.0 correspond to clearly influential points, values over
0.5 have some influence and values lower than 0.5 indicate points that are not
influential on the regression model slope coefficients. One part of the regular
diagnostic plots we will use for regression models displays the leverages on
the <span class="math inline">\(x\)</span>-axis, the standardized residuals on the <span class="math inline">\(y\)</span>-axis, and adds contour lines
for Cook’s Distances in a panel that is labeled “Residuals vs Leverage”.

This
allows us to see the potential for impact of a point (leverage), how far it’s
observation was from the regression line (residual), and to see a measure of
that point’s influence (Cook’s D).
</p>

<div class="figure"><span id="fig:Figure6-20"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-20-1.png" alt="Scatterplot and Residuals vs Leverage plot for the real BAC data. Two high leverage points are flagged, with only one that has a Cook’s D value over 1 (“\(\circ\)”) and is indicated as influential." width="576" />
<p class="caption">
Figure 6.20: Scatterplot and Residuals vs Leverage plot for the real BAC data. Two high leverage points are flagged, with only one that has a Cook’s D value over 1 (“<span class="math inline">\(\circ\)</span>”) and is indicated as influential.
</p>
</div>
<p>To extract the level of Cook’s D on the “Residuals vs Leverage” plot,
look for contours
to show up on the upper and lower right of the plot.

They show increasing
levels of influence going to the upper and lower right corners as you combine
higher leverage (<span class="math inline">\(x\)</span>-axis) and larger residuals (<span class="math inline">\(y\)</span>-axis) – the two ingredients
required to be influential on the line. The contours are displayed for Cook’s D
values of 0.5 and 1.0 if there are points near or over those levels. The Cook’s
D values come from a topographical surface of values that is a sort of U-shaped
valley in the middle of the plot centered at <span class="math inline">\(y=0\)</span> with the lowest contour
corresponding to Cook’s D values below 0.5 (no influence). As you move to the
upper right or lower right corners, the influence increases and the edges of
the valley get steeper. If you do not see any contours in the plot, then no
points were even close to being influential based on Cook’s D.</p>
<p>To illustrate these concepts, the original <em>Beers</em> and <em>BAC</em> data
are used again. In the scatter plot in Figure <a href="chapter6.html#fig:Figure6-20">6.20</a>,
two points are plotted with different characters.
The point for 1 <em>Beer</em> and <em>BAC</em> of 0.010 is displayed as a “<span class="math inline">\(\diamond\)</span>”
and the 9 <em>Beer</em> and <em>BAC</em> 0.19 observation is displayed with a “<span class="math inline">\(\circ\)</span>”.
These two points are the furthest from the mean of the of the <span class="math inline">\(x\text{&#39;s}\)</span>
(<span class="math inline">\(\overline{\text{Beers}}= 4.8\)</span>) but show two different levels of
influence on the line. The “<span class="math inline">\(\diamond\)</span>” point has a leverage of 0.27 and the 9 <em>Beer</em> observation (“<span class="math inline">\(\circ\)</span>”) had a leverage of 0.30. The 1 <em>Beer</em> observation was close to
the pattern defined by the other points, had a small residual, and a Cook’s D
value below 0.5 (it did not exceed the first of the contours). So even though
it had high leverage, it was not an influential point. The 9 <em>Beer</em> observation
had the highest leverage in the data set and was quite a bit above the pattern
defined by the other points and ends up being an influential point with a
Cook’s D over 1. We might want to consider fitting this model without that
observation to get a better estimate of the effects of beer consumption on BAC
or revisit our assumption that the relationship is really linear here.</p>
<p>To further explore influence, we will add a point to the original data set and
move it around so you can see how those changes impact the results. For each
scatterplot in Figure <a href="chapter6.html#fig:Figure6-21">6.21</a>, the Residuals vs Leverage
plot is displayed to its right.

The original data are “<span class="math inline">\(\color{Grey}{\bullet}\)</span>”
and the original regression line is the dashed line in
Figure <a href="chapter6.html#fig:Figure6-21">6.21</a>. First, a fake observation at 11 <em>Beers</em> and
0.1 <em>BAC</em> is added,
at (11, 0.1), in the top panels of the figure. This observation is clearly an
outlier and heavily impacts the slope of the regression line (so is clearly
influential). This added point drops the <strong>R<sup>2</sup></strong> from 0.80 in the
original data to 0.24. The accompanying Residuals vs Leverage plot shows
that this point has extremely high leverage and a Cook’s D over 1 – it
is a clearly influential point. However, <strong>having high leverage does not
always make points influential.</strong>
Consider the second row of plots with an added point of (11, 0.19). The
regression line barely changes and <strong>R<sup>2</sup></strong> increases a little.
This point has the same leverage as in the first example since it is the
same set of <span class="math inline">\(x\text{&#39;s}\)</span> and the distance to the mean of the <span class="math inline">\(x\)</span>’s is unchanged. But
it is not influential
since its Cook’s D value is less than 0.5. This occurred because it followed the
overall pattern of observations even though it was “far away” from the other
observations in the <span class="math inline">\(x\)</span>-direction. The last two rows of plots show what happens
when low leverage outliers are encountered. If observations are near the
center of the <span class="math inline">\(x\text{&#39;s}\)</span>, it ends up that to be influential the points have
to be very far
from the pattern of the other observations. The (5, 0.19) example almost
attains a Cook’s D of 0.5 but has little impact on the regression line,
especially the slope coefficient. It does impact the <span class="math inline">\(y\)</span>-intercept and drops the
R-squared value to 0.57. The same result occurs if the observation is
noticeably lower than the other points.</p>

<div class="figure"><span id="fig:Figure6-21"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-21-1.png" alt="Plots exploring the impacts of moving a single additional observation in the BAC example. The added point is indicated with * and the original regression line is the dashed line in the left column." width="768" />
<p class="caption">
Figure 6.21: Plots exploring the impacts of moving a single additional observation in the BAC example. The added point is indicated with * and the original regression line is the dashed line in the left column.
</p>
</div>
<p>When we are doing regressions, we get very worried about points “at the edges”
having an undue influence on the results. When we start using multiple
predictors, say if we had body weight data on these subjects as well as beer
consumption, it becomes harder to “see” if the points are “far away” from the
other observations and we will trust the Residuals vs Leverage plots to help us
identify the influential points. These techniques work the same in the multiple
regression models in Chapter <a href="chapter8.html#chapter8">8</a> as they do in these
simpler, single predictor
regression models.</p>
</div>
<div id="section6-10" class="section level2">
<h2><span class="header-section-number">6.10</span> Residual diagnostics – setting the stage for inference</h2>
<p>Influential points are not the
only potential issue that can cause us to have concerns about our regression
model. There are two levels to these considerations. The first is related to
issues that directly impact the least squares regression line and cause
concerns about whether a line is a reasonable representation of the
relationship between the two variables. These issues for regression model
estimation have been discussed previously (the same concerns in estimating
correlation apply to regression models). The second level is whether the line
we have will be useful for making inferences for the population that our data
were collected from and whether the data follow our assumed model. Our window
into problems of both types is the residuals <span class="math inline">\((e_i = y_i - \widehat{y}_i)\)</span>.

By exploring patterns in how the line “misses” the responses we can gain
information about the reasonableness of using the estimated regression line and
sometimes information about how we might fix problems. The validity conditions
for doing inference in a regression setting (Chapter <a href="chapter7.html#chapter7">7</a>)
involve two sets of considerations,
those that are assessed based on the data collection and measurement process
and those that can be assessed using diagnostic plots.


The first set is:</p>
<ul>
<li><p><strong>Quantitative variables condition</strong></p>
<ul>
<li>We’ll discuss using categorical predictor variables later – to
use simple linear regression both the explanatory and response
variables need to quantitative.</li>
</ul></li>
<li><p><strong>Independence of observations</strong> </p>
<ul>
<li><p>As in the ANOVA models, linear regression models assume that
the observations are collected in a fashion that makes them
independent.</p></li>
<li><p>This will be based on the “story” of the data. Consult a
statistician if your data violate this assumption as there
are more advanced methods that adjust for dependency in
observations but they are beyond the scope of this material.</p></li>
</ul></li>
</ul>
<p>The remaining assumptions for getting valid inferences from
regression models can be assessed using diagnostic plots:</p>
<ul>
<li><p><strong>Linearity of relationship</strong></p>
<ul>
<li><p>We should not report a linear regression model if the data
show a curve (curvilinear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>).</p></li>
<li><p>Examine the initial scatterplot to assess the potential for a curving relationship.</p></li>
<li><p>Examine the Residuals vs Fitted (top left panel of diagnostic plot display) plot:
</p>
<ul>
<li><p>If the model missed a curve in the relationship, the residuals
often will highlight that missed pattern and a curve will show
up in this plot.</p></li>
<li><p>Try to explain or understand the pattern in what is left over.
If we have a good model, there shouldn’t be much left to
“explain” in the residuals (i.e., no patterns left over after
accounting for <span class="math inline">\(x\)</span>).</p></li>
</ul></li>
</ul></li>
<li><p><strong>Equal (constant) variance</strong></p>
<ul>
<li><p>We assume that the variation is the same for all the observations
and especially that the variability does not change in the responses
as a function of our predictor variables or the fitted values.</p></li>
<li><p>There are three plots to help with this:</p>
<ul>
<li><p>Examine the original scatterplot and look at the variation around the line and whether it looks constant across values of <span class="math inline">\(x\)</span>.</p></li>
<li><p>Examine the Residuals vs Fitted plot and look for evidence of
changing spread in the residuals, being careful to try to
separate curving patterns from non-constant variance (and look
for situations where both are present as you can violate both
conditions simultaneously).
</p></li>
<li><p>Examine the “Scale-Location” plot and look for changing spread
as a function of the fitted values.
</p>
<ul>
<li><p>The <span class="math inline">\(y\)</span>-axis in this plot is the square-root of the absolute
value of the standardized residual. This scale flips the
negative residuals on top of the positive ones to help you
better assess changing variability without being distracted
by whether the residuals are above or below 0.</p></li>
<li><p>Because of the absolute value, curves in the Residuals vs
Fitted plot can present as sort of looking like non-constant
variance in the Scale-Location plot – check for nonlinearity in the residuals vs fitted values
before using this plot. If nonlinearity is present, just use
the Residuals vs Fitted and original scatterplot for assessing constant variance around
the curving pattern.</p></li>
</ul></li>
</ul></li>
<li><p>If there are patterns of increasing or decreasing variation (often
described as funnel or cone shapes), then it might be possible to use a
transformation to fix this problem (more later). It is possible to have
decreasing and then increasing variability and this also is
a violation of this condition.
 </p></li>
</ul></li>
<li><p><strong>Normality of residuals</strong></p>
<ul>
<li><p>Examine the Normal QQ-plot for violations of the normality assumption
as in Chapters <a href="chapter3.html#chapter3">3</a> and <a href="chapter4.html#chapter4">4</a>.
</p>
<ul>
<li><p>Specifically review the discussion of identifying skews in
different directions and heavy vs light tailed distributions.</p></li>
<li><p>Skewed and heavy-tailed distributions are the main problems
for our inferences, especially since both kinds of distributions
can contain outliers that can wreak havoc on the estimated
regression line.</p></li>
<li><p>Light-tailed distributions cause us no real inference issues
except that the results are conservative so you should note when
you observe these situations but feel free to proceed with using
your model results.</p></li>
<li><p>Remember that clear outliers are an example of a violation of
the normality assumption but some outliers may just influence
the regression line and make it fit poorly and this issue will be more clearly observed in the
residuals vs fitted than in the QQ-plot.</p></li>
</ul></li>
</ul></li>
<li><p><strong>No influential points</strong></p>
<ul>
<li><p>Examine the Residuals vs Leverage plot as discussed in the previous
section.   </p></li>
<li><p>Consider removing influential points (one at a time) and focusing on results without
those points in the data set.</p></li>
</ul></li>
</ul>
<p>To assess these later assumptions, we will use the four residual diagnostic
plots that R provides from <code>lm</code> fitted models. They are similar to the
results from ANOVA models but the Residuals vs Leverage plot is
now interesting as was discussed in Section <a href="chapter6.html#section6-9">6.9</a>.

Now we can
fully assess the
potential for trusting the estimated regression models in a couple of our
examples:</p>
<ul>
<li><p><strong>Beers vs BAC:</strong></p>
<ul>
<li><p>Quantitative variables condition:</p>
<ul>
<li>Both variables are quantitative.</li>
</ul></li>
<li><p>Independence of observations:</p>
<ul>
<li>We can assume that all the subjects are independent of each
other. There is only one measurement per student and it is
unlikely that one subject’s beer consumption would impact
another’s BAC. Unless the students were trading blood it
isn’t possible for one person’s beer consumption to change
someone else’s BAC.</li>
</ul>
<div class="sourceCode" id="cb578"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb578-1"><a href="chapter6.html#cb578-1"></a>m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(BAC<span class="op">~</span>Beers, <span class="dt">data=</span>BB)</span>
<span id="cb578-2"><a href="chapter6.html#cb578-2"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb578-3"><a href="chapter6.html#cb578-3"></a><span class="kw">plot</span>(m1, <span class="dt">add.smooth=</span>F, <span class="dt">main=</span><span class="st">&quot;Beers vs BAC&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure6-22"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-22-1.png" alt="Full suite of diagnostics plots for *Beer* vs *BAC* data." width="960" />
<p class="caption">
Figure 6.22: Full suite of diagnostics plots for <em>Beer</em> vs <em>BAC</em> data.
</p>
</div></li>
<li><p>Linearity, constant variance from Residuals vs Fitted:</p>
<ul>
<li>We previously have identified a potentially influential
outlier point in these data. Consulting the Residuals vs
Fitted plot in Figure <a href="chapter6.html#fig:Figure6-22">6.22</a>, if you trust
that influential point, shows some curvature with a pattern of
decreasing residuals as a function of the fitted values and then an increase at the right. Or, if
you do not trust that highest BAC observation, then there is
a mostly linear relationship with an outlier identified. We
would probably suggest that it is an outlier, should be removed
from the analysis, and inferences constrained to the region of
beer consumption from 1 to 8 beers since we
don’t know what might happen at higher values.</li>
</ul></li>
<li><p>Constant variance from Scale-Location:</p>
<ul>
<li>There is some evidence of increasing variability in this
plot as the spread of the results increases from left to
right, however this is just an artifact
of the pattern in the original residuals and not real evidence of
non-constant variance. Note that there is little to no evidence of non-constant variance in the Residuals vs Fitted.</li>
</ul></li>
<li><p>Normality from Normal QQ Plot:</p>
<ul>
<li>The left tail is a little short and the right tail is a
little long, suggesting a slightly right
skewed distribution in the residuals. This also corresponds to
having a large positive outlying value. But we would conclude that there is a minor issue with normality in the residuals here.</li>
</ul></li>
<li><p>Influential points from Residuals vs Leverage:</p>
<ul>
<li>Previously discussed, this plot shows one influential point
with a Cook’s D value over 1 that is distorting the fitted model and is likely the biggest issue here.</li>
</ul></li>
</ul></li>
<li><p><strong>Tree height and tree diameter</strong> (suspicious observation already removed):</p>
<ul>
<li><p>Quantitative variables: Met</p></li>
<li><p>Independence of observations:</p>
<ul>
<li>There are multiple trees that were measured in each plot.
One problem might be that once a tree is established in an
area, the other trees might not grow as tall. The other problem
is that some sites might have better soil conditions
than others. Then, all the trees in those rich soil areas might be
systematically taller than the trees in other areas. Again, there are
statistical methods to account for this sort of “clustering” of
measurements but this technically violates the assumption that the
trees are independent of each other. So this assumption is violated, but
we will proceed with that caveat on our results – the precision of our
inferences might be slightly over-stated due to some potential
dependency in the measurements.</li>
</ul></li>
<li><p>Linearity, constant variance from Residuals vs Fitted in
Figure <a href="chapter6.html#fig:Figure6-23">6.23</a>.</p>
<ul>
<li><p>There is evidence of a curve that was missed by the linear model.</p></li>
<li><p>There is also evidence of increasing variability AROUND the curve
in the residuals.</p></li>
</ul></li>
<li><p>Constant variance from Scale-Location:</p>
<ul>
<li>This plot actually shows relatively constant variance but
this plot is misleading when curves are
present in the data set. <strong>Focus on the Residuals vs Fitted to
diagnose non-constant variance in situations where a curve was
missed.</strong></li>
</ul></li>
<li><p>Normality in Normal QQ plot:</p>
<ul>
<li>There is no indication of any problem with the normality
assumption.</li>
</ul></li>
<li><p>Influential points?</p>
<ul>
<li>The Cook’s D contours do not show up in this plot so none of
the points are influential.</li>
</ul></li>
</ul></li>
</ul>

<div class="figure"><span id="fig:Figure6-23"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-23-1.png" alt="Diagnostics plots for tree height and diameter simple linear regression model." width="960" />
<p class="caption">
Figure 6.23: Diagnostics plots for tree height and diameter simple linear regression model.
</p>
</div>
<p>So the main issues with this model are the curving relationship and
non-constant variance. We’ll revisit this example later to see if we can
find a model on transformed variables that has better diagnostics. Reporting the following regression model that has a decent <span class="math inline">\(R^2\)</span> of 62.6%
would be misleading since it does not accurately represent the relationship
between tree diameter and tree height.</p>
<div class="sourceCode" id="cb579"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb579-1"><a href="chapter6.html#cb579-1"></a>tree1 &lt;-<span class="st"> </span><span class="kw">lm</span>(height.m<span class="op">~</span>dbh.cm, <span class="dt">data=</span>ufc[<span class="op">-</span><span class="dv">168</span>,])</span>
<span id="cb579-2"><a href="chapter6.html#cb579-2"></a><span class="kw">summary</span>(tree1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = height.m ~ dbh.cm, data = ufc[-168, ])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -12.1333  -3.1154   0.0711   2.7548  12.3076 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 11.98364    0.57422   20.87   &lt;2e-16
## dbh.cm       0.32939    0.01395   23.61   &lt;2e-16
## 
## Residual standard error: 4.413 on 333 degrees of freedom
## Multiple R-squared:  0.626,  Adjusted R-squared:  0.6249 
## F-statistic: 557.4 on 1 and 333 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb581"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb581-1"><a href="chapter6.html#cb581-1"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb581-2"><a href="chapter6.html#cb581-2"></a><span class="kw">plot</span>(tree1, <span class="dt">add.smooth=</span>F)</span></code></pre></div>
</div>
<div id="section6-11" class="section level2">
<h2><span class="header-section-number">6.11</span> Old Faithful discharge and waiting times</h2>
<p>A study in August 1985 considered time for Old Faithful and how that might
relate to <em>waiting time</em> for the next eruption (<span class="citation">Ripley (<a href="#ref-R-MASS" role="doc-biblioref">2019</a>)</span>, <span class="citation">Azzalini and Bowman (<a href="#ref-Azzalini1990" role="doc-biblioref">1990</a>)</span>).

This sort of research provides the Yellowstone National Park (YNP) staff a way to show tourists a predicted
time to next eruption so they can quickly see it erupt and then get back in their
cars, not wasting too much time in the outdoors. Or, less cynically, the opportunity to study the behavior of the eruption of a geyser. Both variables are measured in minutes and the scatterplot in
Figure <a href="chapter6.html#fig:Figure6-24">6.24</a>
shows a moderate to strong positive and relatively linear relationship. We
added a <strong><em>smoothing line</em></strong> (dashed line) to this plot. Smoothing lines provide
regression-like fits but are performed on local areas of the relationship
between the two variables and so can highlight where the relationships
change, especially highlighting curvilinear relationships. They can also return
straight lines just like the regression line if that is reasonable. The
technical details of regression smoothing are not covered here but they are a
useful graphical addition to help visualize nonlinearity in relationships.</p>
<p>In these data, there appear to be two groups of eruptions (shorter length, shorter
wait and longer length, longer wait) – but we don’t know enough about these
data to assume that there are two groups. The smoothing line does help us to
see if the relationship appears to change or stay the same across different
values of the explanatory variable, <code>Duration</code>. The smoothing line suggests
that the upper group might have a less steep slope than the lower group as it
sort of levels off for observations with <code>Duration</code> of over 4 minutes. It
also indicates that there is one point for an eruption under 1 minute in
<code>Duration</code> that might be causing some problems. The story of these data
involve some measurements during the night
that were just noted as being short, medium, and long – and they were re-coded
as 2, 3, or 4 minute duration eruptions. You can see responses stacking up at 2 and 4 minute durations and this is obviously a problematic aspect of these data. We’ll see if our diagnostics detect
some of these issues when we fit a simple linear regression to try to explain
waiting time based on duration of prior eruption.</p>

<div class="sourceCode" id="cb582"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb582-1"><a href="chapter6.html#cb582-1"></a><span class="kw">library</span>(MASS)</span>
<span id="cb582-2"><a href="chapter6.html#cb582-2"></a><span class="kw">data</span>(geyser)</span>
<span id="cb582-3"><a href="chapter6.html#cb582-3"></a>geyser &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(geyser)</span>
<span id="cb582-4"><a href="chapter6.html#cb582-4"></a><span class="co">#Aligns the duration with time to next eruption</span></span>
<span id="cb582-5"><a href="chapter6.html#cb582-5"></a>G2 &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">Waiting=</span>geyser<span class="op">$</span>waiting[<span class="op">-</span><span class="dv">1</span>], <span class="dt">Duration=</span>geyser<span class="op">$</span>duration[<span class="op">-</span><span class="dv">299</span>]) </span>
<span id="cb582-6"><a href="chapter6.html#cb582-6"></a><span class="kw">scatterplot</span>(Waiting<span class="op">~</span>Duration, <span class="dt">data=</span>G2, <span class="dt">smooth=</span><span class="kw">list</span>(<span class="dt">spread=</span>F)) <span class="co">#Adds smoothing line</span></span></code></pre></div>
<div class="figure"><span id="fig:Figure6-24"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-24-1.png" alt="Scatterplot of Old Faithful waiting times to next eruption (minutes) and duration of prior eruption (minutes) with smoothing line (dashed) and regression line (solid)." width="576" />
<p class="caption">
Figure 6.24: Scatterplot of Old Faithful waiting times to next eruption (minutes) and duration of prior eruption (minutes) with smoothing line (dashed) and regression line (solid).
</p>
</div>
<p>An initial concern with these data
is that the observations are likely not independent. Since they were taken
consecutively, one waiting time might be related to the next waiting time –
violating the independence assumption. As noted above, there might be two
groups (types) of eruptions – short ones and long ones.
The Normal QQ-Plot in Figure <a href="chapter6.html#fig:Figure6-25">6.25</a>
also suggests a few observations creating a slightly long right tail.
Those observations might warrant further exploration as they also show up as
unusual in the Residuals vs Fitted plot. There are no highly influential points
in the data set with all points having Cook’s D smaller than 0.5 (contours are not displayed because no points are near or over them), so these
outliers are not necessarily moving the regression line around. There are two
distinct groups of observations but the variability is not clearly changing so
we do not have to worry about non-constant variance here. So these results might
be relatively trustworthy if we assume that the same relationship holds for all
levels of duration of eruptions.</p>
<div class="sourceCode" id="cb583"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb583-1"><a href="chapter6.html#cb583-1"></a>OF1 &lt;-<span class="st"> </span><span class="kw">lm</span>(Waiting<span class="op">~</span>Duration, <span class="dt">data=</span>G2)</span></code></pre></div>
<!-- \newpage -->
<div class="sourceCode" id="cb584"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb584-1"><a href="chapter6.html#cb584-1"></a><span class="kw">summary</span>(OF1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Waiting ~ Duration, data = G2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.6940  -4.4954  -0.0966   3.9544  29.9544 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  34.9452     1.1807   29.60   &lt;2e-16
## Duration     10.7751     0.3235   33.31   &lt;2e-16
## 
## Residual standard error: 6.392 on 296 degrees of freedom
## Multiple R-squared:  0.7894, Adjusted R-squared:  0.7887 
## F-statistic:  1110 on 1 and 296 DF,  p-value: &lt; 2.2e-16</code></pre>

<div class="sourceCode" id="cb586"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb586-1"><a href="chapter6.html#cb586-1"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb586-2"><a href="chapter6.html#cb586-2"></a><span class="kw">plot</span>(OF1)</span></code></pre></div>
<div class="figure"><span id="fig:Figure6-25"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-25-1.png" alt="Diagnostic plots for Old Faithful waiting time model." width="960" />
<p class="caption">
Figure 6.25: Diagnostic plots for Old Faithful waiting time model.
</p>
</div>
<p>The estimated regression equation is
<span class="math inline">\(\widehat{\text{WaitingTime}}_i = 34.95 + 10.78\cdot\text{Duration}_i\)</span>,
suggesting that for a 1 minute increase
in eruption <code>Duration</code> we would expect, on average, a 10.78 minute
change in the <code>WaitingTime</code>. This equation might provide a useful tool
for the YNP staff to predict waiting times. The <strong>R<sup>2</sup></strong> is
fairly large: 78.9% of the variation in <em>waiting time</em> is explained by the
<em>duration</em> of the previous eruption. But maybe this is more about two
types of eruptions/waiting
times? We could consider the relationship within the shorter and longer
eruptions but since there are observations residing between the two groups, it
is difficult to know where to split the explanatory variable into two groups.
Maybe we really need to measure additional information that might explain why
there are two groups in the responses…</p>
</div>
<div id="section6-12" class="section level2">
<h2><span class="header-section-number">6.12</span> Chapter summary</h2>
<p>The correlation coefficient (<span class="math inline">\(\boldsymbol{r}\)</span> or Pearson’s Product Moment
Correlation Coefficient) measures the strength
and direction of the linear relationship between two quantitative variables.
Regression models estimate the impacts of changes in <span class="math inline">\(x\)</span> on the mean of the
response variable <span class="math inline">\(y\)</span>. Direction of the assumed relationship (which variable
explains or causes the other) matters for regression models but does not matter
for correlation. Regression lines only describe the pattern of the
relationship; in regression, we use the coefficient of determination to
describe the strength of the relationship between the variables as a percentage
of the response variable that is explained by the model. If we are choosing
between models, we prefer them to have higher <span class="math inline">\(R^2\)</span> values for obvious
reasons, but we will discover in Chapter <a href="chapter8.html#chapter8">8</a> that maximizing
the coefficient of
determination is not a good way to pick a model when we have multiple candidate
options.</p>
<p>In this chapter, a wide variety of potential problems were explored when using
regression models. This included a discussion of the conditions that will be
required for using the models to perform trustworthy inferences in the remaining chapters. It is important to remember that correlation and regression models
only measure the <strong>linear</strong> association between variables and that can be
misleading if a nonlinear relationship is present. Similarly,
influential observations can completely distort the apparent relationship
between variables
and should be assessed before trusting any regression output. It is also
important to remember that regression lines should not be used outside the
scope of the original observations – extrapolation should be checked for and
avoided whenever possible or at least acknowledged when it is being performed.</p>
<p>Regression models look like they
estimate the changes in <span class="math inline">\(y\)</span> that are caused by changes in <span class="math inline">\(x\)</span>, especially when you use <span class="math inline">\(x\)</span> to predict <span class="math inline">\(y\)</span>. This is not true
unless the levels of <span class="math inline">\(x\)</span> are randomly assigned and only then we can make causal
inferences. Since this is not generally true, you should initially always
assume that any regression equation describes the relationship – if you observe
two subjects that are 1 unit of <span class="math inline">\(x\)</span> apart, you can expect their mean to differ by
<span class="math inline">\(b_1\)</span> – you should not, however, say that changing <span class="math inline">\(x\)</span> causes a change in the
mean of the responses. Despite all these cautions,
regression models are very popular statistical methods. They provide detailed
descriptions of relationships between variables and can be extended to
situations where we are interested in multiple predictor variables. They also
share ties to the ANOVA models discussed previously. When you are running
R code, you will note that all the ANOVAs and the regression models are estimated
using <code>lm</code>.

The assumptions and diagnostic plots are quite similar. And in the
next chapter, we will see that inference techniques
look similar. People still like to distinguish among the different types of
situations, but the underlying <strong><em>linear models</em></strong> are actually exactly the
same…</p>
<!-- \newpage -->
</div>
<div id="section6-13" class="section level2">
<h2><span class="header-section-number">6.13</span> Summary of important R code</h2>
<p>The main components of the R code used in this chapter follow with the
components to modify in lighter and/or ALL CAPS text where <code>y</code> is a response variable,
<code>x</code> is an explanatory variable, and the data are in <code>DATASETNAME</code>.</p>
<ul>
<li><p><strong>pairs.panels(<font color='red'>DATASETNAME</font>, ellipses=F, scale=T,
smooth=F, col=0)</strong></p>
<ul>
<li><p>Requires the <code>psych</code> package.</p></li>
<li><p>Makes a scatterplot matrix that also displays the correlation
coefficient. </p></li>
</ul></li>
<li><p><strong>cor(<font color='red'>y</font>~<font color='red'>x</font>,
data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li>Provides the estimated correlation coefficient between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.
</li>
</ul></li>
<li><p><strong>plot(<font color='red'>y</font>~<font color='red'>x</font>,
data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li>Provides a scatter plot.
</li>
</ul></li>
<li><p><strong>scatterplot(<font color='red'>y</font>~<font color='red'>x</font>,
data=<font color='red'>DATASETNAME</font>, smooth=F)</strong></p>
<ul>
<li><p>Requires the <code>car</code> package.</p></li>
<li><p>Provides a scatter plot with a regression line.
</p></li>
</ul></li>
<li><p><strong><font color='red'>MODELNAME</font> <code>&lt;-</code> lm(<font color='red'>y</font>~<font color='red'>x</font>,
data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li>Estimates a regression model using least squares.
</li>
</ul></li>
<li><p><strong>summary(<font color='red'>MODELNAME</font>)</strong></p>
<ul>
<li>Provides parameter estimates and R-squared (used heavily in
Chapter <a href="chapter7.html#chapter7">7</a> and <a href="chapter8.html#chapter8">8</a> as well).
</li>
</ul></li>
<li><p><strong>par(mfrow=c(2, 2)); plot(<font color='red'>MODELNAME</font>)</strong></p>
<ul>
<li>Provides four regression diagnostic plots in one plot.</li>
</ul></li>
</ul>
<div style="page-break-after: always;"></div>
</div>
<div id="section6-14" class="section level2">
<h2><span class="header-section-number">6.14</span> Practice problems</h2>
<p>6.1. <strong>Treadmill data analysis</strong> These questions revisit the treadmill data set from Chapter <a href="chapter1.html#chapter1">1</a>.
Researchers were
interested in whether the run test variable could be used to replace the treadmill
oxygen consumption variable that is expensive to measure. The following code loads
the data set and provides a scatterplot matrix using <code>pairs.panel</code>.</p>
<div class="sourceCode" id="cb587"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb587-1"><a href="chapter6.html#cb587-1"></a>treadmill &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/treadmill.csv&quot;</span>)</span>
<span id="cb587-2"><a href="chapter6.html#cb587-2"></a><span class="kw">library</span>(psych)</span>
<span id="cb587-3"><a href="chapter6.html#cb587-3"></a><span class="kw">pairs.panels</span>(treadmill, <span class="dt">ellipses=</span>F, <span class="dt">smooth=</span>F, <span class="dt">col=</span><span class="dv">0</span>)</span></code></pre></div>
<p>6.1.1. First,
we should get a sense of the strength of the correlation between the variable
of primary interest, <code>TreadMillOx</code>, and the other variables and consider
whether outliers or nonlinearity are going to be
major issues here. Which variable is it most strongly correlated with? Which
variables are next most strongly correlated with this variable?</p>
<p>6.1.2. Fit
the SLR using <code>RunTime</code> as explanatory variable for <code>TreadMillOx</code>.
Report the estimated model.</p>
<p>6.1.3. Predict the treadmill oxygen value for a subject with a run time
of 14 minutes. Repeat for a subject with a run time of 16 minutes. Is there
something different about these two predictions?</p>
<p>6.1.4. Interpret the slope coefficient from the estimated model, remembering
the units on the variables.</p>
<p>6.1.5. Report and interpret the <span class="math inline">\(y\)</span>-intercept from the SLR.</p>
<p>6.1.6. Report and interpret the <span class="math inline">\(R^2\)</span> value from the output. Show how you can find this
value from the original correlation matrix result.</p>
<p>6.1.7. Produce the diagnostic plots and discuss any potential issues. What
is the approximate leverage of the highest leverage observation and how large
is its Cook’s D? What does that tell you about its potential influence in this
model?</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-manipulate">
<p>Allaire, JJ. 2014. <em>Manipulate: Interactive Plots for Rstudio</em>. <a href="https://CRAN.R-project.org/package=manipulate">https://CRAN.R-project.org/package=manipulate</a>.</p>
</div>
<div id="ref-Azzalini1990">
<p>Azzalini, Adelchi, and Adrian W. Bowman. 1990. “A Look at Some Data on the Old Faithful Geyser.” <em>Applied Statistics</em> 39: 357–65.</p>
</div>
<div id="ref-Clevenger2005">
<p>Clevenger, Anthony P, and Nigel Waltho. 2005. “Performance Indices to Identify Attributes of Highway Crossing Structures Facilitating Movement of Large Mammals.” <em>Biological Conservation</em> 121 (3): 453–64.</p>
</div>
<div id="ref-Fox2011">
<p>Fox, John, and Sanford Weisberg. 2011. <em>An R-Companion to Applied Regression, Second Edition</em>. Thousand Oaks, CA: SAGE Publications. <a href="http://socserv.socsci.mcmaster.ca/jfox/Books/Companion">http://socserv.socsci.mcmaster.ca/jfox/Books/Companion</a>.</p>
</div>
<div id="ref-R-carData">
<p>Fox, John, Sanford Weisberg, and Brad Price. 2019b. <em>CarData: Companion to Applied Regression Data Sets</em>. <a href="https://CRAN.R-project.org/package=carData">https://CRAN.R-project.org/package=carData</a>.</p>
</div>
<div id="ref-Gude2009">
<p>Gude, Patricia H., J. Anthony Cookson, Mark C. Greenwood, and Mark Haggerty. 2009. “Homes in Wildfire-Prone Areas: An Empirical Analysis of Wildfire Suppression Costs and Climate Change.” <a href="www.headwaterseconomics.org">www.headwaterseconomics.org</a>.</p>
</div>
<div id="ref-R-spuRs">
<p>Jones, Owen, Robert Maillardet, Andrew Robinson, Olga Borovkova, and Steven Carnie. 2018. <em>SpuRs: Functions and Datasets for "Introduction to Scientific Programming and Simulation Using R"</em>. <a href="https://CRAN.R-project.org/package=spuRs">https://CRAN.R-project.org/package=spuRs</a>.</p>
</div>
<div id="ref-R-psych">
<p>Revelle, William. 2019. <em>Psych: Procedures for Psychological, Psychometric, and Personality Research</em>. <a href="https://CRAN.R-project.org/package=psych">https://CRAN.R-project.org/package=psych</a>.</p>
</div>
<div id="ref-R-MASS">
<p>Ripley, Brian. 2019. <em>MASS: Support Functions and Datasets for Venables and Ripley’s Mass</em>. <a href="https://CRAN.R-project.org/package=MASS">https://CRAN.R-project.org/package=MASS</a>.</p>
</div>
<div id="ref-R-tigerstats">
<p>Robinson, Rebekah, and Homer White. 2016. <em>Tigerstats: R Functions for Elementary Statistics</em>. <a href="https://CRAN.R-project.org/package=tigerstats">https://CRAN.R-project.org/package=tigerstats</a>.</p>
</div>
<div id="ref-R-corrplot">
<p>Wei, Taiyun, and Viliam Simko. 2017. <em>Corrplot: Visualization of a Correlation Matrix</em>. <a href="https://CRAN.R-project.org/package=corrplot">https://CRAN.R-project.org/package=corrplot</a>.</p>
</div>
<div id="ref-Weisberg2005">
<p>Weisberg, Sanford. 2005. <em>Applied Linear Regression, Third Edition</em>. Hoboken, NJ: Wiley.</p>
</div>
<div id="ref-R-alr3">
<p>Weisberg, Sanford. 2018. <em>Alr3: Data to Accompany Applied Linear Regression 3rd Edition</em>. <a href="https://CRAN.R-project.org/package=alr3">https://CRAN.R-project.org/package=alr3</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="99">
<li id="fn99"><p>This is related to what is called Simpson’s paradox, where the overall analysis (ignoring a grouping variable) leads to a conclusion of a relationship in one direction, but when the relationship is broken down into subgroups it is in the opposite direction in each group.  This emphasizes the importance of checking and accounting for differences in groups and the more complex models we are setting the stage to consider in the coming chapters.<a href="chapter6.html#fnref99" class="footnote-back">↩︎</a></p></li>
<li id="fn100"><p>The interval is “far” from the reference value under the null (0) so this provides at least strong evidence. With using confidence intervals for tests, we really don’t know much about the strength of evidence against the null hypothesis but the hypothesis test here is a bit more complicated to construct and understand and we will have to tolerate just having crude information about the p-value to assess strength of evidence.<a href="chapter6.html#fnref100" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter5.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter7.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Greenwood_Book.pdf"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
