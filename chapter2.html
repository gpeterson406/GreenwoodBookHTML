<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 (R)e-Introduction to statistics | Intermediate Statistics with R</title>
  <meta name="description" content="Chapter 2 (R)e-Introduction to statistics | Intermediate Statistics with R" />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 (R)e-Introduction to statistics | Intermediate Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="gpeterson406/Greenwood_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 (R)e-Introduction to statistics | Intermediate Statistics with R" />
  
  
  

<meta name="author" content="Mark C Greenwood" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter1.html"/>
<link rel="next" href="chapter3.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="font-awesome.min.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intermediate Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#section1-1"><i class="fa fa-check"></i><b>1.1</b> Overview of methods</a></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#section1-2"><i class="fa fa-check"></i><b>1.2</b> Getting started in R</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#section1-3"><i class="fa fa-check"></i><b>1.3</b> Basic summary statistics, histograms, and boxplots using R</a></li>
<li class="chapter" data-level="1.4" data-path="chapter1.html"><a href="chapter1.html#section1-4"><i class="fa fa-check"></i><b>1.4</b> Chapter summary</a></li>
<li class="chapter" data-level="1.5" data-path="chapter1.html"><a href="chapter1.html#section1-5"><i class="fa fa-check"></i><b>1.5</b> Summary of important R code</a></li>
<li class="chapter" data-level="1.6" data-path="chapter1.html"><a href="chapter1.html#section1-6"><i class="fa fa-check"></i><b>1.6</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> (R)e-Introduction to statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#section2-1"><i class="fa fa-check"></i><b>2.1</b> Histograms, boxplots, and density curves</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#section2-2"><i class="fa fa-check"></i><b>2.2</b> Pirate-plots</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#section2-3"><i class="fa fa-check"></i><b>2.3</b> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#section2-4"><i class="fa fa-check"></i><b>2.4</b> Permutation testing for the two sample mean situation</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#section2-5"><i class="fa fa-check"></i><b>2.5</b> Hypothesis testing (general)</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#section2-6"><i class="fa fa-check"></i><b>2.6</b> Connecting randomization (nonparametric) and parametric tests</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#section2-7"><i class="fa fa-check"></i><b>2.7</b> Second example of permutation tests</a></li>
<li class="chapter" data-level="2.8" data-path="chapter2.html"><a href="chapter2.html#section2-8"><i class="fa fa-check"></i><b>2.8</b> Reproducibility Crisis: Moving beyond p &lt; 0.05, publication bias, and multiple testing issues</a></li>
<li class="chapter" data-level="2.9" data-path="chapter2.html"><a href="chapter2.html#section2-9"><i class="fa fa-check"></i><b>2.9</b> Confidence intervals and bootstrapping</a></li>
<li class="chapter" data-level="2.10" data-path="chapter2.html"><a href="chapter2.html#section2-10"><i class="fa fa-check"></i><b>2.10</b> Bootstrap confidence intervals for difference in GPAs</a></li>
<li class="chapter" data-level="2.11" data-path="chapter2.html"><a href="chapter2.html#section2-11"><i class="fa fa-check"></i><b>2.11</b> Chapter summary</a></li>
<li class="chapter" data-level="2.12" data-path="chapter2.html"><a href="chapter2.html#section2-12"><i class="fa fa-check"></i><b>2.12</b> Summary of important R code</a></li>
<li class="chapter" data-level="2.13" data-path="chapter2.html"><a href="chapter2.html#section2-13"><i class="fa fa-check"></i><b>2.13</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> One-Way ANOVA</a><ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#section3-1"><i class="fa fa-check"></i><b>3.1</b> Situation</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#section3-2"><i class="fa fa-check"></i><b>3.2</b> Linear model for One-Way ANOVA (cell means and reference-coding)</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#section3-3"><i class="fa fa-check"></i><b>3.3</b> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#section3-4"><i class="fa fa-check"></i><b>3.4</b> ANOVA model diagnostics including QQ-plots</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#section3-5"><i class="fa fa-check"></i><b>3.5</b> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#section3-6"><i class="fa fa-check"></i><b>3.6</b> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li class="chapter" data-level="3.7" data-path="chapter3.html"><a href="chapter3.html#section3-7"><i class="fa fa-check"></i><b>3.7</b> Pair-wise comparisons for the Overtake data</a></li>
<li class="chapter" data-level="3.8" data-path="chapter3.html"><a href="chapter3.html#section3-8"><i class="fa fa-check"></i><b>3.8</b> Chapter summary</a></li>
<li class="chapter" data-level="3.9" data-path="chapter3.html"><a href="chapter3.html#section3-9"><i class="fa fa-check"></i><b>3.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="3.10" data-path="chapter3.html"><a href="chapter3.html#section3-10"><i class="fa fa-check"></i><b>3.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Two-Way ANOVA</a><ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#section4-1"><i class="fa fa-check"></i><b>4.1</b> Situation</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#section4-2"><i class="fa fa-check"></i><b>4.2</b> Designing a two-way experiment and visualizing results</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#section4-3"><i class="fa fa-check"></i><b>4.3</b> Two-Way ANOVA models and hypothesis tests</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#section4-4"><i class="fa fa-check"></i><b>4.4</b> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#section4-5"><i class="fa fa-check"></i><b>4.5</b> Observational study example: The Psychology of Debt</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#section4-6"><i class="fa fa-check"></i><b>4.6</b> Pushing Two-Way ANOVA to the limit: Un-replicated designs and Estimability</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#section4-7"><i class="fa fa-check"></i><b>4.7</b> Chapter summary</a></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#section4-8"><i class="fa fa-check"></i><b>4.8</b> Summary of important R code</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#section4-9"><i class="fa fa-check"></i><b>4.9</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Chi-square tests</a><ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#section5-1"><i class="fa fa-check"></i><b>5.1</b> Situation, contingency tables, and tableplots</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#section5-2"><i class="fa fa-check"></i><b>5.2</b> Homogeneity test hypotheses</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#section5-3"><i class="fa fa-check"></i><b>5.3</b> Independence test hypotheses</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#section5-4"><i class="fa fa-check"></i><b>5.4</b> Models for R by C tables</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#section5-5"><i class="fa fa-check"></i><b>5.5</b> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#section5-6"><i class="fa fa-check"></i><b>5.6</b> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#section5-7"><i class="fa fa-check"></i><b>5.7</b> Examining residuals for the source of differences</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#section5-8"><i class="fa fa-check"></i><b>5.8</b> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#section5-9"><i class="fa fa-check"></i><b>5.9</b> Political party and voting results: Complete analysis</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#section5-10"><i class="fa fa-check"></i><b>5.10</b> Is cheating and lying related in students?</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#section5-11"><i class="fa fa-check"></i><b>5.11</b> Analyzing a stratified random sample of California schools</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#section5-12"><i class="fa fa-check"></i><b>5.12</b> Chapter summary</a></li>
<li class="chapter" data-level="5.13" data-path="chapter5.html"><a href="chapter5.html#section5-13"><i class="fa fa-check"></i><b>5.13</b> Summary of important R commands</a></li>
<li class="chapter" data-level="5.14" data-path="chapter5.html"><a href="chapter5.html#section5-14"><i class="fa fa-check"></i><b>5.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Correlation and Simple Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#section6-1"><i class="fa fa-check"></i><b>6.1</b> Relationships between two quantitative variables</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#section6-2"><i class="fa fa-check"></i><b>6.2</b> Estimating the correlation coefficient</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#section6-3"><i class="fa fa-check"></i><b>6.3</b> Relationships between variables by groups</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#section6-4"><i class="fa fa-check"></i><b>6.4</b> Inference for the correlation coefficient</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#section6-5"><i class="fa fa-check"></i><b>6.5</b> Are tree diameters related to tree heights?</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#section6-6"><i class="fa fa-check"></i><b>6.6</b> Describing relationships with a regression model</a></li>
<li class="chapter" data-level="6.7" data-path="chapter6.html"><a href="chapter6.html#section6-7"><i class="fa fa-check"></i><b>6.7</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.8" data-path="chapter6.html"><a href="chapter6.html#section6-8"><i class="fa fa-check"></i><b>6.8</b> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li class="chapter" data-level="6.9" data-path="chapter6.html"><a href="chapter6.html#section6-9"><i class="fa fa-check"></i><b>6.9</b> Outliers: leverage and influence</a></li>
<li class="chapter" data-level="6.10" data-path="chapter6.html"><a href="chapter6.html#section6-10"><i class="fa fa-check"></i><b>6.10</b> Residual diagnostics – setting the stage for inference</a></li>
<li class="chapter" data-level="6.11" data-path="chapter6.html"><a href="chapter6.html#section6-11"><i class="fa fa-check"></i><b>6.11</b> Old Faithful discharge and waiting times</a></li>
<li class="chapter" data-level="6.12" data-path="chapter6.html"><a href="chapter6.html#section6-12"><i class="fa fa-check"></i><b>6.12</b> Chapter summary</a></li>
<li class="chapter" data-level="6.13" data-path="chapter6.html"><a href="chapter6.html#section6-13"><i class="fa fa-check"></i><b>6.13</b> Summary of important R code</a></li>
<li class="chapter" data-level="6.14" data-path="chapter6.html"><a href="chapter6.html#section6-14"><i class="fa fa-check"></i><b>6.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Simple linear regression inference</a><ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#section7-1"><i class="fa fa-check"></i><b>7.1</b> Model</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#section7-2"><i class="fa fa-check"></i><b>7.2</b> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#section7-3"><i class="fa fa-check"></i><b>7.3</b> Bozeman temperature trend</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#section7-4"><i class="fa fa-check"></i><b>7.4</b> Randomization-based inferences for the slope coefficient</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#section7-5"><i class="fa fa-check"></i><b>7.5</b> Transformations part I: Linearizing relationships</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#section7-6"><i class="fa fa-check"></i><b>7.6</b> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#section7-7"><i class="fa fa-check"></i><b>7.7</b> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li class="chapter" data-level="7.8" data-path="chapter7.html"><a href="chapter7.html#section7-8"><i class="fa fa-check"></i><b>7.8</b> Chapter summary</a></li>
<li class="chapter" data-level="7.9" data-path="chapter7.html"><a href="chapter7.html#section7-9"><i class="fa fa-check"></i><b>7.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="7.10" data-path="chapter7.html"><a href="chapter7.html#section7-10"><i class="fa fa-check"></i><b>7.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#section8-1"><i class="fa fa-check"></i><b>8.1</b> Going from SLR to MLR</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#section8-2"><i class="fa fa-check"></i><b>8.2</b> Validity conditions in MLR</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#section8-3"><i class="fa fa-check"></i><b>8.3</b> Interpretation of MLR terms</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#section8-4"><i class="fa fa-check"></i><b>8.4</b> Comparing multiple regression models</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#section8-5"><i class="fa fa-check"></i><b>8.5</b> General recommendations for MLR interpretations and VIFs</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#section8-6"><i class="fa fa-check"></i><b>8.6</b> MLR inference: Parameter inferences using the t-distribution</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#section8-7"><i class="fa fa-check"></i><b>8.7</b> Overall F-test in multiple linear regression</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#section8-8"><i class="fa fa-check"></i><b>8.8</b> Case study: First year college GPA and SATs</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#section8-9"><i class="fa fa-check"></i><b>8.9</b> Different intercepts for different groups: MLR with indicator variables</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#section8-10"><i class="fa fa-check"></i><b>8.10</b> Additive MLR with more than two groups: Headache example</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#section8-11"><i class="fa fa-check"></i><b>8.11</b> Different slopes and different intercepts</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#section8-12"><i class="fa fa-check"></i><b>8.12</b> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#section8-13"><i class="fa fa-check"></i><b>8.13</b> AICs for model selection</a></li>
<li class="chapter" data-level="8.14" data-path="chapter8.html"><a href="chapter8.html#section8-14"><i class="fa fa-check"></i><b>8.14</b> Case study: Forced expiratory volume model selection using AICs</a></li>
<li class="chapter" data-level="8.15" data-path="chapter8.html"><a href="chapter8.html#section8-15"><i class="fa fa-check"></i><b>8.15</b> Chapter summary</a></li>
<li class="chapter" data-level="8.16" data-path="chapter8.html"><a href="chapter8.html#section8-16"><i class="fa fa-check"></i><b>8.16</b> Summary of important R code</a></li>
<li class="chapter" data-level="8.17" data-path="chapter8.html"><a href="chapter8.html#section8-17"><i class="fa fa-check"></i><b>8.17</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Case studies</a><ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#section9-1"><i class="fa fa-check"></i><b>9.1</b> Overview of material covered</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#section9-2"><i class="fa fa-check"></i><b>9.2</b> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#section9-3"><i class="fa fa-check"></i><b>9.3</b> Ants learn to rely on more informative attributes during decision-making</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#section9-4"><i class="fa fa-check"></i><b>9.4</b> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#section9-5"><i class="fa fa-check"></i><b>9.5</b> What do didgeridoos really do about sleepiness?</a></li>
<li class="chapter" data-level="9.6" data-path="chapter9.html"><a href="chapter9.html#section9-6"><i class="fa fa-check"></i><b>9.6</b> General summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Intermediate Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter2" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> (R)e-Introduction to statistics</h1>
<p>The previous material served to get us started in R and to get a quick review of same basic graphical and
descriptive statistics. Now we will begin to engage some new material and
exploit the power of R to do statistical inference. Because inference is
one of the hardest topics to master in statistics, we will also review some
basic terminology that is required to move forward in learning more
sophisticated statistical methods. To keep this “review” as short as possible,
we will not consider every situation you learned in introductory statistics and
instead focus exclusively on the situation where we have a quantitative
response variable measured on two groups, adding a new graphic called a “pirate-plot”  to help us see the differences in the observations in the groups.</p>
<div id="section2-1" class="section level2">
<h2><span class="header-section-number">2.1</span> Histograms, boxplots, and density curves</h2>
<p>Part of learning statistics is learning to correctly use the terminology, some of which is used colloquially
differently than it is used in formal statistical settings. The most commonly
“misused” statistical term is <strong><em>data</em></strong>.   In statistical parlance, we want to note the plurality of
data. Specifically, <strong><em>datum</em></strong> is a single measurement, possibly on multiple random
variables, and so it is appropriate to say that “<strong>a datum is…</strong>”.
Once we move to discussing data, we are now referring to more than one
observation, again on one, or possibly more than one, random variable, and
so we need to use “<strong>data are…</strong>” when talking about our observations. We want
to distinguish our use of the term “data” from its more
colloquial<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> usage that often involves treating it as singular.
In a statistical setting
“data” refers to measurements of our cases or units. When we summarize the
results of a study (say providing the mean and SD), that information is not
“data”. We used our data to generate that information. Sometimes we also use
the term “data set” to refer to all our observations and this is a singular
term to refer to the group of observations and this makes it really easy to
make mistakes on the usage of “data”<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a>.</p>
<p>It is also really important to note that <strong><em>variables</em></strong> have to vary –
if you measure the level of education of your subjects but all are high school graduates, then you do
not have a “variable”. You may not know if you have real variability in a
“variable” until you explore the results you obtained.</p>
<p>The last, but probably most important, aspect of data is the context
of the measurement. The “who, what, when, and where” of the collection
of the observations is critical to the
sort of conclusions we can make based on the results. The information on the
study design provides information required to assess the scope of inference of
the study.  Generally, remember to think about the research questions the
researchers were trying to answer and whether their study actually would answer
those questions. There are no formulas to help us sort some of these things
out, just critical thinking about the context of the measurements.</p>
<p>To make this concrete, consider the data collected from a study
<span class="citation">(Walker, Garrard, and Jowitt <a href="#ref-Walker2014" role="doc-biblioref">2014</a>)</span> to investigate whether clothing worn by a bicyclist might impact the passing distance of cars. One of the authors wore seven different outfits (outfit for the day was chosen randomly by shuffling seven playing cards) on his regular 26 km commute near London in the United Kingdom. Using a specially instrumented bicycle, they measured how close the vehicles passed to the widest point on the handlebars. The seven outfits (“conditions”) that you can view at <a href="https://www.sciencedirect.com/science/article/pii/S0001457513004636" class="uri">https://www.sciencedirect.com/science/article/pii/S0001457513004636</a> were:</p>
<ul>
<li><p>COMMUTE: Plain cycling jersey and pants, reflective cycle clips, commuting helmet, and bike gloves.</p></li>
<li><p>CASUAL: Rugby shirt with pants tucked into socks, wool hat or baseball cap, plain gloves, and small backpack.</p></li>
<li><p>HIVIZ: Bright yellow reflective cycle commuting jacket, plain pants, reflective cycle clips, commuting helmet, and bike gloves.</p></li>
<li><p>RACER: Colorful, skin-tight, Tour de France cycle jersey with sponsor logos, Lycra bike shorts or tights, race helmet, and bike gloves.</p></li>
<li><p>NOVICE: Yellow reflective vest with “Novice Cyclist, Pass Slowly” and plain pants, reflective cycle clips, commuting helmet, and bike gloves.</p></li>
<li><p>POLICE: Yellow reflective vest with “POLICEwitness.com – Move Over – Camera Cyclist” and plain pants, reflective cycle clips, commuting helmet, and bike gloves.</p></li>
<li><p>POLITE: Yellow reflective vest with blue and white checked banding and the words “POLITE notice, Pass Slowly” looking similar to a police jacket and plain pants, reflective cycle clips, commuting helmet, and bike gloves.</p></li>
</ul>
<p>They collected data (distance to the vehicle in cm for each car “overtake”) on between 8 and 11 rides in each outfit and between 737 and 868 “overtakings” across these rides. The outfit is a categorical <em>predictor</em> or <em>explanatory</em> variable)  that has seven different levels here. The distance is the <em>response</em> variable  and is a quantitative variable here<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a>. Note that we do not have the information on which overtake came from which ride in the data provided or the conditions related to individual overtake observations other than the distance to the vehicle (they only included overtakings that had consistent conditions for the road and riding).</p>
<p>The data are posted on my website<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a> at <a href="http://www.math.montana.edu/courses/s217/documents/Walker2014_mod.csv" class="uri">http://www.math.montana.edu/courses/s217/documents/Walker2014_mod.csv</a> if you want to download the file to a local directory and then import the data into R using “Import Dataset”. Or you can use the code in the following codechunk to directly read the data set into R using the URL.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="chapter2.html#cb24-1"></a><span class="kw">suppressMessages</span>(<span class="kw">library</span>(readr))</span>
<span id="cb24-2"><a href="chapter2.html#cb24-2"></a>dd &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/Walker2014_mod.csv&quot;</span>)</span></code></pre></div>
<p>It is always good to review the data you have read by running the code and printing the tibble  by typing the tibble name (here <code>&gt; dd</code>) at the command prompt in the console, using the <code>View</code> function, (here <code>View(dd)</code>), to open a spreadsheet-like view, or using the <code>head</code> and <code>tail</code> functions have been show the first and last ten observations:</p>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="chapter2.html#cb25-1"></a><span class="kw">head</span>(dd)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 8
##   Condition Distance Shirt Helmet Pants Gloves ReflectClips Backpack
##   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;   
## 1 casual         132 Rugby hat    plain plain  no           yes     
## 2 casual         137 Rugby hat    plain plain  no           yes     
## 3 casual         174 Rugby hat    plain plain  no           yes     
## 4 casual          82 Rugby hat    plain plain  no           yes     
## 5 casual         106 Rugby hat    plain plain  no           yes     
## 6 casual          48 Rugby hat    plain plain  no           yes</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="chapter2.html#cb27-1"></a><span class="kw">tail</span>(dd)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 8
##   Condition Distance Shirt      Helmet Pants Gloves ReflectClips Backpack
##   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;   
## 1 racer          122 TourJersey race   lycra bike   yes          no      
## 2 racer          204 TourJersey race   lycra bike   yes          no      
## 3 racer          116 TourJersey race   lycra bike   yes          no      
## 4 racer          132 TourJersey race   lycra bike   yes          no      
## 5 racer          224 TourJersey race   lycra bike   yes          no      
## 6 racer           72 TourJersey race   lycra bike   yes          no</code></pre>
<p>Another option is to directly access specific rows and/or columns of the tibble, especially for larger data sets. In objects containing data, we can select certain rows and columns using the <strong><em>brackets</em></strong>, <code>[..., ...]</code>, to specify the row (first element) and column (second element). For example, we can extract the datum in the fourth row and second column using <code>dd[4,2]</code>:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="chapter2.html#cb29-1"></a>dd[<span class="dv">4</span>,<span class="dv">2</span>]</span></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   Distance
##      &lt;dbl&gt;
## 1       82</code></pre>
<p>This provides the distance (in cm) of a pass at 82 cm. To get all of either the rows or columns, a space is used instead of specifying a particular number. For example, the information in all the columns on the fourth observation can be obtained using <code>dd[4, ]</code>:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="chapter2.html#cb31-1"></a>dd[<span class="dv">4</span>,]</span></code></pre></div>
<pre><code>## # A tibble: 1 x 8
##   Condition Distance Shirt Helmet Pants Gloves ReflectClips Backpack
##   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;   
## 1 casual          82 Rugby hat    plain plain  no           yes</code></pre>
<p>So this was an observation from the <code>casual</code> condition that had a passing distance of 82 cm. The other columns describe some other specific aspects of the condition. To get a more complete sense of the data set, we can extract a suite of observations from each condition using their row numbers concatenated, <code>c()</code>, together, extracting all columns for two observations from each of the conditions based on their rows.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="chapter2.html#cb33-1"></a>dd[<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">780</span>, <span class="dv">781</span>, <span class="dv">1637</span>, <span class="dv">1638</span>, <span class="dv">2374</span>, <span class="dv">2375</span>, <span class="dv">3181</span>, <span class="dv">3182</span>, <span class="dv">3971</span>, <span class="dv">3972</span>, <span class="dv">4839</span>, <span class="dv">4840</span>),]</span></code></pre></div>
<pre><code>## # A tibble: 14 x 8
##    Condition Distance Shirt       Helmet   Pants Gloves ReflectClips Backpack
##    &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;   
##  1 casual         132 Rugby       hat      plain plain  no           yes     
##  2 casual         137 Rugby       hat      plain plain  no           yes     
##  3 commute         70 PlainJersey commuter plain bike   yes          no      
##  4 commute        151 PlainJersey commuter plain bike   yes          no      
##  5 hiviz           94 Jacket      commuter plain bike   yes          no      
##  6 hiviz          145 Jacket      commuter plain bike   yes          no      
##  7 novice          12 Vest_Novice commuter plain bike   yes          no      
##  8 novice         122 Vest_Novice commuter plain bike   yes          no      
##  9 police         113 Vest_Police commuter plain bike   yes          no      
## 10 police         174 Vest_Police commuter plain bike   yes          no      
## 11 polite         156 Vest_Polite commuter plain bike   yes          no      
## 12 polite          14 Vest_Polite commuter plain bike   yes          no      
## 13 racer          104 TourJersey  race     lycra bike   yes          no      
## 14 racer          141 TourJersey  race     lycra bike   yes          no</code></pre>
<p>Now we can see the <code>Condition</code> variable seems to have seven different levels, the <code>Distance</code> variable contains the overtake distance, and then a suite of columns that describe aspects of each outfit, such as the type of shirt or whether reflective cycling clips were used or not. We will only use the “Distance” and “Condition” variables to start with.</p>
<p>When working with data, we should always start with
summarizing the sample size. We will use <strong><em>n</em></strong>  for the
number of subjects in the sample and denote the population size (if
available) with <strong><em>N</em></strong>.  Here, the sample size is <strong><em>n=5690</em></strong>. In
this situation, we do not have a random sample from a population 
(these were all of the overtakes that met the criteria during the rides) so we cannot make inferences from our sample to a larger group (other rides or for other situations like different places, times, or riders).
But we can assess whether there is a  <strong><em>causal effect</em></strong><a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a>: if sufficient evidence is found to conclude that there is some difference in
the responses across the conditions, we can attribute those differences to
the treatments applied, since the overtake events should be same otherwise due to the
outfit being randomly assigned to the rides. The story of the data set –
that it was collected on a particular route for a particular rider in the UK – becomes pretty important in thinking about
the ramifications of any results. Are drivers and roads in Montana or South Dakota different from drivers and roads near London? Are the road and traffic conditions likely to be different? If so,
then we should not assume that the detected differences, if detected, would
also exist in some other location for a different rider. The lack of a random sample  here from all the overtakes in the area (or more generally all that happen around the world) makes it impossible to assume that this set of overtakes might be like others. So there are definite limitations to the inferences in the following
results. But it is still interesting to see if the outfits worn caused a difference
in the mean overtake distances, even though the inferences are limited to the conditions in this individual’s commute. If this had been an observational study (suppose that the
researcher could select their outfit), then we would have to avoid
any of the “causal” language that we can consider here because the outfits
were not randomly assigned to the rides. Without random assignment,  the
explanatory variable of outfit choice could be <strong><em>confounded</em></strong> 
with another characteristic of rides that might be related to the passing distances, such as wearing a particular outfit because of an expectation of heavy traffic or poor light conditions. Confounding is not the only reason to avoid causal
statements with non-random assignment but the inability to separate the effect
of other variables (measured or unmeasured) from the differences we are
observing means that our inferences in these situations need to be carefully
stated to avoid implying causal effects.</p>
<p>In order to get some summary statistics, we will rely on the R package called
<code>mosaic</code> <span class="citation">(Pruim, Kaplan, and Horton <a href="#ref-R-mosaic" role="doc-biblioref">2019</a>)</span> as introduced previously. First (but only once),
you need to install the package, which can
be done either using the Packages tab in the lower right panel of RStudio or
using the <code>install.packages</code> function with quotes around the package name:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="chapter2.html#cb35-1"></a><span class="op">&gt;</span><span class="st"> </span><span class="kw">install.packages</span>(<span class="st">&quot;mosaic&quot;</span>)</span></code></pre></div>
<p>If you open a .Rmd file that contains code that incorporates packages and they are not installed, the bar at the top of the markdown document will prompt you to install those missing packages. This is the easiest way to get packages you might need installed. After making sure that any required packages are installed, use the <code>library</code>
function around the package name (no quotes now!) to load the package, something that
you need to do any time you want to use features of a package.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="chapter2.html#cb36-1"></a><span class="kw">library</span>(mosaic)</span></code></pre></div>
<p>When you are loading a package, R might mention a need to install other packages. If the output says that it needs a package that is
unavailable, then follow the same process noted above to install that package
and then repeat trying to load the package you wanted. These are called package “dependencies” and are due to one package developer relying on functions that already exist in another package.</p>
<p>With tibbles, you have to declare categorical variables as “factors” to have R correctly handle the variables using the <code>factor</code> function. This can be a bit time repetitive but provides some utility for data wrangling in more complex situations to read in the data and then declare their type. For quantitative variables, this is not required and they are stored as numeric variables. The following code declares the categorical variables in the data set as factors and saves them back into the variables of the same names:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="chapter2.html#cb37-1"></a>dd<span class="op">$</span>Condition &lt;-<span class="st"> </span><span class="kw">factor</span>(dd<span class="op">$</span>Condition)</span>
<span id="cb37-2"><a href="chapter2.html#cb37-2"></a>dd<span class="op">$</span>Shirt &lt;-<span class="st"> </span><span class="kw">factor</span>(dd<span class="op">$</span>Shirt) </span>
<span id="cb37-3"><a href="chapter2.html#cb37-3"></a>dd<span class="op">$</span>Helmet &lt;-<span class="st"> </span><span class="kw">factor</span>(dd<span class="op">$</span>Helmet)</span>
<span id="cb37-4"><a href="chapter2.html#cb37-4"></a>dd<span class="op">$</span>Pants &lt;-<span class="st"> </span><span class="kw">factor</span>(dd<span class="op">$</span>Pants)</span>
<span id="cb37-5"><a href="chapter2.html#cb37-5"></a>dd<span class="op">$</span>Gloves &lt;-<span class="st"> </span><span class="kw">factor</span>(dd<span class="op">$</span>Gloves)</span>
<span id="cb37-6"><a href="chapter2.html#cb37-6"></a>dd<span class="op">$</span>ReflectClips &lt;-<span class="st"> </span><span class="kw">factor</span>(dd<span class="op">$</span>ReflectClips)</span>
<span id="cb37-7"><a href="chapter2.html#cb37-7"></a>dd<span class="op">$</span>Backpack &lt;-<span class="st"> </span><span class="kw">factor</span>(dd<span class="op">$</span>Backpack) </span></code></pre></div>
<p>With many variables in a data set, it is often useful to get some
quick information about all of them; the <code>summary</code> function provides
useful information whether the variables are categorical or
quantitative and notes if any values were missing. </p>

<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="chapter2.html#cb38-1"></a><span class="kw">summary</span>(dd)</span></code></pre></div>
<pre><code>##    Condition      Distance             Shirt          Helmet       Pants        Gloves     ReflectClips Backpack  
##  casual :779   Min.   :  2.0   Jacket     :737   commuter:4059   lycra: 852   bike :4911   no : 779     no :4911  
##  commute:857   1st Qu.: 99.0   PlainJersey:857   hat     : 779   plain:4838   plain: 779   yes:4911     yes: 779  
##  hiviz  :737   Median :117.0   Rugby      :779   race    : 852                                                    
##  novice :807   Mean   :117.1   TourJersey :852                                                                    
##  police :790   3rd Qu.:134.0   Vest_Novice:807                                                                    
##  polite :868   Max.   :274.0   Vest_Police:790                                                                    
##  racer  :852                   Vest_Polite:868</code></pre>

<p>The output is organized by variable,
providing summary information based on the type of
variable, either counts by category for categorical variables or the 5-number summary plus the mean for the quantitative
variable <code>Distance</code>. If present, you would also get a count of missing values that are
called “NAs” in R.  For the first variable, called <code>Condition</code> and that we might more explicitly name <em>Outfit</em>, we find counts of the
number of overtakes for each outfit: <span class="math inline">\(779\)</span> out of <span class="math inline">\(5,690\)</span> were when wearing the casual outfit, <span class="math inline">\(857\)</span> for “commute”, and the other observations from the other five outfits, with the most observations when wearing the “polite” vest.
We can also see that overtake distances (variable
<code>Distance</code>) ranged from 2 cm to 274 cm with a median of 117 cm.</p>
<p>To accompany the numerical summaries, histograms and boxplots can
provide some initial information on the shape of the distribution of
the responses for the different <em>Outfits</em>. Figure <a href="chapter2.html#fig:Figure2-1">2.1</a>
contains the histogram
and boxplot of <em>Distance</em>, ignoring any information on which outfit was being worn. The calls to the two plotting functions are
enhanced slightly to add better labels using <code>xlab</code>, <code>ylab</code>, and <code>main</code>.</p>

<div class="figure"><span id="fig:Figure2-1"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-1-1.png" alt="Histogram and boxplot of passing distances in cm." width="576" />
<p class="caption">
Figure 2.1: Histogram and boxplot of passing distances in cm.
</p>
</div>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="chapter2.html#cb40-1"></a><span class="kw">hist</span>(dd<span class="op">$</span>Distance, <span class="dt">xlab=</span><span class="st">&quot;Distance (cm)&quot;</span>, <span class="dt">labels=</span>T, <span class="dt">main=</span><span class="st">&quot;Histogram of Distances&quot;</span>)</span>
<span id="cb40-2"><a href="chapter2.html#cb40-2"></a><span class="kw">boxplot</span>(dd<span class="op">$</span>Distance, <span class="dt">ylab=</span><span class="st">&quot;Distance (cm)&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Boxplot of Distances&quot;</span>)</span></code></pre></div>
<p>The distribution appears to be relatively symmetric with many observations in both tails flagged as potential
outliers. Despite being flagged as potential outliers, they seem to be part of a common distribution. In real data sets, outliers are commonly encountered and the
first step is to verify that they were not errors in recording (if so, fixing or removing them is easily justified). If they cannot be easily dismissed or fixed, the next step
is to study their impact on the statistical analyses performed, potentially
considering reporting results with and without the influential observation(s)
in the results (if there are just handful). If the analysis is unaffected by the “unusual” observations,
then it matters little whether they are dropped or not. If they do affect the
results, then reporting both versions of results allows the reader to judge the
impacts for themselves. It is important to remember that sometimes the outliers
are the most interesting part of the data set.  For example, those observations that were the closest would be of great interest, whether they are outliers or not.</p>
<p>Often when statisticians think of distributions of data, we think
of the smooth underlying
shape that led to the data set that is being displayed in the histogram.
Instead of binning up observations and making bars in the histogram, we can
estimate what is called a <strong><em>density curve</em></strong> as a smooth curve
that represents the observed distribution of the responses. Density curves can
sometimes help us see features of the data sets more clearly. </p>
<p>To understand the density curve, it is useful to initially see
the histogram and density curve together. The height of the density curve is scaled
so that the total area under the curve<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a> is 1. To make a comparable histogram, the
y-axis needs to be scaled so that the histogram is also on the “density”
scale which makes the bar heights adjust so that the proportion of the
total data set in each bar is represented by the area in each bar
(remember that area is height times width). So the height depends on the
width of the bars and the total area across all the bars has to be 1. In the
<code>hist</code> function, the <code>freq=F</code> option does this required re-scaling to get
density-scaled histogram bars. The
density curve is added to the histogram using the R code of
<code>lines(density())</code>, producing the result in Figure <a href="chapter2.html#fig:Figure2-2">2.2</a> with
added modifications of options for <code>lwd</code> (line width) and <code>col</code> (color)
to make the plot more visually appealing. You can see how the density curve
somewhat matches the histogram bars but deals with the bumps up and down
and edges a little differently. We can pick out the relatively symmetric distribution using
either display and will rarely make both together.</p>

<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="chapter2.html#cb41-1"></a><span class="kw">hist</span>(dd<span class="op">$</span>Distance, <span class="dt">freq=</span>F, <span class="dt">xlab=</span><span class="st">&quot;Distance (cm)&quot;</span>, <span class="dt">labels=</span>T, <span class="dt">main=</span><span class="st">&quot;Histogram of Distances&quot;</span>)</span>
<span id="cb41-2"><a href="chapter2.html#cb41-2"></a><span class="kw">lines</span>(<span class="kw">density</span>(dd<span class="op">$</span>Distance), <span class="dt">lwd=</span><span class="dv">3</span>,<span class="dt">col=</span><span class="st">&quot;purple&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure2-2"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-2-1.png" alt="Histogram and density curve of Distance responses." width="480" />
<p class="caption">
Figure 2.2: Histogram and density curve of Distance responses.
</p>
</div>
<p>Histograms can be sensitive to the choice of the number of bars and
even the cut-offs used to define the bins for a given number of bars.
Small changes in the definition of cut-offs for the bins can have
noticeable impacts on the shapes observed but
this does not impact density curves. We are not going to tinker with the
default choices for bars in histogram, as they are reasonably selected in R, but we
can add information on the original observations being included in each bar to
better understand the choices that <code>hist</code> is making. In the previous
display, we can add what is called a <strong><em>rug</em></strong> to the plot, where a tick
mark is made on the x-axis for each observation.  Because the responses appear to be rounded to the nearest cm, there is some discreteness in the responses and we need to use a graphical
technique called <strong><em>jittering</em></strong> to add a little noise<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a> to each observation so all the
observations at each distance value do not
plot as a single line.  In Figure <a href="chapter2.html#fig:Figure2-3">2.3</a>, the added tick marks
on the x-axis show the approximate locations of the original observations.
We can (barely) see how there are 2 observations at 2 cm (the noise
added generates a wider line than for an individual observation so it is possible to see that it is more than one observation there). A limitation of the
histogram arises at the center of the distribution where the bar that goes from 100 to 120 cm suggests that the mode (peak) is in this range (but it is unclear where) but the density curve suggests that the peak is closer to 120 than 100. The
density curve also shows some small bumps in the tails of the distributions tied to individual observations that are not really displayed in the histogram. Density curves are, however,
not perfect and this one shows a tiny bit of area for distances less than 0 cm which is
not possible here. When we make density curves below, we will cut off the curves at the most extreme values to avoid this issue.</p>

<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="chapter2.html#cb42-1"></a><span class="kw">hist</span>(dd<span class="op">$</span>Distance, <span class="dt">freq=</span>F, <span class="dt">xlab=</span><span class="st">&quot;Distance (cm)&quot;</span>, <span class="dt">labels=</span>T, </span>
<span id="cb42-2"><a href="chapter2.html#cb42-2"></a>     <span class="dt">main=</span><span class="st">&quot;Histogram of Distances with density curve and rug&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.017</span>))</span>
<span id="cb42-3"><a href="chapter2.html#cb42-3"></a><span class="kw">lines</span>(<span class="kw">density</span>(dd<span class="op">$</span>Distance), <span class="dt">lwd=</span><span class="dv">3</span>,<span class="dt">col=</span><span class="st">&quot;purple&quot;</span>)</span>
<span id="cb42-4"><a href="chapter2.html#cb42-4"></a><span class="kw">set.seed</span>(<span class="dv">900</span>)</span>
<span id="cb42-5"><a href="chapter2.html#cb42-5"></a><span class="kw">rug</span>(<span class="kw">jitter</span>(dd<span class="op">$</span>Distance), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure2-3"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-3-1.png" alt="Histogram with density curve and rug plot of the jittered distance responses." width="480" />
<p class="caption">
Figure 2.3: Histogram with density curve and rug plot of the jittered distance responses.
</p>
</div>
<p>The graphical tools we’ve just discussed are going to help us move to comparing the
distribution of responses across more than one group. We will have two displays
that will help us make these comparisons. The simplest is
the <strong><em>side-by-side boxplot</em></strong>, where a boxplot is displayed for each group
of interest using the same y-axis scaling.  In R, we can use its <strong><em>formula</em></strong>
notation to see if the response (<code>Distance</code>) differs based on the group
(<code>Condition</code>) by using something like <code>Y~X</code> or, here, <code>Distance~Condition</code>.
We also need to tell R where to find the variables – use the last option in the command, <code>data=DATASETNAME</code> , to inform R of the tibble to look in
to find the variables. In this example, <code>data=dd</code>. We will use
the formula and <code>data=...</code> options in almost every function we use
from here forward. Figure <a href="chapter2.html#fig:Figure2-4">2.4</a> contains the side-by-side
boxplots showing similar distributions for all the groups, with a slightly higher median in the “police” group and some outliers identified in all groups.</p>

<div class="figure"><span id="fig:Figure2-4"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-4-1.png" alt="Side-by-side boxplot of distances based on outfits." width="480" />
<p class="caption">
Figure 2.4: Side-by-side boxplot of distances based on outfits.
</p>
</div>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="chapter2.html#cb43-1"></a><span class="kw">boxplot</span>(Distance <span class="op">~</span><span class="st"> </span>Condition, <span class="dt">data=</span>dd)</span></code></pre></div>
<p>The “~” (which is read as the <em>tilde</em> symbol<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a>, which you can find in the
upper left corner of your keyboard) notation will be used in two ways in this
material.  The formula use in R employed previously declares that the
response variable here is <em>Distance</em> and the explanatory variable is <em>Condition</em>.
The other use for “~” is as shorthand for “is distributed as” and is used in
the context of <span class="math inline">\(Y \sim N(0,1)\)</span>, which translates (in statistics) to defining the
random variable <em>Y</em> as following a Normal distribution<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a>

with mean 0
and standard deviation of 1. In the current situation, we could ask whether
the <code>Distance</code> variable seems like it may follow a normal distribution in each group, in
other words, is <span class="math inline">\(\text{Distance}\sim N(\mu,\sigma^2)\)</span>? Since the responses are relatively symmetric, it is not clear that we have a violation of the assumption of the normality assumption for the <em>Distance</em> variable for any of the seven groups (more later on how we can assess this and the issues that occur when we have a violation of this assumption). Remember that
<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are parameters where
<span class="math inline">\(\mu\)</span> (“mu”) is our standard symbol for the <strong><em>population mean</em></strong>
and that <span class="math inline">\(\sigma\)</span> (“sigma”) is the symbol of the
<strong><em>population standard deviation</em></strong>.  </p>
</div>
<div id="section2-2" class="section level2">
<h2><span class="header-section-number">2.2</span> Pirate-plots</h2>
<p>An alternative graphical display for comparing multiple groups that we will use is a display called a <strong><em>pirate-plot</em></strong> <span class="citation">(Phillips <a href="#ref-Phillips2017" role="doc-biblioref">2017</a><a href="#ref-Phillips2017" role="doc-biblioref">a</a>)</span> from the <code>yarrr</code> package<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a>.  Figure <a href="chapter2.html#fig:Figure2-5">2.5</a>
shows an example of a pirate-plot that provides a side-by-side display that
contains the density curves, the original observations that generated the
density curve as jittered points (jittered both vertically and horizontally a little), the sample mean of each group (wide bar), and vertical lines to horizontal bars that represents the confidence interval for the true mean of that group. For each group, the density curves
are mirrored to aid in visual assessment of the shape of the distribution. This mirroring also
creates a shape that resembles the outline of a violin with skewed distributions so versions of this
display have also been called a “violin plot” or a “bean plot”. All together this plot shows us information
on the original observations, center (mean) and its confidence interval, spread, and shape of the distributions of the responses. Our inferences typically focus on the means of the groups and this plot allows
us to compare those across the groups while gaining information on the shapes
of the distributions of responses in each group.</p>
<p>To use the <code>pirateplot</code> function  we need to install and then load the <code>yarrr</code>
package <span class="citation">(Phillips <a href="#ref-R-yarrr" role="doc-biblioref">2017</a><a href="#ref-R-yarrr" role="doc-biblioref">b</a>)</span>.

The function works like the boxplot used previously
except that options
for the type of confidence interval needs to be specified with <code>inf.method="ci"</code> - otherwise you will get a different kind of interval than you learned in introductory statistics and we don’t want to get caught up in trying to understand the kind of interval it makes by default. And it seems useful to add <code>inf.disp="line"</code> as an additional option to add bars for the confidence interval<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a>. There are many other options in the function that might be useful in certain situations, but these are the only ones that are really needed to get started with pirate-plots.</p>

<div class="figure"><span id="fig:Figure2-5"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-5-1.png" alt="Pirate-plot of distances by outfit group. Bold horizontal lines correspond to sample mean of each group, boxes around lines (here they are very tight to the lines for the means) are the 95% confidence intervals." width="480" />
<p class="caption">
Figure 2.5: Pirate-plot of distances by outfit group. Bold horizontal lines correspond to sample mean of each group, boxes around lines (here they are very tight to the lines for the means) are the 95% confidence intervals.
</p>
</div>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="chapter2.html#cb44-1"></a><span class="kw">library</span>(yarrr)</span>
<span id="cb44-2"><a href="chapter2.html#cb44-2"></a><span class="kw">pirateplot</span>(Distance<span class="op">~</span>Condition,<span class="dt">data=</span>dd,<span class="dt">inf.method=</span><span class="st">&quot;ci&quot;</span>, <span class="dt">inf.disp=</span><span class="st">&quot;line&quot;</span>)</span></code></pre></div>
<p>Figure <a href="chapter2.html#fig:Figure2-5">2.5</a> suggests that the distributions are relatively symmetric which would suggest that the means and medians are similar even though only the means are displayed in these plots. In this display, none of the observations are flagged as outliers (it is not a part of this display). It is up to the consumer of the graphic to decide if observations look to be outside of the overall pattern of the rest of the observations. By plotting the observations by groups, we can also explore the narrowest (and likely most scary) overtakes in the data set. The <em>police</em> and <em>racer</em> conditions seem to have all observations over 25 cm and the most close passes were in the <em>novice</em> and <em>polite</em> outfits, including the two 2 cm passes. By displaying the original observations, we are able to explore and identify features that aggregation and summarization in plots can sometimes obfuscate. But the pirate-plots also allow you to compare the shape of the distributions (relatively symmetric and somewhat bell-shaped), variability (they look to have relatively similar variability), and the means of the groups. Our inferences are going to focus on the means but those inferences are only valid if the distributions are either approximately normal or at least have similar shapes and spreads (more on this soon).</p>
<p>It appears that the mean for <em>police</em> is higher than the other groups but that the others are not too different. But is this difference real? We will never
know the answer to that question, but we
can assess how likely we are to have seen a result as extreme or more
extreme than our result, assuming that there is no difference in the
means of the groups. And if the observed result is
(extremely) unlikely to occur, then we have (extremely) strong evidence against the hypothesis that the
groups have the same mean and can then conclude that there is likely a real
difference. If we discover that our result was not very unlikely, given the assumption of no difference in the mean of the groups, then we can’t conclude that there is a difference but also can’t conclude that they are equal, just that we failed to find enough evidence against the equal means assumption to discard it as a possibility. Whether the result is unusual or not, we will want to carefully explore how big the estimated differences in the means are – is the difference in means large enough to matter to you? We would be more interested in the implications of the difference in the means when there is strong evidence against the null hypothesis that the means are equal but the size of the estimated differences should always be of some interest. To accompany the pirate-plot that displays estimated means, we
need to have numerical values to compare. We can get means and standard
deviations by groups easily using the same formula notation as for the plots with the <code>mean</code>
and <code>sd</code> functions, if the <code>mosaic</code> package is loaded.</p>
<!-- <!-- \newpage -->
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="chapter2.html#cb45-1"></a><span class="kw">library</span>(mosaic)</span>
<span id="cb45-2"><a href="chapter2.html#cb45-2"></a><span class="kw">mean</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>dd)</span></code></pre></div>
<pre><code>##   casual  commute    hiviz   novice   police   polite    racer 
## 117.6110 114.6079 118.4383 116.9405 122.1215 114.0518 116.7559</code></pre>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="chapter2.html#cb47-1"></a><span class="kw">sd</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>dd)</span></code></pre></div>
<pre><code>##   casual  commute    hiviz   novice   police   polite    racer 
## 29.86954 29.63166 29.03384 29.03812 29.73662 31.23684 30.60059</code></pre>
<p>We can also use the <code>favstats</code> function to get those summaries and others by groups.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="chapter2.html#cb49-1"></a><span class="kw">favstats</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>dd)</span></code></pre></div>
<pre><code>##   Condition min    Q1 median  Q3 max     mean       sd   n missing
## 1    casual  17 100.0    117 134 245 117.6110 29.86954 779       0
## 2   commute   8  98.0    116 132 222 114.6079 29.63166 857       0
## 3     hiviz  12 101.0    117 134 237 118.4383 29.03384 737       0
## 4    novice   2 100.5    118 133 274 116.9405 29.03812 807       0
## 5    police  34 104.0    119 138 253 122.1215 29.73662 790       0
## 6    polite   2  95.0    114 133 225 114.0518 31.23684 868       0
## 7     racer  28  98.0    117 135 231 116.7559 30.60059 852       0</code></pre>
<p>Based on these results, we can see that there is an estimated difference of over 8 cm between the smallest mean (<em>polite</em> at 114.05 cm) and the largest mean (<em>police</em> at 122.12 cm). The differences among some of the other groups are much smaller, such as between <em>casual</em> and <em>commute</em> with sample means of 117.611 and 114.608 cm, respectively. Because there are seven groups being compared in this study, we will have to
wait until Chapter 3 and the One-Way ANOVA test to fully assess evidence
related to some difference among the seven groups. For now, we are going to
focus on comparing the mean <em>Distance</em> between <em>casual</em> and <em>commute</em> groups
– which is a <strong><em>two independent sample mean</em></strong> situation and something you
should have seen before. Remember that the “independent” sample part of
this refers to observations that are independently observed for the two
groups as opposed to the paired sample situation that you may have
explored where one observation from the first group is related to an
observation in the second group (the same person with one measurement in each group (we
generically call this “repeated measures”)
or the famous “twin” studies with one twin assigned to each group).  This study has some potential violations of the “independent” sample situation (for example, repeated measurements made during a single ride), but those do not clearly fit into the matched pairs situation, so we will note this potential issue and proceed with exploring the method that assumes that we have independent samples, even though this is not true here. In Chapter 9, methods for more complex study designs like this one will be discussed briefly, but mostly this is beyond the scope of this material.</p>
<p>Here we are going to use the “simple” two independent group scenario to
review some basic statistical concepts and connect two different
frameworks for conducting statistical inference: randomization and
parametric  inference techniques. <strong><em>Parametric</em></strong> statistical methods
involve making assumptions

about the distribution of the
responses and obtaining confidence intervals and/or p-values using a
<em>named</em> distribution (like the <span class="math inline">\(z\)</span> or <span class="math inline">\(t\)</span>-distributions). Typically these
results are generated using formulas and looking up areas under curves or
cutoffs using a table or a computer. <strong><em>Randomization</em></strong>-based statistical
methods use a computer to shuffle, sample, or simulate observations in ways
that allow you to obtain distributions of possible results to find areas and
cutoffs without resorting to using tables and named distributions.
Randomization methods are what are called <strong><em>nonparametric</em></strong> methods

that often make fewer assumptions (they are <strong><em>not free of assumptions</em></strong>!)
and so can handle a larger set of problems more easily than parametric
methods.

When the assumptions involved in the parametric procedures are
met by a data set, the randomization methods often provide very similar
results to those provided by the parametric techniques. To be a more
sophisticated statistical consumer, it is useful to have some knowledge
of both of these techniques for performing statistical inference and the fact that they can provide similar results might deepen your understanding of both
approaches.</p>
<p>To be able to work just with the observations from two of the conditions (<em>casual</em> and <em>commute</em>) we could remove all the other observations in a spreadsheet program and read that new data set
back into R, but it is actually pretty easy to use R to do data
management once the data set is loaded. It is also a better scientific process to do as much of your data management within R as possible so that your steps in managing the data are fully documented and reproducible. Highlighting and clicking in spreadsheet programs is a dangerous way to work and can be impossible to recreate steps that were taken from initial data set to the version that was analyzed. In R, we could identify the rows that contain the observations we want to retain and just extract those rows, but this is hard with over five thousand observations. The <code>subset</code> function (also an option in some functions) is the best way to be able to focus on observations that meet a particular condition, we can “subset” the data set to retain those rows. The <code>subset</code> function takes the data set as its first argument and then in the “subset” option, we need to define the condition we want to meet to retain those rows. Specifically, we need to define the variable we want to work with, <code>Condition</code>, and then request rows that meet a condition (are <code>%in%</code>) and the aspects that meet that condition (here by concatenating “casual” and “commute”), leading to code of:</p>
<pre><code>subset(dd, Condition %in% c(&quot;casual&quot;, &quot;commute&quot;))</code></pre>
<p>We would actually want to save that new subsetted data set into a new tibble for future work, so we can use the following to save the reduced data set into <code>ddsub</code>:</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="chapter2.html#cb52-1"></a>ddsub &lt;-<span class="st"> </span><span class="kw">subset</span>(dd, Condition <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;casual&quot;</span>, <span class="st">&quot;commute&quot;</span>))</span></code></pre></div>
<p>There is also a “select” option that we could also use to just focus on certain columns in the data set and we can use that just to focus on the <code>Condition</code> and <code>Distance</code> variables using:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="chapter2.html#cb53-1"></a>ddsub &lt;-<span class="st"> </span><span class="kw">subset</span>(dd, Condition <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;casual&quot;</span>, <span class="st">&quot;commute&quot;</span>), </span>
<span id="cb53-2"><a href="chapter2.html#cb53-2"></a>                <span class="dt">select=</span><span class="kw">c</span>(<span class="st">&quot;Distance&quot;</span>, <span class="st">&quot;Condition&quot;</span>))</span></code></pre></div>
<p>You will always want to check that the correct observations were dropped
either using <code>View(ddsub)</code> or by doing a quick summary of the
<code>Condition</code> variable in the new tibble.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="chapter2.html#cb54-1"></a><span class="kw">summary</span>(ddsub<span class="op">$</span>Condition)</span></code></pre></div>
<pre><code>##  casual commute   hiviz  novice  police  polite   racer 
##     779     857       0       0       0       0       0</code></pre>
<p>It ends up that R remembers the other categories even though there are
0 observations in them now and that can cause us some problems. When we remove a
group of observations, we sometimes need to clean up categorical variables to
just reflect the categories that are present. The <code>factor</code>

function
creates categorical variables based on the levels of the variables that are
observed and is useful to run here to clean up <code>Condition</code> to just reflect the categories that are now present.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="chapter2.html#cb56-1"></a>ddsub<span class="op">$</span>Condition &lt;-<span class="st"> </span><span class="kw">factor</span>(ddsub<span class="op">$</span>Condition)</span>
<span id="cb56-2"><a href="chapter2.html#cb56-2"></a><span class="kw">summary</span>(ddsub<span class="op">$</span>Condition)</span></code></pre></div>
<pre><code>##  casual commute 
##     779     857</code></pre>
<p>The two categories of interest now were selected because neither looks particularly “racey” or has high visibility but could present a common choice between getting fully “geared up” for the commute or just jumping on a bike to go to work. Now if we remake the boxplots and pirate-plots, they only contain results for
the two groups of interest here as seen in Figure <a href="chapter2.html#fig:Figure2-6">2.6</a>. Note that these are available in the previous version of the plots, but now we will just focus on these two groups.</p>

<div class="figure"><span id="fig:Figure2-6"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-6-1.png" alt="Boxplot and pirate-plot of the Distance responses on the reduced ddsub data set." width="552" />
<p class="caption">
Figure 2.6: Boxplot and pirate-plot of the <em>Distance</em> responses on the reduced <code>ddsub</code> data set.
</p>
</div>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="chapter2.html#cb58-1"></a><span class="kw">boxplot</span>(Distance<span class="op">~</span>Condition,<span class="dt">data=</span>ddsub) </span>
<span id="cb58-2"><a href="chapter2.html#cb58-2"></a><span class="kw">pirateplot</span>(Distance<span class="op">~</span>Condition,<span class="dt">data=</span>ddsub,<span class="dt">inf.method=</span><span class="st">&quot;ci&quot;</span>, <span class="dt">inf.disp=</span><span class="st">&quot;line&quot;</span>)</span></code></pre></div>
<!-- \newpage -->
<p>The two-sample mean techniques you learned in your previous course all
start with comparing the means the two groups. We can obtain the two
means using the <code>mean</code> function or directly obtain the difference
in the means using the <code>diffmean</code> function (both require the <code>mosaic</code>
package). The <code>diffmean</code> function provides
<span class="math inline">\(\bar{x}_\text{commute} - \bar{x}_\text{casual}\)</span> where <span class="math inline">\(\bar{x}\)</span>
(read as “x-bar”) is the sample mean of observations in the subscripted
group. Note that there are two directions that you could compare the
means and this function chooses to take the mean from the second group
name <em>alphabetically</em> and subtract the mean from the first alphabetical group
name. It is always good to check the direction of this calculation as
having a difference of <span class="math inline">\(-3.003\)</span> cm versus <span class="math inline">\(3.003\)</span> cm could be important.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="chapter2.html#cb59-1"></a><span class="kw">mean</span>(Distance<span class="op">~</span>Condition,<span class="dt">data=</span>ddsub)</span></code></pre></div>
<pre><code>##   casual  commute 
## 117.6110 114.6079</code></pre>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="chapter2.html#cb61-1"></a><span class="kw">diffmean</span>(Distance<span class="op">~</span>Condition,<span class="dt">data=</span>ddsub)</span></code></pre></div>
<pre><code>##  diffmean 
## -3.003105</code></pre>

</div>
<div id="section2-3" class="section level2">
<h2><span class="header-section-number">2.3</span> Models, hypotheses, and permutations for the two sample mean situation</h2>

<p>There appears to be some evidence that the <em>casual</em> clothing group is
getting higher average overtake distances than
the <em>commute</em> group of observations, but we want to try to make sure that the difference is
real – to assess evidence against the assumption that the means
are the same “in the population” and possibly decide that this is not a reasonable assumption. First, a <strong><em>null hypothesis</em></strong><a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a> which
defines a <strong><em>null model</em></strong><a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a>

needs to be determined in terms of <strong><em>parameters</em></strong> (the true values in
the population). The research question should help you determine the form of the
hypotheses for the assumed population. In the two independent sample mean
problem, the interest is in testing a null hypothesis of <span class="math inline">\(H_0: \mu_1 = \mu_2\)</span>
versus the alternative hypothesis of <span class="math inline">\(H_A: \mu_1 \ne \mu_2\)</span>, where
<span class="math inline">\(\mu_1\)</span> is the parameter for the true mean of the first group and <span class="math inline">\(\mu_2\)</span>
is the parameter for the true mean of the second group. The alternative
hypothesis involves assuming a statistical model

for the <span class="math inline">\(i^{th}\ (i=1,\ldots,n_j)\)</span>
response from the <span class="math inline">\(j^{th}\ (j=1,2)\)</span> group, <span class="math inline">\(\boldsymbol{y}_{ij}\)</span>, that
involves modeling it as <span class="math inline">\(y_{ij} = \mu_j + \varepsilon_{ij}\)</span>,
where we assume that <span class="math inline">\(\varepsilon_{ij} \sim N(0,\sigma^2)\)</span>. For the moment,
focus on the models that either assume the means are the same (null) or
different (alternative),

which imply:</p>
<ul>
<li><p>Null Model: <span class="math inline">\(y_{ij} = \mu + \varepsilon_{ij}\)</span> There is <strong>no</strong>
difference in <strong>true</strong> means for the two groups.</p></li>
<li><p>Alternative Model: <span class="math inline">\(y_{ij} = \mu_j + \varepsilon_{ij}\)</span> There is <strong>a</strong>
difference in <strong>true</strong> means for the two groups.</p></li>
</ul>
<p>Suppose we are considering the alternative model for the 4<sup>th</sup>
observation (<span class="math inline">\(i=4\)</span>) from the second group (<span class="math inline">\(j=2\)</span>), then the model for
this observation is <span class="math inline">\(y_{42} = \mu_2 +\varepsilon_{42}\)</span>, that defines the
response as coming from the true mean for the second group plus a
random error term for that observation, <span class="math inline">\(\varepsilon_{42}\)</span>. For, say, the
5<sup>th</sup> observation from the first group (<span class="math inline">\(j=1\)</span>), the model is
<span class="math inline">\(y_{51} = \mu_1 +\varepsilon_{51}\)</span>. If we were working with the null model,
the mean is always the same (<span class="math inline">\(\mu\)</span>) – the group specified does not change
the mean we use for that observation, so the model for <span class="math inline">\(y_{42}\)</span> would be <span class="math inline">\(\mu +\varepsilon_{42}\)</span>.</p>
<p>It can be helpful to think about the null and alternative models graphically.

By assuming the null hypothesis is true (means are equal) and that the random
errors around the mean follow a normal distribution,

we assume that the truth
is as displayed in the left panel of Figure <a href="chapter2.html#fig:Figure2-7">2.7</a> – two
normal distributions with the same mean and variability. The alternative
model allows the two groups to potentially have different means, such as
those displayed in the right panel of Figure <a href="chapter2.html#fig:Figure2-7">2.7</a> where the
second group has a larger mean. Note that in this scenario, we assume that
the observations all came from the same distribution except that they had
different means. Depending on the statistical procedure we are using, we
basically are going to assume that the observations (<span class="math inline">\(y_{ij}\)</span>) either were
generated as samples from the null or alternative model. You can imagine
drawing observations at random from the pictured distributions. For hypothesis
testing, the null model

is assumed to be true and then the unusualness of
the actual result is assessed relative to that assumption. In hypothesis
testing, we have to decide if we have enough evidence to reject the assumption
that the null model (or hypothesis) is true. If we think that we have sufficient evidence to conclude that the null hypothesis is wrong,
then we would conclude that the other model considered (the alternative
model)

is more reasonable. The researchers obviously would have hoped to
encounter some sort of noticeable difference in the distances for the
different outfits and have been able to find enough evidence to against the null
model where the groups “look the same” to be able to conclude that they differ.</p>

<div class="figure" style="text-align: center"><span id="fig:Figure2-7"></span>
<img src="chapter2_files/image015.png" alt="Illustration of the assumed situations under the null (left) and a single possibility that could occur if the alternative were true (right) and the true means were different. There are an infinite number of ways to make a plot like the right panel that satisfies the alternative hypothesis." width="100%" />
<p class="caption">
Figure 2.7: Illustration of the assumed situations under the null (left) and a single possibility that could occur if the alternative were true (right) and the true means were different. There are an infinite number of ways to make a plot like the right panel that satisfies the alternative hypothesis.
</p>
</div>
<p>In statistical inference, null hypotheses (and their
implied models) are set
up as “straw men” with every interest in rejecting them even though we assume
they are true to be able to assess the evidence <span class="math inline">\(\underline{\text{against them}}\)</span>.
Consider the original study design here, the outfits were randomly assigned to
the rides. If the null hypothesis were true, then we would have no difference
in the population means of the groups. And this would apply if we had done a
different random assignment  of the outfits. So let’s try this:
assume that the null hypothesis is true and randomly re-assign the treatments
(outfits) to the observations that were obtained. In other words, keep the
<em>Distance</em> results the same and shuffle the group labels randomly. The
technical term for this is doing a <strong><em>permutation</em></strong>  (a random shuffling of
a grouping<a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a> variable relative to the observed responses). If the null is true
and the means
in the two groups are the same, then we should be able to re-shuffle the
groups to the observed <em>Distance</em> values and get results similar to those we
actually observed. If the null is false and the means are really different in
the two groups, then what we observed should differ from what we get under
other random permutations and the differences between the two groups should be
more noticeable in the observed data set than in (most) of the shuffled data
sets. It helps to see an example of a permutation of the labels to understand
what this means here.</p>
<p>The data set we are working with is a little on the large size, especially to explore individual observations. So for the moment we are going to work with a random sample of 30 of the <span class="math inline">\(n=1,636\)</span> observations in <code>ddsub</code>, fifteen from each group, that are generated using the <code>sample</code> function. To do this<a href="#fn25" class="footnote-ref" id="fnref25"><sup>25</sup></a>, we will use the <code>sample</code> function  twice - once to sample from the subsetted <em>commute</em> observations (creating the <code>s1</code> data set) and once to sample from the <em>casual</em> ones (creating <code>s2</code>). A new function for us, called <code>rbind</code>,  is used to bind the rows together — much like pasting a chunk of rows below another chunk in a spreadsheet program. This operation only works if the columns all have the same names and meanings both for <code>rbind</code> and in a spreadsheet. Together this code creates the <code>dsample</code> data set that we will analyze below and compare to results from the full data set. The sample means are now 135.8 and 109.87 cm for <em>casual</em> and <em>commute</em> groups, respectively, and so the difference in the sample means has increased in magnitude to -25.93 cm (commute - casual). This difference would vary based on the different random samples from the larger data set, but for the moment, pretend this was the entire data set that the researchers had collected and that we want to try to assess how unusual our sample difference was from what we might expect, if the null hypothesis that the true means are the same in these two groups was true.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="chapter2.html#cb63-1"></a><span class="kw">set.seed</span>(<span class="dv">9432</span>)</span>
<span id="cb63-2"><a href="chapter2.html#cb63-2"></a>s1 &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">subset</span>(ddsub, Condition <span class="op">%in%</span><span class="st"> &quot;commute&quot;</span>), <span class="dt">size=</span><span class="dv">15</span>)</span>
<span id="cb63-3"><a href="chapter2.html#cb63-3"></a>s2 &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">subset</span>(ddsub, Condition <span class="op">%in%</span><span class="st"> &quot;casual&quot;</span>), <span class="dt">size=</span><span class="dv">15</span>)</span>
<span id="cb63-4"><a href="chapter2.html#cb63-4"></a>dsample &lt;-<span class="st"> </span><span class="kw">rbind</span>(s1, s2)</span>
<span id="cb63-5"><a href="chapter2.html#cb63-5"></a><span class="kw">mean</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>dsample)</span></code></pre></div>
<pre><code>##   casual  commute 
## 135.8000 109.8667</code></pre>
<p>In order to assess evidence against the null hypothesis of no difference, we want to permute the group labels versus the observations. In the <code>mosaic</code> package, the <code>shuffle</code> function allows us to easily perform
a   permutation<a href="#fn26" class="footnote-ref" id="fnref26"><sup>26</sup></a>. One permutation of the
treatment labels is provided in the <code>PermutedCondition</code> variable below. Note
that the <code>Distances</code> are held in the same place while the group labels are shuffled.</p>
<!-- \newpage -->
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="chapter2.html#cb65-1"></a>Perm1 &lt;-<span class="st"> </span><span class="kw">with</span>(dsample, <span class="kw">tibble</span>(Distance, Condition, <span class="dt">PermutedCondition=</span><span class="kw">shuffle</span>(Condition)))</span>
<span id="cb65-2"><a href="chapter2.html#cb65-2"></a><span class="co">#To force the tibble to print out all rows in data set - not used often</span></span>
<span id="cb65-3"><a href="chapter2.html#cb65-3"></a><span class="kw">data.frame</span>(Perm1) </span></code></pre></div>
<pre><code>##    Distance Condition PermutedCondition
## 1       168   commute           commute
## 2       137   commute           commute
## 3        80   commute            casual
## 4       107   commute           commute
## 5       104   commute            casual
## 6        60   commute            casual
## 7        88   commute           commute
## 8       126   commute           commute
## 9       115   commute            casual
## 10      120   commute            casual
## 11      146   commute           commute
## 12      113   commute            casual
## 13       89   commute           commute
## 14       77   commute           commute
## 15      118   commute            casual
## 16      148    casual            casual
## 17      114    casual            casual
## 18      124    casual           commute
## 19      115    casual            casual
## 20      102    casual            casual
## 21       77    casual            casual
## 22       72    casual           commute
## 23      193    casual           commute
## 24      111    casual           commute
## 25      161    casual            casual
## 26      208    casual           commute
## 27      179    casual            casual
## 28      143    casual           commute
## 29      144    casual           commute
## 30      146    casual            casual</code></pre>
<p>If you count up the number of subjects in each group by counting the number
of times each label (commute, casual) occurs, it is the same in both the
<code>Condition</code> and <code>PermutedCondition</code> columns (15 each). Permutations involve randomly
re-ordering the values of a variable – here the <code>Condition</code> group labels – without
changing the content of the variable.

This result can also be generated using
what is called <strong><em>sampling without replacement</em></strong>:  sequentially select <span class="math inline">\(n\)</span> labels
from the original variable (<em>Condition</em>), removing each observed label and making sure that each of the
original <code>Condition</code> labels is selected once and only once. The new, randomly
selected order of selected labels provides the permuted labels. Stepping
through the process helps to understand how it works: after the initial random
sample of one label, there would <span class="math inline">\(n - 1\)</span> choices possible; on the <span class="math inline">\(n^{th}\)</span>
selection, there would only be one label remaining to select. This makes sure
that all original labels are re-used but that the order is random. Sampling
without replacement is like picking names out of a hat, one-at-a-time, and not
putting the names back in after they are selected. It is an exhaustive process
for all the original observations. <strong><em>Sampling with replacement</em></strong>,  in contrast,
involves sampling from the specified list with each observation having an equal
chance of selection for each sampled observation – in other words, observations
can be selected more than once. This is like picking <span class="math inline">\(n\)</span> names out of a hat that
contains <span class="math inline">\(n\)</span> names, except that every time a name is selected, it goes back into
the hat – we’ll use this technique in Section <a href="chapter2.html#section2-9">2.9</a>
to do what is called <strong><em>bootstrapping</em></strong>.

Both sampling mechanisms can be
used to generate inferences but each has particular situations
where they are most useful. For hypothesis testing,

we will use permutations 
(sampling without replacement) as its mechanism most closely matches the null hypotheses we will be testing.</p>
<p>The comparison of the pirate-plots  between the real <span class="math inline">\(n=30\)</span> data set and permuted version is what is really interesting (Figure <a href="chapter2.html#fig:Figure2-8">2.8</a>). The
original difference in the sample means of the two groups was -25.93 cm (<em>commute</em> - <em>casual</em>). The sample means are the <strong><em>statistics</em></strong>
that estimate the parameters for the true means of the two groups and the difference in the sample means is a way to create a single number that tracks a quantity directly related to the difference between the null and alternative models. In the
permuted data set, the difference in the means is 12.07 cm in the opposite
direction (the <em>commute</em> group had a higher mean than <em>casual</em> in the permuted data).</p>
<!-- \newpage -->
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="chapter2.html#cb67-1"></a><span class="kw">mean</span>(Distance<span class="op">~</span>PermutedCondition, <span class="dt">data=</span>Perm1)</span></code></pre></div>
<pre><code>##   casual  commute 
## 116.8000 128.8667</code></pre>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="chapter2.html#cb69-1"></a><span class="kw">diffmean</span>(Distance<span class="op">~</span>PermutedCondition, <span class="dt">data=</span>Perm1)</span></code></pre></div>
<pre><code>## diffmean 
## 12.06667</code></pre>

<div class="figure"><span id="fig:Figure2-8"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-8-1.png" alt="Pirate-plots of Distance responses versus actual treatment groups and permuted groups. Note how the responses are the same but that they are shuffled between the two groups differently in the permuted data set. With the smaller sample size, the 95% confidence intervals for each of the means are more clearly visible than with the original large data set." width="672" />
<p class="caption">
Figure 2.8: Pirate-plots of Distance responses versus actual treatment groups and permuted groups. Note how the responses are the same but that they are shuffled between the two groups differently in the permuted data set. With the smaller sample size, the 95% confidence intervals for each of the means are more clearly visible than with the original large data set.
</p>
</div>
<p>The <code>diffmean</code> function is a simple way to get the differences in the means, but we can also start to learn about using the <code>lm</code>  function – that will be used for every chapter except for Chapter <a href="chapter5.html#chapter5">5</a>. The <code>lm</code> stands for <strong><em>linear model</em></strong>  and, as we will see moving forward, encompasses a wide array of different models and scenarios. The ability to estimate the difference in the mean of two groups is among its simplest uses.<a href="#fn27" class="footnote-ref" id="fnref27"><sup>27</sup></a> Notationally, it is very similar to other functions we have considered, <code>lm(y ~ x, data=...)</code> where <code>y</code> is the response variable and <code>x</code> is the explanatory variable. Here that is <code>lm(Distance~Condition, data=dsample)</code> with <code>Condition</code> defined as a factor variable. With linear models, we will need to interrogate them to obtain a variety of useful information and our first “interrogation” function is usually the <code>summary</code> function. To use it, it is best to have stored the model into an object, something like <code>lm1</code>, and then we can apply the <code>summary()</code>  function to the stored model object to get a suite of output:</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="chapter2.html#cb71-1"></a>lm1 &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>dsample)</span>
<span id="cb71-2"><a href="chapter2.html#cb71-2"></a><span class="kw">summary</span>(lm1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Distance ~ Condition, data = dsample)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -63.800 -21.850   4.133  15.150  72.200 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)       135.800      8.863  15.322 3.83e-15
## Conditioncommute  -25.933     12.534  -2.069   0.0479
## 
## Residual standard error: 34.33 on 28 degrees of freedom
## Multiple R-squared:  0.1326, Adjusted R-squared:  0.1016 
## F-statistic: 4.281 on 1 and 28 DF,  p-value: 0.04789</code></pre>
<p>This output is explored more in Chapter <a href="chapter3.html#chapter3">3</a>, but for the moment, focus on the row labeled as <code>Conditioncommute</code> in the middle of the output. In the first (<code>Estimate</code>) column, there is -25.933. This is a number we saw before – it is the difference in the sample means between <em>commute</em> and <em>casual</em> (<em>commute</em> - <em>casual</em>). When <code>lm</code> denotes a category in the row of the output (here <code>commute</code>), it is trying to indicate that the information to follow relates to the difference between this category and a baseline or reference category (here <code>casual</code>). The first (<code>(Intercept)</code>) row also contains a number we have seen before: - 135.8 is the sample mean for the <em>casual</em> group. So the <code>lm</code> is generating a coefficient for the mean of one of the groups and another as the difference in the two groups<a href="#fn28" class="footnote-ref" id="fnref28"><sup>28</sup></a>. In developing a test to assess evidence against the null hypothesis, we will focus on the difference in the sample means. So we want to be able to extract that number from this large suite of information. It ends up that we can apply the <code>coef</code>  function to <code>lm</code> models and then access that second coefficient using the bracket notation. Specifically:</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="chapter2.html#cb73-1"></a><span class="kw">coef</span>(lm1)[<span class="dv">2</span>]</span></code></pre></div>
<pre><code>## Conditioncommute 
##        -25.93333</code></pre>
<p>This is the same result as using the <code>diffmean</code> function, so either could be used here. The estimated difference in the sample means in the permuted data set of 12.07 cm is available with:</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="chapter2.html#cb75-1"></a>lmP &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span>PermutedCondition, <span class="dt">data=</span>Perm1)</span>
<span id="cb75-2"><a href="chapter2.html#cb75-2"></a><span class="kw">coef</span>(lmP)[<span class="dv">2</span>]</span></code></pre></div>
<pre><code>## PermutedConditioncommute 
##                 12.06667</code></pre>
<p>Comparing the pirate-plots and the estimated difference in the sample means suggests that the observed difference was larger than what we got
when we did a single permutation.  Conceptually, permuting observations between group labels is
consistent with the null hypothesis – this is a technique to generate results
that we might have gotten if the null hypothesis were true since the true models for the responses
are the same in the two groups if the null is true. We just need to repeat the
permutation process many times and track how unusual our observed result is
relative to this distribution of potential responses if the null were true.
If the observed differences are unusual relative to the results under
permutations, then there is evidence against the null hypothesis, and we can conclude,
in the direction of the alternative hypothesis, that the
true means differ. If the observed differences are similar to (or at least not
unusual relative to) what we get under random shuffling under the null model,
we would have a tough time concluding that there is any real difference between
the groups based on our observed data set. This is formalized using the <strong><em>p-value</em></strong> as a measure of the strength of evidence against the null hypothesis and how we use it.</p>
</div>
<div id="section2-4" class="section level2">
<h2><span class="header-section-number">2.4</span> Permutation testing for the two sample mean situation</h2>
<p>In any testing situation, you must define some function of the observations that
gives us a single number that addresses our question of interest. This quantity
is called a <strong><em>test statistic</em></strong>. These often take on complicated forms and
have names like <span class="math inline">\(t\)</span> or <span class="math inline">\(z\)</span> statistics that relate to their parametric

(named)
distributions so we know where to look up
<strong><em>p-values</em></strong><a href="#fn29" class="footnote-ref" id="fnref29"><sup>29</sup></a>. In randomization settings, they can
have simpler forms because we use the data set to find the
distribution of the statistic under the null hypothesis and don’t need to rely on a
named distribution. We will label our test statistic <strong><em>T</em></strong>
(for <strong>T</strong>est statistic) unless the test statistic has a commonly
used name. Since we are interested in comparing the means of the two groups, we
can define</p>
<p><span class="math display">\[T=\bar{x}_\text{commute} - \bar{x}_\text{casual},\]</span></p>
<p>which coincidentally is what the <code>diffmean</code> function and the second coefficient from the <code>lm</code> provided us previously.
We label our <strong><em>observed test statistic</em></strong> (the one from the original data
set) as</p>
<p><span class="math display">\[T_{obs}=\bar{x}_\text{commute} - \bar{x}_\text{casual},\]</span></p>
<p>which happened to be -25.933 cm here. We will compare this result to the results
for the test statistic that we obtain from permuting the group labels. To
denote permuted results, we will add an * to the labels:</p>
<p><span class="math display">\[T^*=\bar{x}_{\text{commute}^*}-\bar{x}_{\text{casual}^*}.\]</span></p>
<p>We then compare the <span class="math inline">\(T_{obs}=\bar{x}_\text{commute} - \bar{x}_\text{casual} = -25.933\)</span>
to the distribution of results that are possible for the permuted results (<span class="math inline">\(T^*\)</span>)
which corresponds to assuming the null hypothesis is true.</p>
<p>We need to consider lots of permutations to do a permutation test.

In contrast to
your introductory statistics course where, if you did this, it was just a click
away, we are going to learn what was going on “under the hood” of the software you were using. Specifically, we
need a <strong><em>for loop</em></strong>  in R to be able to repeatedly generate the permuted data
sets and record <span class="math inline">\(T^*\)</span> for each one. Loops are a basic programming task that make
randomization methods possible as well as potentially simplifying any repetitive
computing task. To write a “for loop”, we need to choose how many times we want
to do the loop (call that <code>B</code>) and decide on a counter to keep track of where
we are at in the loops (call that <code>b</code>, which goes from 1 up to <code>B</code>). The
simplest loop just involves printing out the index, <code>print(b)</code> at each step.
This is our first use of curly braces, { and }, that are used to group the code
we want to repeatedly run as we proceed through the loop. By typing the following
code in a codechunk and then highlighting it all and hitting the run button,
R will go through the loop <em>B</em> = 5 times, printing out the counter:</p>
<pre><code>B &lt;- 5
for (b in (1:B)){
  print(b)
}</code></pre>
<p>Note that when you highlight and run the code, it will look about the same with
“+” printed after the first line to indicate that all the code is connected when
it appears in the console, looking like this:</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="chapter2.html#cb78-1"></a><span class="op">&gt;</span><span class="st"> </span><span class="cf">for</span>(b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</span>
<span id="cb78-2"><a href="chapter2.html#cb78-2"></a><span class="op">+</span><span class="st">   </span><span class="kw">print</span>(b)</span>
<span id="cb78-3"><a href="chapter2.html#cb78-3"></a><span class="op">+</span><span class="st"> </span>}</span></code></pre></div>
<p>When you run these three lines of code (or compile a .Rmd file that contains this), the console will show you the following
output:</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="chapter2.html#cb79-1"></a>[<span class="dv">1</span>] <span class="dv">1</span></span>
<span id="cb79-2"><a href="chapter2.html#cb79-2"></a>[<span class="dv">1</span>] <span class="dv">2</span></span>
<span id="cb79-3"><a href="chapter2.html#cb79-3"></a>[<span class="dv">1</span>] <span class="dv">3</span></span>
<span id="cb79-4"><a href="chapter2.html#cb79-4"></a>[<span class="dv">1</span>] <span class="dv">4</span></span>
<span id="cb79-5"><a href="chapter2.html#cb79-5"></a>[<span class="dv">1</span>] <span class="dv">5</span></span></code></pre></div>
<p>Instead of printing the counter, we want to use the loop to repeatedly compute
our test statistic across <em>B</em> random permutations of the observations. The
<code>shuffle</code> function performs permutations of the group labels relative to
responses and the <code>coef(lmP)[2]</code> extracts the estimated difference in the two group means in the permuted
data set. For a single permutation, the combination of shuffling <code>Condition</code> and
finding the difference in the means, storing it in a variable called <code>Ts</code> is:</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="chapter2.html#cb80-1"></a>lmP &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span><span class="kw">shuffle</span>(Condition), <span class="dt">data=</span>dsample)</span>
<span id="cb80-2"><a href="chapter2.html#cb80-2"></a>Ts &lt;-<span class="st"> </span><span class="kw">coef</span>(lmP)[<span class="dv">2</span>]</span>
<span id="cb80-3"><a href="chapter2.html#cb80-3"></a>Ts</span></code></pre></div>
<pre><code>## shuffle(Condition)commute 
##               -0.06666667</code></pre>
<p>And putting this inside the <code>print</code> function allows us to find the test
statistic under 5 different permutations easily:</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="chapter2.html#cb82-1"></a>B &lt;-<span class="st"> </span><span class="dv">5</span></span>
<span id="cb82-2"><a href="chapter2.html#cb82-2"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</span>
<span id="cb82-3"><a href="chapter2.html#cb82-3"></a>  lmP &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span><span class="kw">shuffle</span>(Condition), <span class="dt">data=</span>dsample)</span>
<span id="cb82-4"><a href="chapter2.html#cb82-4"></a>  Ts &lt;-<span class="st"> </span><span class="kw">coef</span>(lmP)[<span class="dv">2</span>]</span>
<span id="cb82-5"><a href="chapter2.html#cb82-5"></a>  <span class="kw">print</span>(Ts)</span>
<span id="cb82-6"><a href="chapter2.html#cb82-6"></a>}</span></code></pre></div>
<pre><code>## shuffle(Condition)commute 
##                      -1.4 
## shuffle(Condition)commute 
##                  1.133333 
## shuffle(Condition)commute 
##                  20.86667 
## shuffle(Condition)commute 
##                  3.133333 
## shuffle(Condition)commute 
##                 -2.333333</code></pre>
<p>Finally, we would like to store the values of the test statistic instead of
just printing them out on each pass through the loop. To do this, we need to
create a variable to store the results, let’s call it <code>Tstar</code>. We know that
we need to store <code>B</code> results so will create a vector<a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a> of length <em>B</em>, which
contains <em>B</em> elements, full of missing values (NA) using the <code>matrix</code>  function with the <code>nrow</code> option specifying the number of elements:</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="chapter2.html#cb84-1"></a>Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</span>
<span id="cb84-2"><a href="chapter2.html#cb84-2"></a>Tstar</span></code></pre></div>
<pre><code>##      [,1]
## [1,]   NA
## [2,]   NA
## [3,]   NA
## [4,]   NA
## [5,]   NA</code></pre>
<p>Now we can run our loop <em>B</em> times and store the results in <code>Tstar</code>.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="chapter2.html#cb86-1"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</span>
<span id="cb86-2"><a href="chapter2.html#cb86-2"></a>  lmP &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span><span class="kw">shuffle</span>(Condition), <span class="dt">data=</span>dsample)</span>
<span id="cb86-3"><a href="chapter2.html#cb86-3"></a>  Tstar[b] &lt;-<span class="st"> </span><span class="kw">coef</span>(lmP)[<span class="dv">2</span>]</span>
<span id="cb86-4"><a href="chapter2.html#cb86-4"></a>}</span>
<span id="cb86-5"><a href="chapter2.html#cb86-5"></a><span class="co">#Print out the results stored in Tstar with the next line of code</span></span></code></pre></div>
<!-- \newpage -->
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="chapter2.html#cb87-1"></a>Tstar</span></code></pre></div>
<pre><code>##           [,1]
## [1,] -5.400000
## [2,] -3.266667
## [3,] -7.933333
## [4,] 13.133333
## [5,] -6.466667</code></pre>
<p>Five permutations are still not enough to assess whether our <span class="math inline">\(T_{obs}\)</span>
of -25.933 is unusual and we need to do many permutations to get an accurate
assessment of the possibilities under the null hypothesis.

It is common practice
to consider something like 1,000 permutations. The <code>Tstar</code> vector when we set
<em>B</em> to be large, say <code>B=1000</code>, contains the permutation distribution  for the
selected test statistic under<a href="#fn31" class="footnote-ref" id="fnref31"><sup>31</sup></a> the null
hypothesis – what is called the <strong><em>null distribution</em></strong> of the statistic. The
null distribution is the distribution of possible values of a statistic
under the null hypothesis. We want to visualize this distribution and use it to
assess how unusual our <span class="math inline">\(T_{obs}\)</span> result of -25.933 cm was relative to all the
possibilities under permutations (under the null hypothesis). So we repeat the
loop, now with <span class="math inline">\(B=1000\)</span> and generate a histogram, density curve, and summary
statistics of the results:</p>

<div class="figure"><span id="fig:Figure2-9"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-9-1.png" alt="Histogram (left, with counts in bars) and density curve (right) of values of test statistic for B = 1,000 permutations." width="960" />
<p class="caption">
Figure 2.9: Histogram (left, with counts in bars) and density curve (right) of values of test statistic for <em>B</em> = 1,000 permutations.
</p>
</div>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="chapter2.html#cb89-1"></a>B &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb89-2"><a href="chapter2.html#cb89-2"></a>Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</span>
<span id="cb89-3"><a href="chapter2.html#cb89-3"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</span>
<span id="cb89-4"><a href="chapter2.html#cb89-4"></a>  lmP &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span><span class="kw">shuffle</span>(Condition), <span class="dt">data=</span>dsample)</span>
<span id="cb89-5"><a href="chapter2.html#cb89-5"></a>  Tstar[b] &lt;-<span class="st"> </span><span class="kw">coef</span>(lmP)[<span class="dv">2</span>]</span>
<span id="cb89-6"><a href="chapter2.html#cb89-6"></a>}</span>
<span id="cb89-7"><a href="chapter2.html#cb89-7"></a><span class="kw">hist</span>(Tstar, <span class="dt">label=</span>T,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">300</span>))</span>
<span id="cb89-8"><a href="chapter2.html#cb89-8"></a><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="chapter2.html#cb90-1"></a><span class="kw">favstats</span>(Tstar)</span></code></pre></div>
<pre><code>##        min        Q1     median  Q3      max       mean       sd    n missing
##  -41.26667 -10.06667 -0.3333333 8.6 37.26667 -0.5054667 13.17156 1000       0</code></pre>
<p>Figure <a href="chapter2.html#fig:Figure2-9">2.9</a> contains visualizations of <span class="math inline">\(T^*\)</span> and the <code>favstats</code>
summary provides the related numerical summaries. Our observed <span class="math inline">\(T_{obs}\)</span>
of -25.933 seems somewhat unusual relative to these results with only
9 <span class="math inline">\(T^*\)</span> values smaller than -30 based on the
histogram. We need to make more specific comparisons of the permuted results
versus our observed result to be able to clearly decide whether our observed
result is really unusual.</p>
<p>To make the comparisons more concrete, first we can enhance the previous graphs
by adding the value of the test statistic from the real data set, as shown in
Figure <a href="chapter2.html#fig:Figure2-10">2.10</a>, using the <code>abline</code>  function to draw a vertical
line at our <span class="math inline">\(T_{obs}\)</span> value specified in the <code>v</code> (for vertical) option.</p>

<div class="figure"><span id="fig:Figure2-10"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-10-1.png" alt="Histogram (left) and density curve (right) of values of test statistic for 1,000 permutations with bold vertical line for value of observed test statistic." width="960" />
<p class="caption">
Figure 2.10: Histogram (left) and density curve (right) of values of test statistic for 1,000 permutations with bold vertical line for value of observed test statistic.
</p>
</div>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="chapter2.html#cb92-1"></a>Tobs &lt;-<span class="st"> </span><span class="fl">-25.933</span></span>
<span id="cb92-2"><a href="chapter2.html#cb92-2"></a><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)</span>
<span id="cb92-3"><a href="chapter2.html#cb92-3"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span>
<span id="cb92-4"><a href="chapter2.html#cb92-4"></a><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</span>
<span id="cb92-5"><a href="chapter2.html#cb92-5"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p>Second, we can calculate the exact number of permuted results that were as
small or smaller
than what we observed. To calculate the proportion of the 1,000 values that were
as small or smaller than what we observed, we will use the <code>pdata</code> function.

To use this
function, we need to provide the distribution of values to compare to the cut-off
(<code>Tstar</code>), the cut-off point (<code>Tobs</code>), and whether we want calculate the
proportion that are below (left of) or above (right of) the cut-off
(<code>lower.tail=T</code> option provides the proportion of values to the left of (below) the cutoff
of interest).</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="chapter2.html#cb93-1"></a><span class="kw">pdata</span>(Tstar, Tobs, <span class="dt">lower.tail=</span>T)[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>## [1] 0.027</code></pre>
<p>The proportion of 0.027 tells us that 27 of the 1,000 permuted results
(2.7%) were as small or smaller than what we observed. This type of
work is how we can
generate <strong><em>p-values</em></strong> using permutation distributions.


P-values,

as you should
remember, are the probability of getting a result as extreme as or more extreme
than what we observed, <span class="math inline">\(\underline{\text{given that the null is true}}\)</span>. Finding
only 27
permutations of 1,000 that were as small or smaller than our observed result suggests that it
is hard to find a result like what we observed if there really were no difference,
although it is not impossible.</p>
<p>When testing hypotheses for two groups, there are two types of alternative
hypotheses, one-sided or two-sided. <strong><em>One-sided tests</em></strong> involve only considering
differences in one-direction (like <span class="math inline">\(\mu_1 &gt; \mu_2\)</span>) and are performed when
researchers can decide <strong><em>a priori</em></strong><a href="#fn32" class="footnote-ref" id="fnref32"><sup>32</sup></a> which group should have a larger mean
if there is going to be any sort of difference. In this situation, we did not
know enough about the potential impacts of the outfits to know which group should
be larger than the other so should do a two-sided test. It is important to
remember that you can’t look at the responses to decide on the hypotheses. It is
often safer and more <strong><em>conservative</em></strong><a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a>   to start with a
<strong><em>two-sided alternative</em></strong> (<span class="math inline">\(\mathbf{H_A: \mu_1 \ne \mu_2}\)</span>). To do a 2-sided
test, find the area smaller than what we observed as above (or larger if the test statistic had been positive). We also need to add
the area in the other tail (here the right tail) similar to what we observed in the
right tail. Some statisticians suggest doubling the area in one tail but we will collect
information on the number that were as or more extreme than the same
value in the other
tail<a href="#fn34" class="footnote-ref" id="fnref34"><sup>34</sup></a>. In other words, we count the proportion below -25.933 and over 25.933. So
we need to find how many of the permuted results were larger than or equal
to 25.933 cm
to add to our previous proportion. Using <code>pdata</code> with <code>-Tobs</code> as the cut-off
and <code>lower.tail =F</code> provides this result:
</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="chapter2.html#cb95-1"></a><span class="kw">pdata</span>(Tstar, <span class="op">-</span>Tobs, <span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>## [1] 0.017</code></pre>
<p>So the p-value to test our null hypothesis of no difference in the true means
between the groups is 0.027 + 0.017, providing a p-value of 0.044.
Figure <a href="chapter2.html#fig:Figure2-11">2.11</a> shows both cut-offs on the histogram and density curve.</p>

<div class="figure"><span id="fig:Figure2-11"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-11-1.png" alt="Histogram and density curve of values of test statistic for 1,000 permutations with bold lines for value of observed test statistic (-25.933) and its opposite value (25.933) required for performing the two-sided test." width="960" />
<p class="caption">
Figure 2.11: Histogram and density curve of values of test statistic for 1,000 permutations with bold lines for value of observed test statistic (-25.933) and its opposite value (25.933) required for performing the two-sided test.
</p>
</div>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="chapter2.html#cb97-1"></a><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)</span>
<span id="cb97-2"><a href="chapter2.html#cb97-2"></a><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span>
<span id="cb97-3"><a href="chapter2.html#cb97-3"></a><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</span>
<span id="cb97-4"><a href="chapter2.html#cb97-4"></a><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p>In general, the <strong><em>one-sided test p-value</em></strong>

is the proportion of the
permuted results that are as extreme or more extreme than observed in the
direction of the <em>alternative</em>
hypothesis (lower or upper tail, remembering that this also depends on the
direction of the difference taken). For the two-sided test, the p-value

is the
proportion of the permuted results that are <em>less than or equal to the negative
version of the observed statistic and greater than or equal to the positive
version of the observed statistic</em>. Using absolute
values (| |), we can simplify this: the <strong><em>two-sided p-value</em></strong> is the
<em>proportion of the |permuted statistics| that are as large or larger than
|observed statistic|</em>.
This will always work and finds areas in both tails regardless of whether the
observed statistic is positive or negative. In R, the <code>abs</code> function  provides the
<strong><em>absolute value</em></strong> and we can again use <code>pdata</code> to find our p-value in one line
of code:
</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="chapter2.html#cb98-1"></a><span class="kw">pdata</span>(<span class="kw">abs</span>(Tstar), <span class="kw">abs</span>(Tobs), <span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>## [1] 0.044</code></pre>
<p>We will encourage you to think through what might constitute strong evidence
against your null hypotheses and then discuss how strong you feel the evidence
is against the null hypothesis in the p-value that you obtained. Basically,
p-values present a measure of evidence against the null hypothesis,

with smaller
values presenting more evidence against the null. They range from 0 to 1 and you
should interpret them on a graded scale from strong evidence (close to 0) to little evidence to no
evidence (1). We will discuss the use of a fixed <strong><em>significance level</em></strong>
below as it is still commonly used in many fields and is necessary to discuss to think
about the theory of hypothesis testing, but, for the moment,
we can say that there is moderate evidence against the null hypothesis
presented by having a p-value of 0.044 because our observed result is somewhat
rare relative to what we would expect if the null hypothesis was true. And so
we might conclude (in the direction
of the alternative) that there is a difference in the population means in the
two groups, but that depends on what you think about how unusual that result was. It is also reasonable to feel that this is not sufficient evidence to conclude that there is a difference in the true means even though many people feel that p-values less than 0.05 are fairly strong evidence against the null hypothesis. If you do not rate this as strong enough evidence (or in general obtain weak evidence) to conclude that there is a difference, then you can only say that there might not be a difference in the means. We can’t conclude that the null hypothesis is true – we just failed to find enough evidence to be sure that it is wrong. It might still be wrong but we couldn’t detect it, either as a mistake because of an unusual sample from our population, or because our sample size was not large enough to detect the size of difference in the populations, or results with larger p-values could happen because there really isn’t a difference. We don’t know which of these might be the truth and certainly don’t know that the null hypothesis is true even if the p-value obtained is 1<a href="#fn35" class="footnote-ref" id="fnref35"><sup>35</sup></a>.</p>
<p>Before we move on, let’s note some interesting features of the permutation
distribution of the difference in the sample means shown in
Figure <a href="chapter2.html#fig:Figure2-11">2.11</a>. </p>
<ol style="list-style-type: decimal">
<li><p>It is basically centered at 0. Since we are performing permutations assuming
the null model is true, we are assuming that <span class="math inline">\(\mu_1 = \mu_2\)</span> which implies that
<span class="math inline">\(\mu_1 - \mu_2 = 0\)</span>. This also suggests that 0 should be the center of the
permutation distribution and it was.</p></li>
<li><p>It is approximately normally distributed. This is due to the <strong><em>Central Limit Theorem</em></strong><a href="#fn36" class="footnote-ref" id="fnref36"><sup>36</sup></a>, where the
<strong><em>sampling distribution</em></strong> (distribution of all possible results for samples
of this size) of the difference in sample means (<span class="math inline">\(\bar{x}_1 - \bar{x}_2\)</span>) becomes
more normally distributed as the sample sizes increase. With 15
observations in each group, we have no guarantee to have a relatively normal looking
distribution of the difference in the sample means but with the distributions of the original observations looking somewhat normally distributed, the sampling distribution of the sample means likely will look fairly normal. This result will allow us to
use a parametric

method to approximate this sampling distribution under the null
model if some assumptions are met, as we’ll discuss below.</p></li>
<li><p>Our observed difference in the sample means (-25.933) is a fairly unusual
result relative to the rest of these results but there are some permuted data
sets that produce more extreme differences in the sample means. When the
observed differences are really large, we may not see any permuted results
that are as extreme as what we observed. When <code>pdata</code> gives you 0, the p-value

should be reported to be smaller than 0.001 (<strong>not 0!</strong>) if <em>B</em> is 1,000 since it happened
in less than 1 in 1,000 tries but does occur once – in the actual data set.</p></li>
<li><p>Since our null model is not specific about the direction of the difference,
considering a result like ours but in the other direction (25.933 cm) needs to
be included. The observed result seems to put about the same area in both tails
of the distribution but it is not exactly the same. The small difference in the
tails is a useful aspect of this approach compared to the parametric method
discussed below as it accounts for potential asymmetry in the sampling distribution.</p></li>
</ol>
<p>Earlier, we decided that the p-value provided moderate evidence against
the null hypothesis. You should use your own judgment about whether the p-value obtain is sufficiently small to conclude that you think the null hypothesis is wrong. Remembering
that the p-value is the probability

you would observe a result like you did (or more extreme), assuming the null
hypothesis is true; this tells you that the smaller the p-value is, the more
evidence you have against the null. Figure <a href="chapter2.html#fig:Figure2-12">2.12</a> provides a
diagram of some suggestions for the graded p-value interpretation that you can
use. The next section provides a more formal
review of the hypothesis testing

infrastructure, terminology, and some of
things that can happen when testing hypotheses. P-values have been (validly)

criticized for the inability of studies to be reproduced, for the bias in
publications to only include studies that have small p-values, and for the lack of
thought that often accompanies using a fixed significance level to make decisions (and only focusing on that decision). To alleviate
some of these criticisms, we recommend reporting the strength of evidence of the
result based on the p-value and also reporting and discussing the size of the
estimated results (with a measure of precision of the estimated difference). We will explore the implications of how p-values are used in scientific research in Section <a href="chapter2.html#section2-8">2.8</a>.</p>

<div class="figure" style="text-align: center"><span id="fig:Figure2-12"></span>
<img src="chapter2_files/pvalueStrengths.png" alt="Graphic suggesting potential interpretations of strength of evidence based on gradient of p-values." width="100%" />
<p class="caption">
Figure 2.12: Graphic suggesting potential interpretations of strength of evidence based on gradient of p-values.
</p>
</div>
</div>
<div id="section2-5" class="section level2">
<h2><span class="header-section-number">2.5</span> Hypothesis testing (general)</h2>
<p>In hypothesis testing

(sometimes more explicitly called “Null Hypothesis
Significance Testing” or NHST), it is formulated to answer a specific question about
a population or true parameter(s) using a statistic based on a data set.
In your previous statistics course, you (hopefully) considered one-sample
hypotheses about population means and proportions and the two-sample mean
situation we are focused on here. Hypotheses relate to trying to answer
the question about whether the population mean overtake distances between the two
groups are different, with an initial assumption of no difference.</p>
<p>NHST is much like a criminal trial with a jury where you are in the role
of a jury member. Initially, the defendant
is assumed innocent. In our situation, the true means are assumed to be
equal between the groups. Then evidence is presented and, as a juror,
you analyze it. In statistical hypothesis testing,

data are collected and
analyzed. Then you have to decide if we had “enough” evidence to reject
the initial assumption (“innocence” that is initially assumed). To make
this decision, you want to have thought about and decided on the standard of
evidence required to reject the initial assumption. In criminal cases,
“beyond a reasonable doubt” is used. Wikipedia’s definition
(<a href="https://en.wikipedia.org/wiki/Reasonable_doubt" class="uri">https://en.wikipedia.org/wiki/Reasonable_doubt</a>) suggests that this
standard is that “there can still be a doubt, but only to the extent
that it would not affect a reasonable person’s belief regarding whether
or not the defendant is guilty”. In civil trials, a lower standard called
a “preponderance of evidence” is used. Based on that defined and pre-decided
(<em>a priori</em>) measure, you decide that the defendant is guilty or not guilty.
In statistics, the standard is set by choosing a significance level, <span class="math inline">\(\alpha\)</span>, and then you compare the p-value

to it. In this approach, if the p-value is less than
<span class="math inline">\(\alpha\)</span>, we reject the null hypothesis. The choice of the significance level
is like the variation in standards of evidence between criminal and civil
trials – and in all situations everyone should know the standards required
for rejecting the initial assumption before any information is “analyzed”.
Once someone is found guilty, then there is the matter of sentencing which
is related to the impacts (“size”) of the crime. In statistics, this is
similar to the estimated size of differences and the related judgments
about whether the differences are practically important or not. If the
crime is proven beyond a reasonable doubt but it is a minor crime, then
the sentence will be small. With the same level of evidence and a more
serious crime, the sentence will be more dramatic. This latter step is more critical than the p-value as it directly relates to actions to be taken based on the research but unfortunately p-values and the related decisions get most of the attention.</p>
<p>There are some important aspects of the testing process to note that inform
how we interpret statistical hypothesis test results. When someone is found
“not guilty”, it does not mean “innocent”, it just means that there was not
enough evidence to find the person guilty “beyond a reasonable doubt”.
Not finding enough evidence to reject the null hypothesis does not imply
that the true means are equal, just that there was not enough evidence to
conclude that they were different. There are many potential reasons why we
might fail to reject the null, but the most common one is that our sample
size was too small (which is related to having too little evidence). Other
reasons include simply the variation in taking a random sample  from the
population(s). This randomness in samples and the differences in the sample
means also implies that p-values are random and can easily vary if the data set
had been slightly different. This also relates to the suggestion of using a
graded interpretation of p-values instead of the fixed <span class="math inline">\(\alpha\)</span> usage – if the p-value is an estimated quantity,
is there really any difference between p-values of 0.049 and 0.051? We
probably shouldn’t think there is a big difference in results for these two
p-values even though the standard NHST reject/fail to reject the null approach
considers these as completely different results. So where does that leave us?
Interpret the p-values

using strength of evidence against the null hypothesis,
remembering that smaller (but not really small) p-values can still be
interesting. And if you think the p-value is small enough, then you can reject
the null hypothesis and conclude that the alternative hypothesis is a better characterization of the truth – and then make sure to estimate and think about the size of the differences.</p>
<p>Throughout this material, we will continue to re-iterate the distinctions
between parameters and statistics and want you to be clear about the
distinctions between estimates based on the sample and inferences for the
population or true values of the parameters of interest. Remember that
statistics are summaries of the sample information and parameters are
characteristics of populations (which we rarely know). In the two-sample
mean situation, the sample means are always at least a little different
– that is not an interesting conclusion. What is interesting is whether
we have enough evidence to feel like we have proven that the population or true means
differ “beyond a reasonable doubt”.</p>
<p>The scope of any inferences is constrained based on whether there is a
<strong><em>random sample</em></strong> (RS) and/or <strong><em>random assignment</em></strong> (RA).   
Table <a href="chapter2.html#tab:Table2-1">2.1</a> contains the four possible combinations of these
two characteristics of a given study. Random assignment of treatment levels to
subjects allows for causal
inferences for differences that are observed – the difference in treatment
levels is said to cause differences in the mean responses. Random sampling (or
at least some sort of representative sample) allows inferences to be made to
the population of interest. If we do not have RA, then causal inferences cannot
be made. If we do not have a representative sample, then our inferences are
limited to the sampled subjects.
</p>


<table>
<caption><span id="tab:Table2-1">Table 2.1: </span> Scope of inference summary.</caption>
<colgroup>
<col width="30%" />
<col width="34%" />
<col width="34%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>Random<br />
Sampling/Random<br />
Assignment</strong></th>
<th align="left"><strong>Random Assignment (RA)<br />
– Yes (controlled
experiment)</strong></th>
<th align="left"><strong>Random Assignment (RA)<br />
– No (observational study)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Random Sampling (RS)<br />
– Yes (or some method<br />
that results in a<br />
representative sample of<br />
population of<br />
interest)</strong></td>
<td align="left">Because we have RS, we can<br />
generalize inferences to the<br />
population the RS was taken<br />
from. Because we have<br />
RA we can assume the groups<br />
were equivalent on all
aspects<br />
except for the treatment<br />
and can establish causal
inference.<br />
</td>
<td align="left">Can generalize inference to<br />
population the RS was taken<br />
from but cannot establish<br />
causal inference (no RA<br />
– cannot isolate treatment<br />
variable as only difference<br />
among groups, could be<br />
confounding variables).</td>
</tr>
<tr class="even">
<td align="left"><strong>Random Sampling (RS)<br />
– No (usually a<br />
convenience sample)</strong></td>
<td align="left">Cannot generalize inference
to<br />
the population of interest<br />
because the sample was<br />
not random and could be<br />
biased – may not be<br />
“representative” of the<br />
population of interest.<br />
Can establish causal<br />
inference due to RA
<span class="math inline">\(\rightarrow\)</span><br />
the inference from this type
of<br />
study applies only to the
sample.</td>
<td align="left">Cannot generalize inference
to<br />
the population of interest<br />
because the sample was<br />
not random and could be<br />
biased – may not be<br />
“representative” of the<br />
population of interest.<br />
Cannot establish causal<br />
inference due to lack of RA
of<br />
the treatment.</td>
</tr>
</tbody>
</table>

<p>A simple example helps to clarify how the scope of inference can change
based on the study design.  Suppose
we are interested in studying the GPA of students. If we had taken a random
sample from, say, Intermediate Statistics students in a given semester at a university, our scope of
inference would be the population of students in that semester taking that course. If we had
taken a random sample  from the entire population of students at that school, then the inferences would
be to the entire population of students in that semester. These are similar types of
problems but the two populations are very different and the group you are trying
to make conclusions about should be noted carefully in your results – it does
matter! If we did not have a representative sample, say the students could
choose to provide this information or not and some chose not to, then we can only make inferences to
volunteers. These volunteers might differ in systematic ways from the entire
population of Intermediate Statistics students (for example, they are proud of their GPA) so we cannot safely extend our inferences beyond the group that volunteered.</p>
<p>To consider the impacts of RA versus results from purely observational
studies, we need to be
comparing groups. Suppose that we are interested in differences in the mean
GPAs for different sections of Intermediate Statistics and that we take a random sample  of
students from each section and compare the results and find evidence of some
difference. In this scenario, we can conclude that there is some difference in
the population of these statistics students but we can’t say that being in different
sections caused the differences in the mean GPAs. Now suppose that we randomly
assigned every student to get extra training in one of three different
study techniques and found evidence of differences among the training methods.
We could conclude that the training methods caused the differences in these
students. These conclusions would only apply to Intermediate Statistics students at this university in this semester and could
not be generalized to a larger population of students. If we took a random
sample of Intermediate Statistics students (say only 10 from each section) and then randomly
assigned them to one of three training programs and found evidence of
differences, then we can say that the training programs caused the differences.
But we can also say that we have evidence that those differences pertain to the
population of Intermediate Statistics students in that semester at this university. This seems similar to the scenario where all
the students participated in the training programs except that by using random
sampling, only a fraction of the population needs to actually be studied to
make inferences to the entire population of interest – saving time and money.</p>
<p>A quick summary of the terminology of hypothesis testing

is useful at this
point. The <strong><em>null hypothesis</em></strong> (<span class="math inline">\(H_0\)</span>) states that there is no difference
or no relationship in the population. This is the statement of no effect or
no difference and the claim that we are trying to find evidence against in NHST. In
this chapter, <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\mu_1=\mu_2\)</span>. When doing two-group problems, you
always need to specify which group is 1 and which one is 2 because the order
does matter. The <strong><em>alternative hypothesis</em></strong> (<span class="math inline">\(H_1\)</span> or <span class="math inline">\(H_A\)</span>) states a
specific difference between parameters. This is the research hypothesis and
the claim about the population that we often hope to demonstrate is more reasonable
to conclude than the null hypothesis. In the two-group situation, we can have
<strong><em>one-sided alternatives</em></strong> <span class="math inline">\(H_A: \mu_1 &gt; \mu_2\)</span> (greater than) or
<span class="math inline">\(H_A: \mu_1 &lt; \mu_2\)</span> (less than) or, the more common, <strong><em>two-sided
alternative</em></strong> <span class="math inline">\(H_A: \mu_1 \ne \mu_2\)</span> (not equal to). We usually default to
using two-sided tests because we often do not know enough to know the
direction of a difference <em>a priori</em>, especially in more complicated
situations. The <strong><em>sampling distribution under the null</em></strong> is the
distribution of all possible values of a statistic under the assumption that
<span class="math inline">\(H_0\)</span> is true. It
is used to calculate the <strong><em>p-value</em></strong>,

the probability of obtaining a
result as extreme or more extreme (defined by the alternative) than what we observed given that the null
hypothesis is true. We will find sampling distributions

using
<strong><em>nonparametric</em></strong>

approaches (like the permutation  approach used previously)
and <strong><em>parametric</em></strong>

methods (using “named” distributions like the
<span class="math inline">\(t\)</span>, F, and <span class="math inline">\(\chi^2\)</span>).</p>
<p>Small p-values are evidence against the null hypothesis

because the observed
result is unlikely due to chance if <span class="math inline">\(H_0\)</span> is true. Large p-values provide
little to no evidence against <span class="math inline">\(H_0\)</span> but do not allow us to conclude that the null
hypothesis is correct – just that we didn’t find enough evidence to think it
was wrong. The <strong><em>level of significance</em></strong> is an <em>a priori</em> definition of
how small the p-value needs to be to provide “enough” (sufficient) evidence
against <span class="math inline">\(H_0\)</span>. This is most useful to prevent sliding the standards after
the results are found but you can interpret p-values as strength of evidence against the null hypothesis without employing the fixed significance level. If using a fixed significance level, we can compare the p-value to the level of significance to
decide if the p-value is small enough to constitute sufficient evidence to
reject the null hypothesis. We use <span class="math inline">\(\alpha\)</span> to denote the level of
significance and most typically use 0.05 which we refer to as the 5%
significance level. We can compare the p-value to this level and make a
decision, focusing our interpretation on the strength of evidence we found
based on the p-value from very strong to little to none.
If we are using the strict version of NHST, the two options for <em>decisions</em> are
to either <em>reject the null hypothesis</em>
if the p-value <span class="math inline">\(\le \alpha\)</span> or <em>fail to reject the null hypothesis</em> if the
p-value <span class="math inline">\(&gt; \alpha\)</span>. When interpreting hypothesis testing results, remember
that the p-value is a measure of how unlikely the observed outcome was,
assuming that the null hypothesis is true. It is <strong>NOT</strong> the probability of
the data or the probability of either hypothesis being true. The p-value,
simply, is a measure of evidence against the null hypothesis.</p>
<p>Although we want to use graded evidence to interpret p-values, there
is one situation where thinking about comparisons to fixed <span class="math inline">\(\alpha\)</span> levels is
useful for understanding and studying statistical hypothesis testing. The
specific definition of <span class="math inline">\(\alpha\)</span> is that it is the probability of rejecting
<span class="math inline">\(H_0\)</span> when <span class="math inline">\(H_0\)</span> is true, the probability of what is called a
<strong><em>Type I error</em></strong>.  Type I errors are also called <strong><em>false rejections</em></strong> or
<strong><em>false detections</em></strong>. In the two-group mean situation, a Type I error would
be concluding that there
is a difference in the true means between the groups when none really exists
in the population. In the courtroom setting, this is like falsely finding
someone guilty. We don’t want to do this very often, so we use small values
of the significance level, allowing us to control the rate of Type I errors
at <span class="math inline">\(\alpha\)</span>.  We also have to worry about <strong>Type II errors</strong>,  which are failing
to reject the null hypothesis when it’s false. In a courtroom, this is the same
as failing to convict a truly guilty person. This most often occurs due to a
lack of evidence that could be due to a small sample size or merely just an
unusual sample from the population. You can use the Table <a href="chapter2.html#tab:Table2-2">2.2</a>
to help you remember all the possibilities.</p>
<p>
</p>

<table>
<caption><span id="tab:Table2-2">Table 2.2: </span> Table of decisions and truth scenarios in a hypothesis testing situation. But we never know the truth in a real situation.</caption>
<colgroup>
<col width="34%" />
<col width="32%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"> </th>
<th align="left"><span class="math inline">\(\mathbf{H_0}\)</span> <strong>True</strong></th>
<th align="left"><span class="math inline">\(\mathbf{H_0}\)</span> <strong>False</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>FTR</strong> <span class="math inline">\(\mathbf{H_0}\)</span></td>
<td align="left">Correct decision</td>
<td align="left">Type II error</td>
</tr>
<tr class="even">
<td align="left"><strong>Reject</strong> <span class="math inline">\(\mathbf{H_0}\)</span></td>
<td align="left">Type I error</td>
<td align="left">Correct decision</td>
</tr>
</tbody>
</table>
<p>In comparing different procedures or in planning studies, there is an
interest in studying the rate or
probability of Type I and II errors. The probability of a Type I error was
defined previously as <span class="math inline">\(\alpha\)</span>, the significance level. The <strong><em>power</em></strong> of a
procedure is the probability of rejecting the null hypothesis when it is false.   
Power is defined as</p>
<p><span class="math display">\[\text{Power} = 1 - \text{Probability(Type II error) } = 
\text{Probability(Reject } H_0 | H_0 \text{ is false),}\]</span></p>
<p>
</p>
<p>or, in words, the probability of detecting a difference when it actually
exists. We want to use a statistical procedure that controls the Type I error
rate at the pre-specified level and has high power to detect false null
hypotheses. Increasing the sample size is one of the most commonly used
methods for increasing the power  in a given situation. Sometimes we can choose
among different procedures and use the power of the procedures to help us make
that selection. Note that there are many ways <span class="math inline">\(H_0\)</span> can be false and the power changes
based on how false the null hypothesis actually is. To make this concrete,
suppose that the true mean overtake distances differed by either 1 or 30 cm in
previous example. The chances of rejecting the null hypothesis are much larger
when the group means actually differ by 30 cm than if they differ by just 1 cm,
given the same sample size. The null hypothesis is false in both cases. Similarly, for a given difference in the true means, the larger
the sample, the higher the power  of the study to actually find evidence of a
difference in the groups. We will see this difference when we return to using the entire overtake data set instead of the sample of <span class="math inline">\(n=30\)</span> used to illustrate the permutation procedures.</p>
<p>After making a decision (was there enough evidence to reject the null
or not), we want to make the conclusions specific to the problem of interest.
If we reject <span class="math inline">\(H_0\)</span>, then we can conclude that there was sufficient evidence at
the <span class="math inline">\(\alpha\)</span>-level that the null hypothesis is wrong (and the results point in
the direction of the alternative). If we fail to reject <span class="math inline">\(H_0\)</span> (FTR <span class="math inline">\(H_0\)</span>), then
we can conclude that there was insufficient evidence at the <span class="math inline">\(\alpha\)</span>-level to say
that the null hypothesis is wrong. We are <strong>NOT</strong> saying that the null is
correct and we <strong>NEVER</strong> accept the null hypothesis. We just failed to find
enough evidence to say it’s wrong. If we find sufficient evidence to reject the
null, then we need to revisit the method of data collection and design of the
study to discuss the scope of inference.  Can we discuss causality (due to RA) and/or
make inferences to a larger group than those in the sample (due to RS)?</p>
<p>To perform a hypothesis test, there are some steps to remember to
complete to make sure you have thought through and reported all aspects of the results.</p>
<table>
<colgroup>
<col width="100%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="left"><strong>Outline of 6+ steps to perform a Hypothesis Test</strong></td>
</tr>
<tr class="even">
<td align="left">Preliminary steps:</td>
</tr>
<tr class="odd">
<td align="left">* Define research question (RQ) and consider study design - what question can the data collected address?</td>
</tr>
<tr class="even">
<td align="left">* What graphs are appropriate to visualize the data?</td>
</tr>
<tr class="odd">
<td align="left">* What model/statistic (T) is needed to address RQ?</td>
</tr>
<tr class="even">
<td align="left">1. Write the null and alternative hypotheses.</td>
</tr>
<tr class="odd">
<td align="left">2. Plot the data and assess the “Validity Conditions” for the procedure being used (discussed below).</td>
</tr>
<tr class="even">
<td align="left">3. Find the value of the appropriate test statistic and p-value for your hypotheses. </td>
</tr>
<tr class="odd">
<td align="left">4. Write a conclusion specific to the problem based on the p-value, reporting the strength of evidence  against the null hypothesis (include test statistic, its distribution under the null hypothesis, and p-value).</td>
</tr>
<tr class="even">
<td align="left">5. Report and discuss an estimate of the size of the differences, with confidence interval(s) if appropriate. </td>
</tr>
<tr class="odd">
<td align="left">6. Scope of inference discussion for results. </td>
</tr>
</tbody>
</table>
</div>
<div id="section2-6" class="section level2">
<h2><span class="header-section-number">2.6</span> Connecting randomization (nonparametric) and parametric tests</h2>
<p>In developing statistical inference techniques, we need to define the test
statistic, <span class="math inline">\(T\)</span>, that measures the quantity of interest. To compare the means of
two groups, a statistic is needed
that measures their differences. In general, for comparing two groups, the
choice is simple – a difference in the means often works well and is a
natural choice. There are other options such as tracking the ratio of means or
possibly the difference in medians. Instead of just using the difference in the
means, we also could “standardize” the difference in the means by dividing by
an appropriate quantity that reflects the variation in the difference in the
means. All of these are valid and can sometimes provide similar results - it
ends up that there are many possibilities for testing using the randomization
(nonparametric)

techniques introduced previously. Parametric

statistical
methods focus on means because the statistical theory surrounding means is
quite a bit easier (not easy, just easier) than other options. There are
just a couple of test statistics that you can use and end up with named
distributions to use for generating inferences. Randomization techniques allow
inference for other quantities (such as ratios of means or differences in medians) but our focus here will be on using
randomization for inferences on means to see the similarities with the more
traditional parametric procedures used in these situations.</p>
<p>In two-sample mean situations, instead of working just with the
difference in the means, we often calculate a test statistic that is called the
<strong><em>equal variance two-independent samples t-statistic</em></strong>. The test statistic is</p>
<p><span class="math display">\[t = \frac{\bar{x}_1 - \bar{x}_2}{s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}},\]</span></p>
<p>where <span class="math inline">\(s_1^2\)</span> and <span class="math inline">\(s_2^2\)</span> are the sample variances for the two groups, <span class="math inline">\(n_1\)</span> and
<span class="math inline">\(n_2\)</span> are the sample sizes for the two groups, and the
<strong><em>pooled sample standard deviation</em></strong>,</p>
<p><span class="math display">\[s_p = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}.\]</span></p>
<p>The <span class="math inline">\(t\)</span>-statistic keeps the important comparison between the means in the
numerator that we used before and standardizes (re-scales) that difference so
that <span class="math inline">\(t\)</span> will follow a <span class="math inline">\(t\)</span>-distribution

(a parametric

“named” distribution) if
certain assumptions are met. But first we should see if standardizing the
difference in the means had an impact on our permutation test

results. It ends up that, while not too obvious, the <code>summary</code> of the <code>lm</code> we fit earlier contains this test statistic<a href="#fn37" class="footnote-ref" id="fnref37"><sup>37</sup></a>. Instead
of using the second model coefficient that estimates the difference in the means of the groups, we will extract the test statistic from the table of summary output that is in the <code>coef</code> object in the summary – using <code>$</code> to reference the <code>coef</code> information only. In the <code>coef</code> object in the summary, results related to the <code>ConditionCommute</code> are again useful for the comparison of two groups.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="chapter2.html#cb100-1"></a><span class="kw">summary</span>(lm1)<span class="op">$</span>coef</span></code></pre></div>
<pre><code>##                   Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)      135.80000   8.862996 15.322133 3.832161e-15
## Conditioncommute -25.93333  12.534169 -2.069011 4.788928e-02</code></pre>
<p>The first column of numbers contains the estimated difference in the sample means (-25.933 here) that was used before. The next column is the <code>Std. Error</code> column that contains the standard error (SE) of the estimated difference in the means, which is <span class="math inline">\(s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}\)</span> and also the denominator used to form the <span class="math inline">\(t\)</span>-test statistic (12.53 here). It will be a common theme in this material to take the ratio of the estimate (-25.933) to its SE (12.53) to generate test statistics, which provides -2.07 – this is the “standardized” estimate of the difference in the means. It is also a test statistic (<span class="math inline">\(T\)</span>) that we can use in a permutation test. This value is in the second row and third column of <code>summary(lm1)$coef</code> and to extract it the bracket notation is again employed. Specifically we want to extract <code>summary(lm1)$coef[2,3]</code> and using it and its permuted data equivalents to calculate a p-value. Since we are doing a two-sided
test, the code resembles the permutation test code in Section <a href="chapter2.html#section2-4">2.4</a>
with the new <span class="math inline">\(t\)</span>-statistic replacing the difference in the sample means that we
used before.</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="chapter2.html#cb102-1"></a>Tobs &lt;-<span class="st"> </span><span class="kw">summary</span>(lm1)<span class="op">$</span>coef[<span class="dv">2</span>,<span class="dv">3</span>]</span>
<span id="cb102-2"><a href="chapter2.html#cb102-2"></a>Tobs</span></code></pre></div>
<pre><code>## [1] -2.069011</code></pre>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="chapter2.html#cb104-1"></a>B &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb104-2"><a href="chapter2.html#cb104-2"></a><span class="kw">set.seed</span>(<span class="dv">406</span>)</span>
<span id="cb104-3"><a href="chapter2.html#cb104-3"></a>Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</span>
<span id="cb104-4"><a href="chapter2.html#cb104-4"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</span>
<span id="cb104-5"><a href="chapter2.html#cb104-5"></a>  lmP &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span><span class="kw">shuffle</span>(Condition), <span class="dt">data=</span>dsample)</span>
<span id="cb104-6"><a href="chapter2.html#cb104-6"></a>  Tstar[b] &lt;-<span class="st"> </span><span class="kw">summary</span>(lmP)<span class="op">$</span>coef[<span class="dv">2</span>,<span class="dv">3</span>]</span>
<span id="cb104-7"><a href="chapter2.html#cb104-7"></a>}</span>
<span id="cb104-8"><a href="chapter2.html#cb104-8"></a><span class="kw">pdata</span>(<span class="kw">abs</span>(Tstar), <span class="kw">abs</span>(Tobs), <span class="dt">lower.tail=</span>F)</span></code></pre></div>
<pre><code>## [1] 0.041</code></pre>
<p>The permutation distribution

in Figure <a href="chapter2.html#fig:Figure2-13">2.13</a> looks
similar to the previous results with slightly different <span class="math inline">\(x\)</span>-axis scaling. The
observed <span class="math inline">\(t\)</span>-statistic was <span class="math inline">\(-2.07\)</span> and the proportion of permuted results that
were as or more extreme than the observed result
was 0.041. This difference is due to a different set of random permutations
being selected. If you run permutation code, you will often get slightly
different results each time you run it. If you are uncomfortable with the
variation in the results, you can run more than <em>B</em> = 1,000 permutations (say
10,000) and the variability in the resulting p-values will be reduced further.
Usually this uncertainty will not cause any substantive problems – but do not
be surprised if your results vary if you use different random number seeds.</p>

<div class="figure"><span id="fig:Figure2-13"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-13-1.png" alt="Permutation distribution of the \(t\)-statistic." width="960" />
<p class="caption">
Figure 2.13: Permutation distribution of the <span class="math inline">\(t\)</span>-statistic.
</p>
</div>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="chapter2.html#cb106-1"></a><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)</span>
<span id="cb106-2"><a href="chapter2.html#cb106-2"></a><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span>
<span id="cb106-3"><a href="chapter2.html#cb106-3"></a><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</span>
<span id="cb106-4"><a href="chapter2.html#cb106-4"></a><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p>The parametric version

of these results is based on using what is called
the <strong><em>two-independent sample t-test</em></strong>. There are actually two versions of this
test, one that assumes that variances are equal in the groups and one that
does not. There is a rule of thumb that if the <strong>ratio of the larger standard
deviation over the smaller standard deviation is less than 2, the equal variance
procedure is OK</strong>. It ends up that this assumption
is less important if the sample sizes in the groups are approximately equal
and more important if the groups contain different numbers of observations. In
comparing the two potential test statistics, the procedure that assumes equal
variances has a complicated denominator (see the formula above for <span class="math inline">\(t\)</span>
involving <span class="math inline">\(s_p\)</span>) but a simple formula for <strong><em>degrees of freedom</em></strong> (<strong><em>df</em></strong>)


for the <span class="math inline">\(t\)</span>-distribution (<span class="math inline">\(df=n_1+n_2-2\)</span>) that approximates the
distribution of the test statistic, <span class="math inline">\(t\)</span>, under the null hypothesis. The
procedure that assumes unequal variances has a simpler test statistic and a
very complicated degrees of freedom formula. The equal variance procedure is
equivalent to the methods we will consider in Chapters
<a href="chapter3.html#chapter3">3</a> and <a href="chapter4.html#chapter4">4</a> so that
will be our focus for the two group problem and is what we get when using the <code>lm</code> model to estimate the differences in the group means. The unequal variance version of the two-sample t-test is available in the <code>t.test</code> function if needed.</p>

<div class="figure"><span id="fig:Figure2-14"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-14-1.png" alt="Plots of \(t\)-distributions with 2, 10, and 20 degrees of freedom and a normal distribution (dashed line). Note how the \(t\)-distributions get closer to the normal distribution as the degrees of freedom increase and at 20 degrees of freedom, the \(t\)-distribution almost matches a standard normal curve." width="672" />
<p class="caption">
Figure 2.14: Plots of <span class="math inline">\(t\)</span>-distributions with 2, 10, and 20 degrees of freedom and a normal distribution (dashed line). Note how the <span class="math inline">\(t\)</span>-distributions get closer to the normal distribution as the degrees of freedom increase and at 20 degrees of freedom, the <span class="math inline">\(t\)</span>-distribution <em>almost</em> matches a standard normal curve.
</p>
</div>
<p>If the assumptions for the equal variance <span class="math inline">\(t\)</span>-test and the null
hypothesis are true, then the sampling distribution of the test statistic should
follow a <span class="math inline">\(t\)</span>-distribution

with <span class="math inline">\(n_1+n_2-2\)</span> degrees of freedom (so the total sample size, <span class="math inline">\(n\)</span>, minus 2). The <strong><em>t-distribution</em></strong> is a bell-shaped curve that is more spread out for smaller
values of degrees of freedom as shown in Figure <a href="chapter2.html#fig:Figure2-14">2.14</a>. The
<span class="math inline">\(t\)</span>-distribution looks more and more like a <strong><em>standard normal distribution</em></strong>

(<span class="math inline">\(N(0,1)\)</span>) as the degrees of freedom increase.</p>
<p>To get the p-value for the parametric <span class="math inline">\(t\)</span>-test,

we need to calculate the
test statistic and <span class="math inline">\(df\)</span>, then look up the areas in the tails of the
<span class="math inline">\(t\)</span>-distribution

relative to the observed <span class="math inline">\(t\)</span>-statistic. We’ll learn how to use
R to do this below, but for now we will allow the <code>summary</code> of the <code>lm</code> function to take
care of this. In the <code>ConditionCommute</code> row of the summary and the <code>Pr(&gt;|t|)</code> column, we can find the p-value associated with the test statistic. We can either calculate the degrees of freedom for the <span class="math inline">\(t\)</span>-distribution using <span class="math inline">\(n_1+n_2-2 = 15+15-2 = 28\)</span> or explore the full suite of the model summary that is repeated below. In the first row below the <code>ConditionCommute</code> row, it reports “… 28 degrees of freedom” and these are the same <span class="math inline">\(df\)</span> that are needed to report and look up for any of the <span class="math inline">\(t\)</span>-statistics in the model summary.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="chapter2.html#cb107-1"></a><span class="kw">summary</span>(lm1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Distance ~ Condition, data = dsample)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -63.800 -21.850   4.133  15.150  72.200 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)       135.800      8.863  15.322 3.83e-15
## Conditioncommute  -25.933     12.534  -2.069   0.0479
## 
## Residual standard error: 34.33 on 28 degrees of freedom
## Multiple R-squared:  0.1326, Adjusted R-squared:  0.1016 
## F-statistic: 4.281 on 1 and 28 DF,  p-value: 0.04789</code></pre>
<p>So the parametric <span class="math inline">\(t\)</span>-test gives a p-value of 0.0479 from a test statistic of
-2.07. The p-value is very similar to the two permutation results found before. The
reason for this similarity is that the permutation distribution looks like a <span class="math inline">\(t\)</span>-distribution with 28 degrees
of freedom. Figure <a href="chapter2.html#fig:Figure2-15">2.15</a> shows how similar the two distributions
happened to be here, where the only difference in shape is near the peak of the distributions with a slight difference of the permutation distribution to shift to the right.</p>

<div class="figure"><span id="fig:Figure2-15"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-15-1.png" alt="Plot of permutation and \(t\)-distribution with \(df=28\). Note the close match in the two distributions, especially in the tails of the distributions where we are obtaining the p-values." width="576" />
<p class="caption">
Figure 2.15: Plot of permutation and <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df=28\)</span>. Note the close match in the two distributions, especially in the tails of the distributions where we are obtaining the p-values.
</p>
</div>
<p>In your previous statistics course, you might have used an applet or
a table to find p-values such as what was provided in the previous R output.
When not directly provided in the output of a function, R can be used to look up
p-values<a href="#fn38" class="footnote-ref" id="fnref38"><sup>38</sup></a> from
named distributions such as the <span class="math inline">\(t\)</span>-distribution. In this case, the distribution
of the test statistic under the null hypothesis is a <span class="math inline">\(t(28)\)</span> or a <span class="math inline">\(t\)</span> with 28
degrees of freedom. The <code>pt</code> function is used to get p-values from the

<span class="math inline">\(t\)</span>-distribution in the same manner that <code>pdata</code> could help us to find p-values
from the permutation distribution.

We need to provide the <code>df=...</code> and specify
the tail of the distribution of interest using the <code>lower.tail</code> option along
with the cutoff of interest. If we want the area to the left of -2.07:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="chapter2.html#cb109-1"></a><span class="kw">pt</span>(<span class="op">-</span><span class="fl">2.069</span>, <span class="dt">df=</span><span class="dv">28</span>, <span class="dt">lower.tail=</span>T)</span></code></pre></div>
<pre><code>## [1] 0.02394519</code></pre>
<p>And we can double it to get the p-value that was in the output, because
the <span class="math inline">\(t\)</span>-distribution is symmetric:
</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="chapter2.html#cb111-1"></a><span class="dv">2</span><span class="op">*</span><span class="kw">pt</span>(<span class="op">-</span><span class="fl">2.069</span>, <span class="dt">df=</span><span class="dv">28</span>, <span class="dt">lower.tail=</span>T)</span></code></pre></div>
<pre><code>## [1] 0.04789038</code></pre>
<p>More generally, we could always make the test statistic positive using the
absolute value (<code>abs</code>), find the area to the right of it (<code>lower.tail=F</code>), and then double that for a
two-sided test p-value:</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="chapter2.html#cb113-1"></a><span class="dv">2</span><span class="op">*</span><span class="kw">pt</span>(<span class="kw">abs</span>(<span class="op">-</span><span class="fl">2.069</span>), <span class="dt">df=</span><span class="dv">28</span>, <span class="dt">lower.tail=</span>F)</span></code></pre></div>
<pre><code>## [1] 0.04789038</code></pre>
<p>Permutation distributions  do not need to match the named
parametric distribution

to work correctly, although this happened in the previous example.
The parametric approach, the <span class="math inline">\(t\)</span>-test, requires certain conditions to be true (or at least not be clearly violated)
for the sampling distribution of the
statistic to follow the named distribution and provide accurate p-values. The
conditions for the <span class="math inline">\(t\)</span>-test are:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Independent observations</strong>:

Each observation obtained is unrelated to all other
observations. To assess this, consider whether anything in the data collection
might lead to clustered or related observations that are un-related to the
differences in the groups. For example, was the same person measured more than
once<a href="#fn39" class="footnote-ref" id="fnref39"><sup>39</sup></a>?</p></li>
<li><p><strong>Equal variances</strong> in the groups (because we used a procedure that assumes
equal variances! – there is another procedure that allows you to relax this
assumption if needed…). To assess this, compare the standard deviations and
variability in the pirate-plots  and see if they look noticeably different. Be
particularly critical of this assessment if the sample sizes differ greatly
between groups.</p></li>
<li><p><strong>Normal distributions</strong> of the observations in each group. We’ll learn more
diagnostics later, but the pirate-plots are a good place to start to
help you look for potential skew or outliers. If you find
skew and/or outliers, that would suggest a problem with the assumption of
normality as normal distributions
  
are symmetric and extreme observations occur
very rarely.</p></li>
</ol>
<p>For the permutation test,  we relax the third condition and replace it with:</p>
<ol start="3" style="list-style-type: decimal">
<li><strong><em>Similar distributions for the groups:</em></strong> The permutation approach
allows valid inferences as long as the two groups have similar shapes and only
possibly differ in their centers. In other words, the distributions need not
look normal for the procedure to work well, but they do need to look similar.
</li>
</ol>
<p>In the bicycle overtake study, the independent
observation condition is violated because of multiple measurements taken on the same ride. The fact that the same rider was used for all observations is not really a violation of independence here because there was only one subject used. If multiple subjects had been used, then that also could present a violation of the independence assumption. This violation is important to note as the inferences may not be correct due to the violation of this assumption and more sophisticated statistical methods would be needed to complete this analysis correctly. The equal variance condition does not appear to be violated. The standard deviations are 28.4 vs 39.4, so this difference is not “large”
according to the rule of thumb noted above (ratio of SDs is about 1.4). There is also little evidence in the pirate-plots to suggest a violation of the normality condition for each of the groups (Figure <a href="chapter2.html#fig:Figure2-6">2.6</a>). Additionally, the shapes look similar for the two groups so we also could feel comfortable using the permutation approach based on
its version of condition (3) above. Note that when assessing assumptions, it is important to never state that assumptions are met – we never know the truth and can only look at the information in the sample to look for evidence of problems with particular conditions. Violations of those conditions suggest a need for either more sophisticated statistical tools<a href="#fn40" class="footnote-ref" id="fnref40"><sup>40</sup></a> or possibly transformations of the response variable (discussed in Chapter <a href="chapter7.html#chapter7">7</a>).</p>
<p>The permutation approach is resistant

to impacts of violations of the
normality assumption. It is not resistant to impacts of violations of any of
the other assumptions.

In fact, it can be quite sensitive to unequal variances
as it will detect differences in the variances of the groups instead of
differences in the means. Its scope of inference is the same as the parametric
approach.  It also provides similarly inaccurate conclusions in the presence
of non-independent observations as for the parametric approach. In this
example, we discover that parametric and permutation approaches provide very
similar inferences, but both are subject to concerns related to violations of the independent observations condition. And we haven’t directly addressed the size and direction of the differences, which is addressed in the coming discussion of confidence intervals.</p>
<p>For comparison, we can also explore the original data set of all <span class="math inline">\(n=1,636\)</span> observations for the two outfits. The estimated difference in the means is -3.003 cm (<em>commute</em> minus <em>casual</em>), the standard error is 1.472, the <span class="math inline">\(t\)</span>-statistic is -2.039 and using a <span class="math inline">\(t\)</span>-distribution with 1634 <span class="math inline">\(df\)</span>, the p-value is 0.0416. The estimated difference in the means is much smaller but the p-value is similar to the results for the sub-sample we analyzed. The SE is much smaller with the large sample size which corresponds to having higher power to detect smaller differences.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="chapter2.html#cb115-1"></a>lm_all &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>ddsub)</span>
<span id="cb115-2"><a href="chapter2.html#cb115-2"></a><span class="kw">summary</span>(lm_all)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Distance ~ Condition, data = ddsub)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -106.608  -17.608    0.389   16.392  127.389 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)       117.611      1.066 110.357   &lt;2e-16
## Conditioncommute   -3.003      1.472  -2.039   0.0416
## 
## Residual standard error: 29.75 on 1634 degrees of freedom
## Multiple R-squared:  0.002539,   Adjusted R-squared:  0.001929 
## F-statistic:  4.16 on 1 and 1634 DF,  p-value: 0.04156</code></pre>
<p>The permutations take a little more computing power with almost two thousand observations to shuffle, but this is manageable on a modern laptop as it only has to be completed once to fill in the distribution of the test statistic under 1,000 shuffles. And the p-value obtained is a close match to the parametric result at 0.045 for the permutation version and 0.042 for the parametric approach. So we would get similar inferences for strength of evidence against the null with either the smaller data set or the full data set but the estimated size of the differences is quite a bit different. It is important to note that other random samples from the larger data set would give different p-values and this one happened to match the larger set more closely than one might expect in general.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="chapter2.html#cb117-1"></a>Tobs &lt;-<span class="st"> </span><span class="kw">summary</span>(lm_all)<span class="op">$</span>coef[<span class="dv">2</span>,<span class="dv">3</span>]</span>
<span id="cb117-2"><a href="chapter2.html#cb117-2"></a>Tobs</span></code></pre></div>
<pre><code>## [1] -2.039491</code></pre>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="chapter2.html#cb119-1"></a>B &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb119-2"><a href="chapter2.html#cb119-2"></a><span class="kw">set.seed</span>(<span class="dv">406</span>)</span>
<span id="cb119-3"><a href="chapter2.html#cb119-3"></a>Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</span>
<span id="cb119-4"><a href="chapter2.html#cb119-4"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</span>
<span id="cb119-5"><a href="chapter2.html#cb119-5"></a>  lmP &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span><span class="kw">shuffle</span>(Condition), <span class="dt">data=</span>ddsub)</span>
<span id="cb119-6"><a href="chapter2.html#cb119-6"></a>  Tstar[b] &lt;-<span class="st"> </span><span class="kw">summary</span>(lmP)<span class="op">$</span>coef[<span class="dv">2</span>,<span class="dv">3</span>]</span>
<span id="cb119-7"><a href="chapter2.html#cb119-7"></a>}</span>
<span id="cb119-8"><a href="chapter2.html#cb119-8"></a><span class="kw">pdata</span>(<span class="kw">abs</span>(Tstar), <span class="kw">abs</span>(Tobs), <span class="dt">lower.tail=</span>F)</span></code></pre></div>
<pre><code>## [1] 0.045</code></pre>

<div class="figure"><span id="fig:Figure2-16"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-16-1.png" alt="Permutation distribution of the \(t\)-statistic for \(n=1,636\) overtake data set." width="960" />
<p class="caption">
Figure 2.16: Permutation distribution of the <span class="math inline">\(t\)</span>-statistic for <span class="math inline">\(n=1,636\)</span> overtake data set.
</p>
</div>
<div style="page-break-after: always;"></div>
</div>
<div id="section2-7" class="section level2">
<h2><span class="header-section-number">2.7</span> Second example of permutation tests</h2>
<p>In every chapter, the first example, used to motivate and explain
the methods, is followed with a “worked” example where we focus just on the
results. In a
previous semester, some of the Intermediate Statistics students at Montana State University (<strong><em>n</em></strong>=79) provided
information on their <em>Sex</em><a href="#fn41" class="footnote-ref" id="fnref41"><sup>41</sup></a>, <em>Age</em>, and current cumulative <em>GPA</em>. We might be
interested in whether Males and Females had different average GPAs. First,
we can take a look at the difference in the responses by groups based on the
output and as displayed in Figure <a href="chapter2.html#fig:Figure2-17">2.17</a>.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="chapter2.html#cb121-1"></a>s217 &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/s217.csv&quot;</span>)</span>
<span id="cb121-2"><a href="chapter2.html#cb121-2"></a><span class="kw">library</span>(mosaic)</span>
<span id="cb121-3"><a href="chapter2.html#cb121-3"></a><span class="kw">library</span>(yarrr)</span></code></pre></div>
<!-- \newpage -->
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="chapter2.html#cb122-1"></a><span class="kw">mean</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217)</span></code></pre></div>
<pre><code>##        F        M 
## 3.338378 3.088571</code></pre>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="chapter2.html#cb124-1"></a><span class="kw">favstats</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217)</span></code></pre></div>
<pre><code>##   Sex  min  Q1 median   Q3 max     mean        sd  n missing
## 1   F 2.50 3.1  3.400 3.70   4 3.338378 0.4074549 37       0
## 2   M 1.96 2.8  3.175 3.46   4 3.088571 0.4151789 42       0</code></pre>

<div class="figure"><span id="fig:Figure2-17"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-17-1.png" alt="Side-by-side boxplot and pirate-plot of GPAs of Intermediate Statistics students by gender." width="480" />
<p class="caption">
Figure 2.17: Side-by-side boxplot and pirate-plot of GPAs of Intermediate Statistics students by gender.
</p>
</div>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="chapter2.html#cb126-1"></a><span class="kw">boxplot</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217)</span>
<span id="cb126-2"><a href="chapter2.html#cb126-2"></a><span class="kw">pirateplot</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217, <span class="dt">inf.method=</span><span class="st">&quot;ci&quot;</span>, <span class="dt">inf.disp=</span><span class="st">&quot;line&quot;</span>)</span></code></pre></div>
<p>In these data, the distributions of the GPAs look to be left skewed. The
Female GPAs look to be slightly higher than for Males (0.25 GPA difference in the
means) but is that a “real” difference? We need our inference tools to more fully
assess these differences.</p>
<p>First, we can try the parametric approach:</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="chapter2.html#cb127-1"></a>lm_GPA &lt;-<span class="st"> </span><span class="kw">lm</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217)</span>
<span id="cb127-2"><a href="chapter2.html#cb127-2"></a><span class="kw">summary</span>(lm_GPA)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = GPA ~ Sex, data = s217)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.12857 -0.28857  0.06162  0.36162  0.91143 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  3.33838    0.06766  49.337  &lt; 2e-16
## SexM        -0.24981    0.09280  -2.692  0.00871
## 
## Residual standard error: 0.4116 on 77 degrees of freedom
## Multiple R-squared:  0.08601,    Adjusted R-squared:  0.07414 
## F-statistic: 7.246 on 1 and 77 DF,  p-value: 0.008713</code></pre>
<p>So the test statistic was observed to be <span class="math inline">\(t=2.69\)</span> and it hopefully
follows a <span class="math inline">\(t(77)\)</span> distribution under the null hypothesis. This provides a
p-value of 0.008713 that we can trust if the conditions to use this
procedure are at least not clearly violated.
Compare these results to the permutation approach, which relaxes that normality
assumption, with the results that follow. In the permutation test, <span class="math inline">\(T=-2.692\)</span> and
the p-value is 0.011 which is a little larger than the result provided
by the parametric approach. The general agreement of the two approaches, again, provides
some re-assurance about the use of either approach when there are not dramatic violations of validity conditions. </p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="chapter2.html#cb129-1"></a>B=<span class="dv">1000</span></span>
<span id="cb129-2"><a href="chapter2.html#cb129-2"></a>Tobs &lt;-<span class="st"> </span><span class="kw">summary</span>(lm_GPA)<span class="op">$</span>coef[<span class="dv">2</span>,<span class="dv">3</span>]</span>
<span id="cb129-3"><a href="chapter2.html#cb129-3"></a>Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</span>
<span id="cb129-4"><a href="chapter2.html#cb129-4"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</span>
<span id="cb129-5"><a href="chapter2.html#cb129-5"></a>  lmP &lt;-<span class="st"> </span><span class="kw">lm</span>(GPA<span class="op">~</span><span class="kw">shuffle</span>(Sex), <span class="dt">data=</span>s217)</span>
<span id="cb129-6"><a href="chapter2.html#cb129-6"></a>  Tstar[b] &lt;-<span class="st"> </span><span class="kw">summary</span>(lmP)<span class="op">$</span>coef[<span class="dv">2</span>,<span class="dv">3</span>]</span>
<span id="cb129-7"><a href="chapter2.html#cb129-7"></a>}</span>
<span id="cb129-8"><a href="chapter2.html#cb129-8"></a><span class="kw">pdata</span>(<span class="kw">abs</span>(Tstar),<span class="kw">abs</span>(Tobs),<span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</span></code></pre></div>

<div class="figure"><span id="fig:Figure2-18"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-18-1.png" alt="Histogram and density curve of permutation distribution of test statistic for Intermediate Statistics student GPAs." width="960" />
<p class="caption">
Figure 2.18: Histogram and density curve of permutation distribution of test statistic for Intermediate Statistics student GPAs.
</p>
</div>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="chapter2.html#cb130-1"></a><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)</span>
<span id="cb130-2"><a href="chapter2.html#cb130-2"></a><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span>
<span id="cb130-3"><a href="chapter2.html#cb130-3"></a><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</span>
<span id="cb130-4"><a href="chapter2.html#cb130-4"></a><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p>Here is a full write-up of the results using all 6+ hypothesis testing

steps,
using the permutation  results for the grade data:</p>
<ol start="0" style="list-style-type: decimal">
<li><p>The research question involves exploring differences in GPAs between males and females. With data collected from both groups, we should be able to assess this RQ. The pirate-plot with GPAs by gender is a useful visualization. We could use either differences in the sample means or the <span class="math inline">\(t\)</span>-statistic for the test statistic here.</p></li>
<li><p>Write the null and alternative hypotheses:</p>
<ul>
<li><p><span class="math inline">\(H_0: \mu_\text{male} = \mu_\text{female}\)</span></p>
<ul>
<li>where <span class="math inline">\(\mu_\text{male}\)</span> is the true mean GPA for males and
<span class="math inline">\(\mu_\text{female}\)</span> is true mean GPA for females.</li>
</ul></li>
<li><p><span class="math inline">\(H_A: \mu_\text{male} \ne \mu_\text{female}\)</span></p></li>
</ul></li>
<li><p>Plot the data and assess the “Validity Conditions” for the procedure being used:</p>
<ul>
<li><p><strong>Independent observations condition</strong>: It does not appear that this assumption
is violated because there is no reason to assume any clustering or grouping of
responses that might create dependence in the observations. The only
possible consideration is that the observations were taken from different
sections and there could be some differences among the sections. However,
for overall GPA there is not too much likelihood that the overall GPAs
would vary greatly so this not likely to be a big issue. However, it is possible that certain sections (times of day) attract
students with different GPA levels.</p></li>
<li><p><strong>Equal variance condition</strong>: There is a small difference in the range
of the observations in the two groups but the standard deviations are very
similar (close to 0.41) so there is little evidence that this condition is violated.</p></li>
<li><p><strong>Similar distribution condition</strong>: Based on the side-by-side boxplots and
pirate-plots, it appears that both groups have slightly left-skewed
distributions, which could be problematic for the parametric approach. The two distributions are not exactly alike but
they are similar enough that the permutation approach condition is not clearly violated.</p></li>
</ul></li>
<li><p>Find the value of the appropriate test statistic and p-value for your hypotheses:</p>
<ul>
<li><p><span class="math inline">\(T=-2.69\)</span> from the previous R output.</p></li>
<li><p>p-value <span class="math inline">\(=\)</span> 0.011 from the permutation distribution results.</p></li>
<li><p>This means that there is about a 1.1% chance we would observe
a difference in mean GPA (female-male or male-female) of 0.25 points or more
if there in fact is no difference in true mean GPA between females and males
in Intermediate Statistics in a particular semester.</p></li>
</ul></li>
<li><p>Write a conclusion specific to the problem based on the p-value:</p>
<ul>
<li>There is strong evidence against the null hypothesis of no difference
in the true mean GPA between males and females for the Intermediate Statistics students
in this semester and so we conclude that there is a difference
in the mean GPAs between males and females in these students.</li>
</ul></li>
<li><p>Report and discuss an estimate of the size of the differences, with confidence interval(s) if appropriate. </p>
<ul>
<li>Females were estimated to have a higher mean GPA by 0.25 points. The next
section discusses confidence intervals that we could add to this result to
quantify the uncertainty in this estimate since an estimate without any idea of its precision is only a partial result. This difference of 0.25 on a GPA scale does not seem like a very large difference in the means even though we were able to detect a difference in the groups.</li>
</ul></li>
<li><p>Scope of inference: </p>
<ul>
<li>Because this was not a randomized experiment  in our explanatory variable, we can’t say that the
difference in gender causes the difference in mean GPA. Because it was
not a random sample from a larger population (they were asked to participate but not required to and not all the students did participate), our inferences only pertain
the Intermediate Statistics students that responded to the survey in that semester. </li>
</ul></li>
</ol>

</div>
<div id="section2-8" class="section level2">
<h2><span class="header-section-number">2.8</span> Reproducibility Crisis: Moving beyond p &lt; 0.05, publication bias, and multiple testing issues</h2>

<p>In the previous examples, some variation in p-values was observed as different methods (parametric, nonparametric) were applied to the same data set and in the permutation approach, the p-values can vary as well from one set of permutations to another. P-values also vary based on randomness in the data that were collected – take a different (random) sample and you will get different data and a different p-value. We want the best estimate of a p-value we can obtain, so should use the best sampling method and inference technique that we can. But it is just an estimate of the evidence against the null hypothesis. These sources of variability make fixed <span class="math inline">\(\alpha\)</span> NHST especially worry-some as sampling variability could take a p-value from just below to just above <span class="math inline">\(\alpha\)</span> and this would lead to completely different inferences if the only focus is on rejecting the null hypothesis at a fixed significance level. But viewing p-values on a gradient from extremely strong (close to 0) to no (1) evidence against the null hypothesis, p-values of, say, 0.046 and 0.054 provide basically the same evidence against the null hypothesis. The fixed <span class="math inline">\(\alpha\)</span> decision-making is tied into the use of the terminology of “significant results” or, slightly better, “statistically significant results”  that are intended to convey that there was sufficient evidence to reject the null hypothesis at some pre-decided <span class="math inline">\(\alpha\)</span> level. You will notice that this is the only time that the “s-word” (significant) is considered here.</p>
<p></p>
<p>The focus on p-values has been criticized for a suite of reasons <span class="citation">(Wasserstein and Lazar <a href="#ref-Wasserstein2016" role="doc-biblioref">2016</a>)</span>. There are situations when p-values do not address the question of interest or the fact that a small p-value was obtained is so un-surprising that one wonders why it was even reported. For example, in Smith <span class="citation">(Smith <a href="#ref-Smith2014" role="doc-biblioref">2014</a>)</span> the researcher considered bee sting pain ratings across 27 different body locations<a href="#fn42" class="footnote-ref" id="fnref42"><sup>42</sup></a>. I don’t think anyone would be surprised to learn that there was strong evidence against the null hypothesis of no difference in the true mean pain ratings across different body locations. What is really of interest are the differences in the means – especially which locations are most painful and how much more painful those locations were than others, on average.</p>
<p>As a field, Statistics is trying to encourage a move away from the focus on p-values and the use of the term “significant”, even when modified by “statistically”. There are a variety of reasons for this change. Science (especially in research going into academic journals and in some introductory statistics books) has taken to using p-value &lt; 0.05 and rejected null hypotheses as the only way to “certify” that a result is interesting. It has (and unfortunately still is) hard to publish a paper with a primary result with a p-value that is higher than 0.05, even if the p-value is close to that “magical” threshold. One thing that is lost when using that strict cut-off for decisions is that any p-value that is not exactly 1 suggests that there is at least some evidence against the null hypothesis in the data and that evidence is then on a continuum from none to very strong. And that p-values are both a function of the size of the difference and the sample size. It is easy to get small p-values for small size differences with large data sets. A small p-value can be associated with an unimportant (not practically meaningful) size difference. And large p-values, especially in smaller sample situations, could be associated with very meaningful differences in size even though evidence is not strong against the null hypothesis. It is critical to always try to estimate and discuss the size of the differences, whether a large or small p-value is encountered.</p>
<p>There are some other related issues to consider in working with p-values that help to illustrate some of the issues with how p-values and “statistical significance” are used in practice. In many studies, researchers have a suite of outcome variables that they measure on their subjects. For example, in an agricultural experiment they might measure the yield of the crops, the protein concentration, the digestibility, and other characteristics of the crops. In various “omics” fields such as genomics, proteomics, and metabolomics, responses for each subject on hundreds, thousands, or even millions of variables are considered and a p-value may be generated for each of those variables. In education, researchers might be interested in impacts on grades (as in the previous discussion) but we could also be interested in reading comprehension, student interest in the subject, and the amount of time spent studying, each as response variables in their own right. In each of these situations it means that we are considering not just one null hypothesis and assessing evidence against it, but are doing it many times, from just a few to millions of repetitions. There are two aspects of this process and implications for research to explore further: the impacts on scientific research of focusing solely on “statistically significant” results and the impacts of considering more than one hypothesis test in the same study.</p>
<p>There is the systematic bias in scientific research that has emerged from scientists having a difficult time publishing research if p-values for their data are not smaller than 0.05. This has two implications. Many researchers have assumed that results with “large” p-values are not interesting – so they either exclude these results from papers (they put them in <em>their</em> file drawer instead of into their papers - the so-called “file-drawer” bias)  or reviewers reject papers because they did not have small p-values to support their discussions (only results with small p-values are judged as being of interest for publication - the so-called “publication bias”).  Some also include bias from researchers only choosing to move forward with attempting to publish results if they are in the same direction that the researchers expect/theorized as part of this problem – ignoring results that contradict their theories is an example of “confirmation bias”  but also would hinder the evolution of scientific theories to ignore contradictory results. But since most researchers focus on p-values and not on estimates of size (and direction) of differences, that will be our focus here.</p>
<p>We will use some of our new abilities in R to begin to study some of the impacts of systematically favoring only results with small p-values using a “simulation study” inspired by the explorations in <span class="citation">Schneck (<a href="#ref-Schneck2017" role="doc-biblioref">2017</a>)</span>.  Specifically, let’s focus on the bicycle passing data. We start with assuming that there really is no difference in the two groups, so the true mean is the same in both groups, the variability is the same around the means in the two groups, and all responses follow normal distributions. This is basically like the permutation idea where we assumed the group labels could be equivalently swapped among responses if the null hypothesis were true except that observations will be generated by a normal distribution instead of shuffling the original observations among groups. This is a little stronger assumption than in the permutation approach but makes it possible to study Type I error rates, power, and to explore a process that is similar to how statistical results are generated and used in academic research settings.</p>
<p>Now let’s suppose that we are interested in what happens when we do ten independent studies of the same research question. You could think of this as ten different researchers conducting their own studies of the same topic (say passing distance) or ten times the same researchers did the the same study or (less obviously) a researcher focusing on ten different response variables in the same study. Now suppose that one of two things happens with these ten unique response variables – we just report one of them (any could be used, but suppose the first one is selected) OR we only report the one of the ten with the smallest p-value. This would correspond to reporting the results of <em>a</em> study or to reporting the “most significant” of ten tries at the same study – either because nine researchers decided not to publish/ got their papers rejected by journals or because one researcher put the other nine results into their drawer of “failed studies” and never even tried to report the results.</p>
<p>The following code generates one realization of this process to explore both the p-values that are created and the estimated differences. To simulate new observations with the null hypothesis true, there are two new ideas to consider. First, we need to fit a model that makes the means the same in both groups. This is called the “mean-only” model  and is implemented with <code>lm(y~1, data=...)</code>, with the <code>~1</code> indicating that no predictor variable is used and that a common mean is considered for all observations. Note that this notation also works in the <code>favstats</code> function to get summary statistics for the response variable without splitting it apart based on a grouping variable. In the <span class="math inline">\(n=30\)</span> passing distance data set, the mean of all the observations is 116.04 cm and this estimate is present in the <code>(Intercept)</code> row in the <code>lm_commonmean</code> model summary.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="chapter2.html#cb131-1"></a>lm_commonmean &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data=</span>ddsub)</span>
<span id="cb131-2"><a href="chapter2.html#cb131-2"></a><span class="kw">summary</span>(lm_commonmean)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Distance ~ 1, data = ddsub)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -108.038  -17.038   -0.038   16.962  128.962 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 116.0379     0.7361   157.6   &lt;2e-16
## 
## Residual standard error: 29.77 on 1635 degrees of freedom</code></pre>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="chapter2.html#cb133-1"></a><span class="kw">favstats</span>(Distance <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data=</span>ddsub)</span></code></pre></div>
<pre><code>##   1 min Q1 median  Q3 max     mean       sd    n missing
## 1 1   8 99    116 133 245 116.0379 29.77388 1636       0</code></pre>
<p>The second new R code needed is the <code>simulate</code>  function that can be applied to <code>lm</code>-objects; it generates a new data set that contains the same number of observations as the original one but assumes that all the aspects of the estimated model (mean(s), variance, and normal distributions) are true to generate the new observations. In this situation that implies generating new observations with the same mean (116.04) and standard deviation (29.77, also found as the “residual standard error” in the model summary).  The new responses are stored in <code>ddsub$SimDistance</code> and then plotted in Figure <a href="chapter2.html#fig:Figure2-19">2.19</a>.</p>

<div class="figure"><span id="fig:Figure2-19"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-19-1.png" alt="Pirate-plot of a simulated data set that assumes the same mean for both groups. The means in the two groups are very similar." width="960" />
<p class="caption">
Figure 2.19: Pirate-plot of a simulated data set that assumes the same mean for both groups. The means in the two groups are very similar.
</p>
</div>
<p>The following codechunk generates one run through generating ten data sets as the loop works through the index <code>c</code>, simulates a new set of responses (<code>ddsub$SimDistance</code>), fits a model that explores the difference in the means of the two groups (<code>lm_sim</code>), and extracts the ten p-values (stored in <code>pval10</code>) and estimated difference in the means (stored in <code>diff10</code>). The smallest p-value of the ten p-values (<code>min(pval10)</code>) is 0.00576. By finding the value of <code>diff10</code> where <code>pval10</code> is equal to (<code>==</code>) the <code>min(pval10)</code>, the estimated difference in the means from the simulated responses that produced the smallest p-value can be extracted. The difference was -4.17 here. As in the previous initial explorations of permutations, this is just one realization of this process and it needs to be repeated many times to study the impacts of using (1) the first realization of the responses to estimate the difference and p-value and (2) the result with the smallest p-value from ten different realizations of the responses to estimate the difference and p-value. In the following code, we added
octothorpes (#)<a href="#fn43" class="footnote-ref" id="fnref43"><sup>43</sup></a> and then some text to explain what is being calculated. In computer code, octothorpes
provide a way of adding comments that tell the software (here R) to ignore any
text after a “#” on a given line. In the color version of the text, comments are
even more clearly distinguished.</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="chapter2.html#cb135-1"></a><span class="co">#For one iteration through generating 10 data sets:</span></span>
<span id="cb135-2"><a href="chapter2.html#cb135-2"></a>  diff10 &lt;-<span class="st"> </span>pval10 &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span><span class="dv">10</span>) <span class="co">#Create empty vectors to store 10 results</span></span>
<span id="cb135-3"><a href="chapter2.html#cb135-3"></a>  <span class="kw">set.seed</span>(<span class="dv">222</span>)</span>
<span id="cb135-4"><a href="chapter2.html#cb135-4"></a>  <span class="co">#Create 10 data sets, keep estimated differences and p-values in diff10 and pval10</span></span>
<span id="cb135-5"><a href="chapter2.html#cb135-5"></a>  <span class="cf">for</span> (c <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)){</span>
<span id="cb135-6"><a href="chapter2.html#cb135-6"></a>    ddsub<span class="op">$</span>SimDistance &lt;-<span class="st"> </span><span class="kw">simulate</span>(lm_commonmean)[[<span class="dv">1</span>]]</span>
<span id="cb135-7"><a href="chapter2.html#cb135-7"></a>    <span class="co">#Estimate two group model using simulated responses</span></span>
<span id="cb135-8"><a href="chapter2.html#cb135-8"></a>    lm_sim &lt;-<span class="st"> </span><span class="kw">lm</span>(SimDistance <span class="op">~</span><span class="st"> </span>Condition, <span class="dt">data=</span>ddsub) </span>
<span id="cb135-9"><a href="chapter2.html#cb135-9"></a>    diff10[c] &lt;-<span class="st"> </span><span class="kw">coef</span>(lm_sim)[<span class="dv">2</span>]</span>
<span id="cb135-10"><a href="chapter2.html#cb135-10"></a>    pval10[c] &lt;-<span class="st"> </span><span class="kw">summary</span>(lm_sim)<span class="op">$</span>coef[<span class="dv">2</span>,<span class="dv">4</span>]</span>
<span id="cb135-11"><a href="chapter2.html#cb135-11"></a>  }</span>
<span id="cb135-12"><a href="chapter2.html#cb135-12"></a>  </span>
<span id="cb135-13"><a href="chapter2.html#cb135-13"></a><span class="kw">tibble</span>(pval10, diff10)  </span></code></pre></div>
<pre><code>## # A tibble: 10 x 2
##    pval10[,1] diff10[,1]
##         &lt;dbl&gt;      &lt;dbl&gt;
##  1    0.735      -0.492 
##  2    0.326       1.44  
##  3    0.158      -2.06  
##  4    0.265      -1.66  
##  5    0.153       2.09  
##  6    0.00576    -4.17  
##  7    0.915       0.160 
##  8    0.313      -1.50  
##  9    0.983       0.0307
## 10    0.268      -1.69</code></pre>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="chapter2.html#cb137-1"></a><span class="kw">min</span>(pval10) <span class="co">#Smallest of 10 p-values</span></span></code></pre></div>
<pre><code>## [1] 0.005764602</code></pre>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="chapter2.html#cb139-1"></a>diff10[pval10<span class="op">==</span><span class="kw">min</span>(pval10)] <span class="co">#Estimated difference for data set with smallest p-value</span></span></code></pre></div>
<pre><code>## [1] -4.170526</code></pre>
<p>In these results, the first data set shows little evidence against the null hypothesis with a p-value of 0.735 and an estimated difference of -0.49. But if you repeat this process and focus just on the “top” p-value result, you think that there is moderate evidence against the null hypothesis with a p-value from the sixth data set due to its p-value of 0.0057. Remember that these are all data sets simulated with the null hypothesis being true, so we should not reject the null hypothesis. But we would expect an occasional false detection (Type I error – rejecting the null hypothesis when it is true) due to sampling variability in the data sets. But by exploring many results and selecting a single result from that suite of results (and not accounting for that selection process in the results), there is a clear issue with exaggerating the strength of evidence. While not obvious yet, we also create an issue with the estimated mean difference in the groups that is demonstrated below.</p>
<p>To fully explore the impacts of either the office drawer or publication bias (they basically have the same impacts on published results even though they are different mechanisms), this process must be repeated many times. The code is a bit more complex here, as the previous code that created ten data sets needs to be replicated <em>B</em> = 1,000 times and four sets of results stored (estimated mean differences and p-values for the first data set and the smallest p-value one). This involves a loop that is very similar to our permutation loop but with more activity inside that loop, with the code for generating and extracting the realization of ten results repeated <em>B</em> times. Figure <a href="chapter2.html#fig:Figure2-20">2.20</a> contains the results for the simulation study. In the left plot that contains the p-values we can immediately see some important differences in the distribution of p-values. In the “first” result, the p-values are evenly spread from 0 to 1 – this is what happens when the null hypothesis is true and you simulate from that scenario one time and track the p-values. A good testing method should make a mistake at the <span class="math inline">\(\alpha\)</span>-level at a rate around <span class="math inline">\(\alpha\)</span> (a 5% significance level test should make a mistake 5% of the time). If the p-values are evenly spread from 0 to 1, then about 0.05 will be between 0 and 0.05 (think of areas in rectangles with a height of 1 where the total area from 0 to 1 has to add up to 1). But when a researcher focuses only on the top result of ten, then the p-value distribution is smashed toward 0. Using <code>favstats</code> on each distribution of p-values shows that the median for the p-values from taking the first result is around 0.5 but for taking the minimum of ten results, the median p-value is 0.065. So half the results are at the “moderate” evidence level or better when selection of results is included. This gets even worse as more results are explored but seems quite problematic here.</p>
<p>The estimated difference in the means also presents an interesting story. When just reporting the first result, the distribution of the estimated means in panel b of Figure <a href="chapter2.html#fig:Figure2-20">2.20</a> shows a symmetric distribution that is centered around 0 with results extending just past <span class="math inline">\(\pm\)</span> 4 in each tail. When selection of results is included, only more extreme estimated differences are considered and no results close to 0 are even reported. There are two modes here around <span class="math inline">\(\pm\)</span> 2.5 and multiple results close to <span class="math inline">\(\pm\)</span> 5 are observed. Interestingly, the mean of both distributions is close to 0 so both are “unbiased”<a href="#fn44" class="footnote-ref" id="fnref44"><sup>44</sup></a> estimators but the distribution for the estimated difference from the selected “top” result is clearly flawed and would not give correct inferences for differences when the null hypothesis is correct. If a one-sided test had been employed, the selection of the top result would result is a clearly biased estimator as only one of the two modes would be selected. The presentation of these results is a great example of why pirate-plots are better than boxplots as a boxplot of these results would not allow the viewer to notice the two distinct groups of results.</p>

<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="chapter2.html#cb141-1"></a><span class="co"># Simulation study of generating 10 data sets and either using the first</span></span>
<span id="cb141-2"><a href="chapter2.html#cb141-2"></a><span class="co"># or &quot;best p-value&quot; result:</span></span>
<span id="cb141-3"><a href="chapter2.html#cb141-3"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb141-4"><a href="chapter2.html#cb141-4"></a></span>
<span id="cb141-5"><a href="chapter2.html#cb141-5"></a>B &lt;-<span class="st"> </span><span class="dv">1000</span> <span class="co"># # of simulations</span></span>
<span id="cb141-6"><a href="chapter2.html#cb141-6"></a><span class="co"># To store results</span></span>
<span id="cb141-7"><a href="chapter2.html#cb141-7"></a>Diffmeans &lt;-<span class="st"> </span>pvalues &lt;-<span class="st"> </span>Diffmeans_Min &lt;-<span class="st"> </span>pvalues_Min &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B) </span>
<span id="cb141-8"><a href="chapter2.html#cb141-8"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){ <span class="co">#Simulation study loop to repeat process B times</span></span>
<span id="cb141-9"><a href="chapter2.html#cb141-9"></a>  <span class="co"># Create empty vectors to store 10 results for each b</span></span>
<span id="cb141-10"><a href="chapter2.html#cb141-10"></a>  diff10 &lt;-<span class="st"> </span>pval10 &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span><span class="dv">10</span>) </span>
<span id="cb141-11"><a href="chapter2.html#cb141-11"></a>  <span class="cf">for</span> (c <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)){ <span class="co">#Loop to create 10 data sets and extract results</span></span>
<span id="cb141-12"><a href="chapter2.html#cb141-12"></a>    ddsub<span class="op">$</span>SimDistance &lt;-<span class="st"> </span><span class="kw">simulate</span>(lm_commonmean)[[<span class="dv">1</span>]]</span>
<span id="cb141-13"><a href="chapter2.html#cb141-13"></a>    <span class="co"># Estimate two group model using simulated responses</span></span>
<span id="cb141-14"><a href="chapter2.html#cb141-14"></a>    lm_sim &lt;-<span class="st"> </span><span class="kw">lm</span>(SimDistance <span class="op">~</span><span class="st"> </span>Condition, <span class="dt">data=</span>ddsub) </span>
<span id="cb141-15"><a href="chapter2.html#cb141-15"></a>    diff10[c] &lt;-<span class="st"> </span><span class="kw">coef</span>(lm_sim)[<span class="dv">2</span>]</span>
<span id="cb141-16"><a href="chapter2.html#cb141-16"></a>    pval10[c] &lt;-<span class="st"> </span><span class="kw">summary</span>(lm_sim)<span class="op">$</span>coef[<span class="dv">2</span>,<span class="dv">4</span>]</span>
<span id="cb141-17"><a href="chapter2.html#cb141-17"></a>  }</span>
<span id="cb141-18"><a href="chapter2.html#cb141-18"></a>  </span>
<span id="cb141-19"><a href="chapter2.html#cb141-19"></a>  pvalues[b] &lt;-<span class="st"> </span>pval10[<span class="dv">1</span>] <span class="co">#Store first result p-value</span></span>
<span id="cb141-20"><a href="chapter2.html#cb141-20"></a>  Diffmeans[b] &lt;-<span class="st"> </span>diff10[<span class="dv">1</span>] <span class="co">#Store first result estimated difference</span></span>
<span id="cb141-21"><a href="chapter2.html#cb141-21"></a>  </span>
<span id="cb141-22"><a href="chapter2.html#cb141-22"></a>  pvalues_Min[b] &lt;-<span class="st"> </span><span class="kw">min</span>(pval10) <span class="co">#Store smallest p-value</span></span>
<span id="cb141-23"><a href="chapter2.html#cb141-23"></a>  Diffmeans_Min[b] &lt;-<span class="st"> </span>diff10[pval10<span class="op">==</span><span class="kw">min</span>(pval10)] <span class="co">#Store est. diff of smallest p-value</span></span>
<span id="cb141-24"><a href="chapter2.html#cb141-24"></a>  </span>
<span id="cb141-25"><a href="chapter2.html#cb141-25"></a>}</span>
<span id="cb141-26"><a href="chapter2.html#cb141-26"></a></span>
<span id="cb141-27"><a href="chapter2.html#cb141-27"></a><span class="co">#Put results together</span></span>
<span id="cb141-28"><a href="chapter2.html#cb141-28"></a>results &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">pvalue_results=</span><span class="kw">c</span>(pvalues,pvalues_Min), </span>
<span id="cb141-29"><a href="chapter2.html#cb141-29"></a>                  <span class="dt">Diffmeans_results=</span><span class="kw">c</span>(Diffmeans, Diffmeans_Min), </span>
<span id="cb141-30"><a href="chapter2.html#cb141-30"></a>                  <span class="dt">Scenario =</span> <span class="kw">rep</span>(<span class="kw">c</span>(<span class="st">&quot;First&quot;</span>, <span class="st">&quot;Min&quot;</span>), <span class="dt">each=</span>B))</span>
<span id="cb141-31"><a href="chapter2.html#cb141-31"></a></span>
<span id="cb141-32"><a href="chapter2.html#cb141-32"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)) <span class="co">#Plot results</span></span>
<span id="cb141-33"><a href="chapter2.html#cb141-33"></a><span class="kw">pirateplot</span>(pvalue_results<span class="op">~</span>Scenario,<span class="dt">data=</span>results, <span class="dt">inf.f.o =</span> <span class="dv">0</span>,<span class="dt">inf.b.o =</span> <span class="dv">0</span>,</span>
<span id="cb141-34"><a href="chapter2.html#cb141-34"></a>           <span class="dt">avg.line.o =</span> <span class="dv">0</span>, <span class="dt">main=</span><span class="st">&quot;(a) P-value results&quot;</span>)</span>
<span id="cb141-35"><a href="chapter2.html#cb141-35"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="fl">0.05</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</span>
<span id="cb141-36"><a href="chapter2.html#cb141-36"></a><span class="kw">pirateplot</span>(Diffmeans_results<span class="op">~</span>Scenario,<span class="dt">data=</span>results, <span class="dt">inf.f.o =</span> <span class="dv">0</span>,<span class="dt">inf.b.o =</span> <span class="dv">0</span>,</span>
<span id="cb141-37"><a href="chapter2.html#cb141-37"></a>           <span class="dt">avg.line.o =</span> <span class="dv">0</span>, <span class="dt">main=</span><span class="st">&quot;(b) Estimated difference in mean results&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure2-20"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-20-1.png" alt="Pirate-plot of a simulation study results. Panel (a) contains the B = 1,000 p-values and (b) contains the B=1,000 estimated differences in the means. Note that the estimated means and confidence intervals normally present in pirate-plots are suppressed here with inf.f.o = 0, inf.b.o = 0, avg.line.o = 0 because these plots are being used to summarize simulation results instead of an original data set." width="960" />
<p class="caption">
Figure 2.20: Pirate-plot of a simulation study results. Panel (a) contains the <em>B</em> = 1,000 p-values and (b) contains the <em>B</em>=1,000 estimated differences in the means. Note that the estimated means and confidence intervals normally present in pirate-plots are suppressed here with <code>inf.f.o = 0, inf.b.o = 0, avg.line.o = 0</code> because these plots are being used to summarize simulation results instead of an original data set.
</p>
</div>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="chapter2.html#cb142-1"></a><span class="co">#Numerical summaries of results</span></span>
<span id="cb142-2"><a href="chapter2.html#cb142-2"></a><span class="kw">favstats</span>(pvalue_results<span class="op">~</span>Scenario,<span class="dt">data=</span>results)</span></code></pre></div>
<pre><code>##   Scenario          min         Q1    median        Q3       max       mean
## 1    First 0.0017051496 0.27075755 0.5234412 0.7784957 0.9995293 0.51899179
## 2      Min 0.0005727895 0.02718018 0.0646370 0.1273880 0.5830232 0.09156364
##           sd    n missing
## 1 0.28823469 1000       0
## 2 0.08611836 1000       0</code></pre>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="chapter2.html#cb144-1"></a><span class="kw">favstats</span>(Diffmeans_results<span class="op">~</span>Scenario,<span class="dt">data=</span>results)</span></code></pre></div>
<pre><code>##   Scenario       min         Q1     median       Q3      max       mean
## 1    First -4.531864 -0.8424604 0.07360378 1.002228 4.458951 0.05411473
## 2      Min -5.136510 -2.6857436 1.24042295 2.736930 5.011190 0.03539750
##         sd    n missing
## 1 1.392940 1000       0
## 2 2.874454 1000       0</code></pre>
<p>Generally, the challenge in this situation is that if you perform many tests (ten were the focus before) at the same time (instead of just one test), you inflate the
Type I error rate across the tests.

We can define the <strong><em>family-wise error rate</em></strong> 
as the probability that at least one error is made on a set of tests or, more
compactly, Pr(At least 1 error is made) where Pr() is the probability of an
event occurring. The family-wise error is meant to capture the overall
situation in terms of measuring the likelihood of making a mistake if we
consider many tests, each with some chance of making their own mistake, and
focus on how often we make at least one error when we do many tests. A quick
probability calculation shows the magnitude of the problem. If we start with a
5% significance level test, then Pr(Type I error on one test) = 0.05 and the Pr(no
errors made on one test) = 0.95, by definition. This is our standard hypothesis
testing situation. Now, suppose we have <span class="math inline">\(m\)</span> independent tests, then</p>
<p><span class="math display">\[\begin{array}{ll}
&amp; \text{Pr(make at least 1 Type I error given all null hypotheses are true)} \\
&amp; = 1 - \text{Pr(no errors made)} \\
&amp; = 1 - 0.95^m.
\end{array}\]</span></p>
<p>Figure <a href="chapter2.html#fig:Figure2-21">2.21</a> shows how the probability of having at least one
false detection grows rapidly with the number of tests, <span class="math inline">\(m\)</span>. The plot stops at 100
tests since it is effectively a 100% chance of at least one false detection.
It might seem like doing 100 tests is a lot, but, as mentioned before, some researchers consider situations where millions of tests are
considered. Researchers want to make sure that when they report a “significant” result that
it is really likely to be a real result and will show up as a difference in the
next data set they collect. Some researchers are now collecting multiple data
sets to use in a single study and using one data set to identify interesting
results and then using a validation or test data set that they withheld from
initial analysis to try to verify that the first results are also present in that
second data set. This also has problems but the only way to develop an understanding of a process is to look across a suite of studies and learn from that accumulation of evidence. This is a good start but needs to be coupled with complete reporting of all results, even those that have p-values larger than 0.05 to avoid the bias identified in the previous simulation study.</p>

<div class="figure"><span id="fig:Figure2-21"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-21-1.png" alt="Plot of family-wise error rate (Bold solid line) as the number of tests performed increases. Dashed line indicates 0.05 and grey solid line highlights the probability of at least on error on \(m\)=10 tests." width="480" />
<p class="caption">
Figure 2.21: Plot of family-wise error rate (Bold solid line) as the number of tests performed increases. Dashed line indicates 0.05 and grey solid line highlights the probability of at least on error on <span class="math inline">\(m\)</span>=10 tests.
</p>
</div>
<p>All hope is not lost when multiple tests are being considered in the same study or by a researcher and exploring more than one result need not lead to clearly biased and flawed results being reported. To account for multiple testing in the same study/analysis, there are many approaches that adjust results to acknowledge that multiple tests are being considered. A simple approach called the “Bonferroni Correction” <span class="citation">(Bland and Altman <a href="#ref-Bland1995" role="doc-biblioref">1995</a>)</span> is a good starting point for learning about these methods. It works to control the family-wise error rate of a suite of tests by either dividing <span class="math inline">\(\alpha\)</span> by the number of tests (<span class="math inline">\(\alpha/m\)</span>) or, equivalently and more usefully, multiplying the p-value by the number of tests being considered (<span class="math inline">\(p-value_{adjusted} = p-value \cdot m\)</span> or <span class="math inline">\(1\)</span> if <span class="math inline">\(p-value \cdot m &gt; 1\)</span>). The “Bonferroni adjusted p-values” are then used as regular p-values to assess evidence against each null hypothesis but now accounting for exploring many of them together. There are some assumptions that this adjustment method makes that make it to generally be a conservative adjustment method. In particular, it assumes that all <span class="math inline">\(m\)</span> tests are independent of each other and that the null hypothesis was true for all <span class="math inline">\(m\)</span> tests conducted. While all p-values should be reported in this situation when considering ten results, the impacts of using a Bonferroni correction are that the resulting p-values are not driving inflated Type I error rates even if the smallest p-value is the main focus of the results. The correction also provides a suggestion of decreasing evidence in the first test result because it is now incorporated in considering ten results instead of one.</p>
<p>The following code repeats the simulation study but with the p-values adjusted for multiple testing within each simulation but does not repeat tracking the estimated differences in the means as this is not impacted by the p-value adjustment process. The <code>p.adjust</code> function provides Bonferroni corrections to a vector of p-values (here ten are collected together) using the <code>bonferroni</code> method option (<code>p.adjust(pval10, method="bonferroni")</code>) and then stores those results. Figure <a href="chapter2.html#fig:Figure2-22">2.22</a> shows the results for the first result and minimum result again, but now with these corrections incorporated. The plots may look a bit odd, but in the first data set, so many of the first data sets had p-values that were “large” that they were adjusted to have p-values of 1 (so no evidence against the null once we account for multiple testing). The distribution for the minimum p-value results with adjustment more closely resembles the distribution of the first result p-values from Figure <a href="chapter2.html#fig:Figure2-20">2.20</a>, except for some minor clumping up at adjusted p-values of 1.</p>

<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="chapter2.html#cb146-1"></a><span class="co"># Simulation study of generating 10 data sets and either using the first </span></span>
<span id="cb146-2"><a href="chapter2.html#cb146-2"></a><span class="co"># or &quot;best p-value&quot; result:</span></span>
<span id="cb146-3"><a href="chapter2.html#cb146-3"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb146-4"><a href="chapter2.html#cb146-4"></a></span>
<span id="cb146-5"><a href="chapter2.html#cb146-5"></a>B &lt;-<span class="st"> </span><span class="dv">1000</span> <span class="co"># # of simulations</span></span>
<span id="cb146-6"><a href="chapter2.html#cb146-6"></a>pvalues &lt;-<span class="st"> </span>pvalues_Min &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B) <span class="co">#To store results</span></span>
<span id="cb146-7"><a href="chapter2.html#cb146-7"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){ <span class="co">#Simulation study loop to repeat process B times</span></span>
<span id="cb146-8"><a href="chapter2.html#cb146-8"></a>  <span class="co"># Create empty vectors to store 10 results for each b</span></span>
<span id="cb146-9"><a href="chapter2.html#cb146-9"></a>  pval10 &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span><span class="dv">10</span>) </span>
<span id="cb146-10"><a href="chapter2.html#cb146-10"></a>  <span class="cf">for</span> (c <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)){ <span class="co">#Loop to create 10 data sets and extract results</span></span>
<span id="cb146-11"><a href="chapter2.html#cb146-11"></a>    ddsub<span class="op">$</span>SimDistance &lt;-<span class="st"> </span><span class="kw">simulate</span>(lm_commonmean)[[<span class="dv">1</span>]]</span>
<span id="cb146-12"><a href="chapter2.html#cb146-12"></a>    <span class="co"># Estimate two group model using simulated responses</span></span>
<span id="cb146-13"><a href="chapter2.html#cb146-13"></a>    lm_sim &lt;-<span class="st"> </span><span class="kw">lm</span>(SimDistance <span class="op">~</span><span class="st"> </span>Condition, <span class="dt">data=</span>ddsub) </span>
<span id="cb146-14"><a href="chapter2.html#cb146-14"></a>    pval10[c] &lt;-<span class="st"> </span><span class="kw">summary</span>(lm_sim)<span class="op">$</span>coef[<span class="dv">2</span>,<span class="dv">4</span>]</span>
<span id="cb146-15"><a href="chapter2.html#cb146-15"></a>  }</span>
<span id="cb146-16"><a href="chapter2.html#cb146-16"></a>  </span>
<span id="cb146-17"><a href="chapter2.html#cb146-17"></a>  pval10 &lt;-<span class="st"> </span><span class="kw">p.adjust</span>(pval10, <span class="dt">method=</span><span class="st">&quot;bonferroni&quot;</span>)</span>
<span id="cb146-18"><a href="chapter2.html#cb146-18"></a>  </span>
<span id="cb146-19"><a href="chapter2.html#cb146-19"></a>  pvalues[b] &lt;-<span class="st"> </span>pval10[<span class="dv">1</span>] <span class="co">#Store first result adjusted p-value</span></span>
<span id="cb146-20"><a href="chapter2.html#cb146-20"></a>  </span>
<span id="cb146-21"><a href="chapter2.html#cb146-21"></a>  pvalues_Min[b] &lt;-<span class="st"> </span><span class="kw">min</span>(pval10) <span class="co">#Store smallest adjusted p-value</span></span>
<span id="cb146-22"><a href="chapter2.html#cb146-22"></a>  </span>
<span id="cb146-23"><a href="chapter2.html#cb146-23"></a>}</span>
<span id="cb146-24"><a href="chapter2.html#cb146-24"></a></span>
<span id="cb146-25"><a href="chapter2.html#cb146-25"></a><span class="co">#Put results together</span></span>
<span id="cb146-26"><a href="chapter2.html#cb146-26"></a>results &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">pvalue_results=</span><span class="kw">c</span>(pvalues,pvalues_Min), </span>
<span id="cb146-27"><a href="chapter2.html#cb146-27"></a>                  <span class="dt">Scenario =</span> <span class="kw">rep</span>(<span class="kw">c</span>(<span class="st">&quot;First&quot;</span>, <span class="st">&quot;Min&quot;</span>), <span class="dt">each=</span>B))</span>
<span id="cb146-28"><a href="chapter2.html#cb146-28"></a></span>
<span id="cb146-29"><a href="chapter2.html#cb146-29"></a><span class="kw">pirateplot</span>(pvalue_results<span class="op">~</span>Scenario,<span class="dt">data=</span>results, <span class="dt">inf.f.o =</span> <span class="dv">0</span>,<span class="dt">inf.b.o =</span> <span class="dv">0</span>,</span>
<span id="cb146-30"><a href="chapter2.html#cb146-30"></a>           <span class="dt">avg.line.o =</span> <span class="dv">0</span>, <span class="dt">main=</span><span class="st">&quot;P-value results&quot;</span>)</span>
<span id="cb146-31"><a href="chapter2.html#cb146-31"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="fl">0.05</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure2-22"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-22-1.png" alt="Pirate-plot of a simulation study results of p-values with Bonferroni correction." width="960" />
<p class="caption">
Figure 2.22: Pirate-plot of a simulation study results of p-values with Bonferroni correction.
</p>
</div>
<p>By applying the <code>pdata</code> function to the two groups of results, we can directly assess how many of each type (“First” or “Min”) resulted in p-values less than 0.05. It ends up that if we adjust for ten tests and just focus on the first result, it is really hard to find moderate or strong evidence against the null hypothesis as only 3 in 1,000 results had adjusted p-values less than 0.05. When the focus is on the “best” (or minimum) p-value result when ten are considered and adjustments are made, 52 out of 1,000 results (0.052) show at least moderate evidence against the null hypothesis. This is the rate we would expect from a well-behaved hypothesis test when the null hypothesis is true – that we would only make a mistake 5% of the time when <span class="math inline">\(\alpha\)</span> is 0.05.</p>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="chapter2.html#cb147-1"></a><span class="co">#Numerical summaries of results</span></span>
<span id="cb147-2"><a href="chapter2.html#cb147-2"></a><span class="kw">favstats</span>(pvalue_results<span class="op">~</span>Scenario,<span class="dt">data=</span>results)</span></code></pre></div>
<pre><code>##   Scenario         min        Q1  median Q3 max      mean        sd    n missing
## 1    First 0.017051496 1.0000000 1.00000  1   1 0.9628911 0.1502805 1000       0
## 2      Min 0.005727895 0.2718018 0.64637  1   1 0.6212932 0.3597701 1000       0</code></pre>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="chapter2.html#cb149-1"></a><span class="co">#Proportion of simulations with adjusted p-values less than 0.05</span></span>
<span id="cb149-2"><a href="chapter2.html#cb149-2"></a><span class="kw">pdata</span>(pvalue_results<span class="op">~</span>Scenario,<span class="dt">data=</span>results,.<span class="dv">05</span>, <span class="dt">lower.tail=</span>T)</span></code></pre></div>
<pre><code>##   Scenario pdata_v
## 1    First   0.003
## 2      Min   0.052</code></pre>
<p>So adjusting for multiple testing is suggested when multiple tests are being considered “simultaneously”. The Bonferroni adjustment is easy but also crude and can be conservative in applications, especially when the number of tests grows very large (think of multiplying all your p-values by <span class="math inline">\(m\)</span>=1,000,000). So other approaches are considered in situations with many tests (there are six other options in the <code>p.adjust</code> function and other functions for doing similar things in R) and there are other approaches that are customized for particular situations with one example discussed in Chapter <a href="chapter3.html#chapter3">3</a>. The biggest lesson as a statistics student to take from this is that all results are of interest and should be reported and that adjustment of p-values should be considered in studies where many results are being considered. If you are reading results that seem to have walked discretely around these issues you should be suspicious of the real strength of their evidence.</p>
<p>While it wasn’t used here, the same general code used to explore this multiple testing issue could be used to explore the power of a particular procedure. If simulations were created from a model with a difference in the means in the groups, then the null hypothesis would have been false and the rate of correctly rejecting the null hypothesis could be studied. The rate of correct rejections is the <em>power</em> of a procedure for a chosen version of a true alternative hypothesis (there are many ways to have it be true and you have to choose one to study power) and simply switching the model being simulated from would allow that to be explored. We could also use similar code to compare the power and Type I error rates of parametric versus permutation procedures or to explore situations where an assumption is not true. The steps would be similar – decide on what you need to simulate from and track a quantity of interest across repeated simulated data sets.</p>
</div>
<div id="section2-9" class="section level2">
<h2><span class="header-section-number">2.9</span> Confidence intervals and bootstrapping</h2>
<p>Up to this point the focus has been on hypotheses, p-values, and estimates of the size of differences. But so far this has not explored inference techniques for the size of the difference. <strong><em>Confidence intervals</em></strong> provide an interval where we are __% <strong><em>confident</em></strong> that the true parameter lies.  The idea of “confidence” is that if we repeated randomly sampling from the same population and made a similar confidence interval, the collection of all these confidence intervals would contain the true parameter at the specified confidence level (usually 95%). We only get to make one interval and so it either has the true parameter in it or not, and we don’t know the truth in real situations.</p>
<p>Confidence intervals can be constructed with parametric  and a
nonparametric approaches. The nonparametric 
approach will be using what is
called <strong><em>bootstrapping</em></strong>

and draws its name from “pull yourself up by
your bootstraps” where you improve your situation based on your own efforts.
In statistics, we make our situation or inferences better by re-using the
observations we have by assuming that the sample represents the population.
Since each observation represents other similar observations in the
population that we didn’t get to measure, if we <strong><em>sample with replacement</em></strong>
to generate a new data set of size <em>n</em> from our data set (also of size <em>n</em>)
it mimics the process of taking repeated random samples  of size <span class="math inline">\(n\)</span> from our
population of interest. This process also
ends up giving us useful sampling distributions

of statistics even when our
standard normality assumption is violated, similar to what we encountered
in the permutation tests. Bootstrapping is especially useful in situations
where we are interested in statistics other than the mean (say we want a
confidence interval for a median or a standard deviation) or when we consider
functions of more than one parameter and don’t want to derive the distribution
of the statistic (say the difference in two medians). Here,
bootstrapping is used to provide more trustworthy inferences when some of our
assumptions (especially normality) might be violated for our parametric confidence interval procedure.
</p>
<p>To perform bootstrapping, the <code>resample</code> function from the
<code>mosaic</code> package will be used. We can apply this function to a data set and get a new
version of the
data set by sampling new observations <em>with replacement</em> from the original one<a href="#fn45" class="footnote-ref" id="fnref45"><sup>45</sup></a>.
The new, bootstrapped version of the data set (called <code>dsample_BTS</code> below)
contains a new variable called <code>orig.id</code> which is the number of the subject
from the original data set. By summarizing how often each of these id’s
occurred in a bootstrapped data set, we can see how the re-sampling works.
The <code>table</code> function will count up how many times each observation was used in
the bootstrap sample,

providing a row with the id followed by a row with the
count<a href="#fn46" class="footnote-ref" id="fnref46"><sup>46</sup></a>. In the first bootstrap
sample shown, the 1<sup>st</sup>, 14<sup>th</sup>, and 26<sup>th</sup> observations
were sampled twice, the 9<sup>th</sup> and 28<sup>th</sup> observations were sampled four
times, and the 4<sup>th</sup>, 5<sup>th</sup>, 6<sup>th</sup>, and many others
were not sampled at all. Bootstrap sampling thus picks some observations
multiple times and to do that it has to ignore some<a href="#fn47" class="footnote-ref" id="fnref47"><sup>47</sup></a> observations.</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="chapter2.html#cb151-1"></a><span class="kw">set.seed</span>(<span class="dv">406</span>)</span>
<span id="cb151-2"><a href="chapter2.html#cb151-2"></a>dsample_BTS &lt;-<span class="st"> </span><span class="kw">resample</span>(dsample)</span></code></pre></div>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="chapter2.html#cb152-1"></a><span class="kw">table</span>(<span class="kw">as.numeric</span>(dsample_BTS<span class="op">$</span>orig.id))</span></code></pre></div>
<pre><code>## 
##  1  2  3  7  8  9 10 11 12 13 14 16 18 19 23 24 25 26 27 28 30 
##  2  1  1  1  1  4  1  1  1  1  2  1  1  1  1  1  1  2  1  4  1</code></pre>
<p>Like in permutations, one randomization isn’t enough. A second bootstrap sample
is also provided to help you get a sense of what bootstrap data sets contain.
It did not select observations two through five but did select eight others more than once.
You can see other variations in the resulting re-sampling of subjects with the
most sampled observation used four times. With <span class="math inline">\(n=30\)</span>, the the chance of
selecting any observation for any slot
in the new data set is <span class="math inline">\(1/30\)</span> and the expected or mean number of appearances we
expect to see for an observation is the number of random draws times the probably
of selection on each so <span class="math inline">\(30*1/30=1\)</span>. So we expect to see each observation in the bootstrap sample on average once but random variability in the samples then creates the possibility of seeing it more than once or not all.</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="chapter2.html#cb154-1"></a>dsample_BTS2 &lt;-<span class="st"> </span><span class="kw">resample</span>(dsample)</span>
<span id="cb154-2"><a href="chapter2.html#cb154-2"></a><span class="kw">table</span>(<span class="kw">as.numeric</span>(dsample_BTS2<span class="op">$</span>orig.id))</span></code></pre></div>
<pre><code>## 
##  1  6  7  8  9 10 11 12 13 16 17 20 22 23 24 25 26 28 30 
##  2  2  1  1  2  1  4  1  3  1  1  1  2  2  1  1  2  1  1</code></pre>
<p>We can use the two results to get an idea of distribution of results in terms
of number of times observations might be re-sampled when sampling with
replacement and the variation in those results, as shown in
Figure <a href="chapter2.html#fig:Figure2-23">2.23</a>. We could also derive the expected counts for
each number of times of re-sampling when we start with all observations having
an equal chance and sampling with replacement but this isn’t important for
using bootstrapping methods.</p>

<div class="figure"><span id="fig:Figure2-23"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-23-1.png" alt="Counts of number of times of observation (or not observed for times re-sampled of 0) for two bootstrap samples." width="480" />
<p class="caption">
Figure 2.23: Counts of number of times of observation (or not observed for times re-sampled of 0) for two bootstrap samples.
</p>
</div>
<p>The main point of this exploration was to see that each run of the
<code>resample</code> function provides a new version of the data set. Repeating this
<span class="math inline">\(B\)</span> times using
another <code>for</code> loop, we will track our quantity of interest, say <span class="math inline">\(T\)</span>, in all
these new “data sets” and call those results <span class="math inline">\(T^*\)</span>. The distribution of the
bootstrapped

<span class="math inline">\(T^*\)</span> statistics tells us about the range of results to expect
for the statistic. The middle <strong>% of the <span class="math inline">\(T^*\)</span>’s provides a </strong>%
<strong><em>bootstrap confidence interval</em></strong><a href="#fn48" class="footnote-ref" id="fnref48"><sup>48</sup></a> for the true parameter – here the <em>difference in the two population means</em>.</p>
<p>To make this concrete, we can revisit our previous examples, starting
with the <code>dsample</code> data created before and our interest in comparing the
mean passing distances for the <em>commuter</em> and <em>casual</em> outfit groups in the <span class="math inline">\(n=30\)</span> stratified random sample that was extracted. The
bootstrapping code is very similar to the permutation code except that we apply
the <code>resample</code> function to the entire data set used in <code>lm</code> as opposed to the <code>shuffle</code>
function that was applied only to the explanatory variable.</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="chapter2.html#cb156-1"></a>lm1 &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>dsample)</span>
<span id="cb156-2"><a href="chapter2.html#cb156-2"></a>Tobs &lt;-<span class="st"> </span><span class="kw">coef</span>(lm1)[<span class="dv">2</span>]; Tobs</span></code></pre></div>
<pre><code>## Conditioncommute 
##        -25.93333</code></pre>
<!-- \newpage -->
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="chapter2.html#cb158-1"></a>B &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb158-2"><a href="chapter2.html#cb158-2"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb158-3"><a href="chapter2.html#cb158-3"></a>Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</span>
<span id="cb158-4"><a href="chapter2.html#cb158-4"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</span>
<span id="cb158-5"><a href="chapter2.html#cb158-5"></a>  lmP &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span><span class="kw">resample</span>(dsample))</span>
<span id="cb158-6"><a href="chapter2.html#cb158-6"></a>  Tstar[b] &lt;-<span class="st"> </span><span class="kw">coef</span>(lmP)[<span class="dv">2</span>]</span>
<span id="cb158-7"><a href="chapter2.html#cb158-7"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="chapter2.html#cb159-1"></a><span class="kw">favstats</span>(Tstar)</span></code></pre></div>
<pre><code>##        min        Q1    median        Q3      max      mean       sd    n missing
##  -66.96429 -34.57159 -25.65881 -17.12391 17.17857 -25.73641 12.30987 1000       0</code></pre>

<div class="figure"><span id="fig:Figure2-24"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-24-1.png" alt="Histogram and density curve of bootstrap distributions of difference in sample mean Distances with vertical line for the observed difference in the means of -25.933." width="960" />
<p class="caption">
Figure 2.24: Histogram and density curve of bootstrap distributions of difference in sample mean <code>Distances</code> with vertical line for the observed difference in the means of -25.933.
</p>
</div>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="chapter2.html#cb161-1"></a><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)</span>
<span id="cb161-2"><a href="chapter2.html#cb161-2"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</span>
<span id="cb161-3"><a href="chapter2.html#cb161-3"></a><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</span>
<span id="cb161-4"><a href="chapter2.html#cb161-4"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p>In this situation, the observed difference in the mean passing distances is -25.933 cm
(<em>commute</em> - <em>casual</em>), which is the bold vertical line in Figure
<a href="chapter2.html#fig:Figure2-24">2.24</a>.
The bootstrap distribution

shows the results for the difference in the sample
means when fake data sets are re-constructed by sampling from the original data set with
replacement. The bootstrap distribution is approximately centered at the observed
value (difference in the sample means) and is relatively symmetric.</p>
<p>The permutation distribution  in the same situation (Figure
<a href="chapter2.html#fig:Figure2-11">2.11</a>) had a similar shape but was centered at 0.
Permutations create sampling
distributions

based on assuming the null hypothesis is true, which is useful for
hypothesis testing. Bootstrapping creates distributions centered at the observed
result, which is the sampling distribution “under the alternative” or when no null
hypothesis is assumed; bootstrap distributions are useful for generating
confidence intervals for the true parameter values.</p>
<p>To create a 95% bootstrap confidence interval for the difference in
the true mean distances (<span class="math inline">\(\mu_\text{commute}-\mu_\text{casual}\)</span>), select the
middle 95% of results from
the bootstrap distribution. Specifically, find the 2.5<sup>th</sup>
percentile and the 97.5<sup>th</sup> percentile (values that put 2.5 and 97.5%
of the results to the left) in the bootstrap distribution, which leaves 95% in
the middle for the confidence interval. To find percentiles in a distribution
in R, functions are of the form <code>q[Name of distribution]</code>, with the function
<code>qt</code> extracting percentiles from a <span class="math inline">\(t\)</span>-distribution (examples below). From the
bootstrap results, use the <code>qdata</code> function on the <code>Tstar</code> results that
contain the bootstrap distribution of the statistic of interest.</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="chapter2.html#cb162-1"></a><span class="kw">qdata</span>(Tstar, <span class="fl">0.025</span>)</span></code></pre></div>
<pre><code>##        p quantile 
##   0.0250 -50.0055</code></pre>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="chapter2.html#cb164-1"></a><span class="kw">qdata</span>(Tstar, <span class="fl">0.975</span>)</span></code></pre></div>
<pre><code>##         p  quantile 
##  0.975000 -2.248774</code></pre>
<p>These results tell us that the 2.5<sup>th</sup> percentile of the bootstrap
distribution is at -50.01 cm and the 97.5<sup>th</sup> percentile is at -2.249 cm. We can combine these results to provide a 95% confidence for
<span class="math inline">\(\mu_\text{commute}-\mu_\text{casaual}\)</span> that is between -50 and -2.25 cm. This interval is interpreted as with any confidence interval, that we are 95% confident that the difference
in the true mean distances (<em>commute</em> minus <em>casual</em> groups) is
between -50 and -2.25 cm. Or we can switch the direction of the comparison and say that we are 95% confident that the difference in the true means is between 2.25 and 50 cm (<em>casual</em> minus <em>commute</em>). This result would be incorporated into step 5 of the hypothesis testing protocol to accompany discussing the size of the estimated difference in the groups or used as a result of interest in itself. Both percentiles can be obtained in one line
of code using:</p>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="chapter2.html#cb166-1"></a>quantiles &lt;-<span class="st"> </span><span class="kw">qdata</span>(Tstar, <span class="kw">c</span>(<span class="fl">0.025</span>,<span class="fl">0.975</span>))</span></code></pre></div>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="chapter2.html#cb167-1"></a>quantiles</span></code></pre></div>
<pre><code>##         quantile     p
## 2.5%  -50.005502 0.025
## 97.5%  -2.248774 0.975</code></pre>
<p>Figure <a href="chapter2.html#fig:Figure2-25">2.25</a> displays those same percentiles on the bootstrap distribution residing in <code>Tstar</code>.</p>

<div class="figure"><span id="fig:Figure2-25"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-25-1.png" alt="Histogram and density curve of bootstrap distribution with 95% bootstrap confidence intervals displayed (bold vertical lines)." width="960" />
<p class="caption">
Figure 2.25: Histogram and density curve of bootstrap distribution with 95% bootstrap confidence intervals displayed (bold vertical lines).
</p>
</div>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="chapter2.html#cb169-1"></a><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)</span>
<span id="cb169-2"><a href="chapter2.html#cb169-2"></a><span class="kw">abline</span>(<span class="dt">v=</span>quantiles<span class="op">$</span>quantile, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb169-3"><a href="chapter2.html#cb169-3"></a><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</span>
<span id="cb169-4"><a href="chapter2.html#cb169-4"></a><span class="kw">abline</span>(<span class="dt">v=</span>quantiles<span class="op">$</span>quantile, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span></code></pre></div>
<p>Although confidence intervals can exist without referencing hypotheses,
we can
revisit our previous hypotheses and see what this confidence interval tells
us about the test of <span class="math inline">\(H_0: \mu_\text{commute} = \mu_\text{casual}\)</span>. This null
hypothesis is equivalent to testing <span class="math inline">\(H_0: \mu_\text{commute} - \mu_\text{casual} = 0\)</span>,
that the difference
in the true means is equal to 0 cm. And the difference in the means was the
scale for our confidence interval, which did not contain 0 cm. The
0 cm values is an interesting <strong><em>reference value</em></strong> for the confidence interval, because
here it is the value where the true means are equal to each other (have a
difference of 0 cm). In general, if our confidence interval does not contain
0, then it is saying that 0 is not one of the likely values for the difference
in the true means at the selected confidence level. This implies that we should reject a claim that they are
equal. This provides the same inferences for the hypotheses that we considered
previously using both parametric and permutation approaches using a fixed <span class="math inline">\(\alpha\)</span> approach where <span class="math inline">\(\alpha\)</span> = 1 - confidence level.</p>
<p>The general summary
is that we can use confidence intervals to test hypotheses by assessing whether
the reference value under the null hypothesis is in the confidence interval
(suggests insufficient evidence against <span class="math inline">\(H_0\)</span> to reject it, at least at the <span class="math inline">\(\alpha\)</span> level and equivalent to having a p-value larger than <span class="math inline">\(\alpha\)</span>) or outside the confidence interval (sufficient evidence against <span class="math inline">\(H_0\)</span> to reject it and equivalent to having a p-value that is less than <span class="math inline">\(\alpha\)</span>). P-values

are more
informative about hypotheses (measure of evidence against the null hypothesis)
but confidence intervals are more informative
about the size of differences, so both offer useful information and, as shown
here, can provide consistent conclusions about hypotheses. But it is best practice to use p-values to assess evidence against null hypotheses and confidence intervals to do inferences for the size of differences.</p>
<p>As in the previous situation, we also want to consider the parametric
approach
for comparison purposes and to have that method available, especially to help
us understand some methods where we will only consider parametric inferences
in later chapters. The parametric confidence interval is called the
<strong><em>equal variance, two-sample t confidence interval</em></strong> and additionally
assumes that the populations
being sampled from are normally distributed instead of just that they have similar shapes in the bootstrap approach. The parametric method leads to using a <span class="math inline">\(t\)</span>-distribution

to form the interval with the degrees of freedom for the <span class="math inline">\(t\)</span>-distribution of <span class="math inline">\(n-2\)</span> although we can obtain it without direct reference to this distribution using the <code>confint</code> function applied to the <code>lm</code> model. This function generates two confidence intervals and the one in the second row is the one we are interested as it pertains to the difference in the true means of the two groups. The parametric 95% confidence interval here is from -51.6 to -0.26 cm which is a bit different in width from the nonparametric bootstrap interval that was from -50 and -2.25 cm.</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="chapter2.html#cb170-1"></a><span class="kw">confint</span>(lm1)</span></code></pre></div>
<pre><code>##                      2.5 %      97.5 %
## (Intercept)      117.64498 153.9550243
## Conditioncommute -51.60841  -0.2582517</code></pre>
<p>The bootstrap interval was narrower by almost 4 cm and its upper limit was much further from 0. The bootstrap CI can vary depending on the random number seed used and additional runs of the code produced intervals of (-49.6, -2.8), (-48.3, -2.5), and (-50.9, -1.1) so the differences between the parametric and nonparametric approaches was not just due to an unusual bootstrap distribution. It is not entirely clear why the two intervals differ but there are slightly more results in the left tail of Figure <a href="chapter2.html#fig:Figure2-25">2.25</a> than in the right tail and this shifts the 95% confidence slightly away from 0 as compared to the parametric approach. All intervals have the same interpretation, only the methods for calculating the
intervals and the assumptions differ. Specifically, the bootstrap interval can
tolerate different distribution shapes other than normal and still provide
intervals that work well<a href="#fn49" class="footnote-ref" id="fnref49"><sup>49</sup></a>. The other assumptions

are all the same as for the hypothesis
test, where we continue to assume that we have independent observations with
equal variances for the two groups and maintain concerns about inferences here due to the violation of independence in these responses.</p>
<p>The formula that <code>lm</code> is using to calculate the parametric
<strong><em>equal variance, two-sample <span class="math inline">\(t\)</span>-based confidence interval</em></strong> is:</p>
<p><span class="math display">\[\bar{x}_1 - \bar{x}_2 \mp t^*_{df}s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}\]</span></p>
<p>In this situation, the <em>df</em> is again <span class="math inline">\(n_1+n_2-2\)</span> (the total sample size - 2) and
<span class="math inline">\(s_p = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}\)</span>. The <span class="math inline">\(t^*_{df}\)</span> is
a multiplier that comes from finding the percentile from the <span class="math inline">\(t\)</span>-distribution
that puts <span class="math inline">\(C\)</span>% in the middle of the distribution with <span class="math inline">\(C\)</span> being the confidence
level. It is important to note that this <span class="math inline">\(t^*\)</span> has nothing to do with the previous
test statistic <span class="math inline">\(t\)</span>. It is confusing and students first engaging these two options often happily
take the result from a test statistic calculation and use it for a multiplier
in a <span class="math inline">\(t\)</span>-based confidence interval – try to focus on which <span class="math inline">\(t\)</span> you are interested in before you use either. Figure <a href="chapter2.html#fig:Figure2-26">2.26</a> shows the
<span class="math inline">\(t\)</span>-distribution with 28 degrees of freedom and the cut-offs that put 95% of the
area in the middle.</p>

<div class="figure"><span id="fig:Figure2-26"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-26-1.png" alt="Plot of \(t(28)\) with cut-offs for putting 95% of distribution in the middle that delineate the \(t^*\) multiplier to make a 95% confidence interval." width="480" />
<p class="caption">
Figure 2.26: Plot of <span class="math inline">\(t(28)\)</span> with cut-offs for putting 95% of distribution in the middle that delineate the <span class="math inline">\(t^*\)</span> multiplier to make a 95% confidence interval.
</p>
</div>
<p>For 95% confidence intervals, the multiplier is going to be close to 2 and
anything else is a likely indication of a mistake. We can use R to get the multipliers for
confidence intervals using the <code>qt</code> function in a similar fashion to how
<code>qdata</code> was used in the bootstrap results, except that this new value must be
used in the previous confidence interval formula. This function produces values
for requested percentiles, so if we want to put 95% in the middle, we place
2.5% in each tail of the distribution and need to request the 97.5<sup>th</sup>
percentile. Because the <span class="math inline">\(t\)</span>-distribution is always symmetric around 0, we merely
need to look up the value for the 97.5<sup>th</sup> percentile and know that the
multiplier for the 2.5<sup>th</sup> percentile is just <span class="math inline">\(-t^*\)</span>. The <span class="math inline">\(t^*\)</span>
multiplier to form the confidence interval is 2.0484 for a 95% confidence interval
when the <span class="math inline">\(df=28\)</span> based on the results from <code>qt</code>:</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="chapter2.html#cb172-1"></a><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">28</span>)</span></code></pre></div>
<pre><code>## [1] 2.048407</code></pre>
<p>Note that the 2.5<sup>th</sup> percentile is just the negative of this value due
to symmetry and the real source of the minus in the minus/plus in the formula
for the confidence interval.</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="chapter2.html#cb174-1"></a><span class="kw">qt</span>(<span class="fl">0.025</span>, <span class="dt">df=</span><span class="dv">28</span>)</span></code></pre></div>
<pre><code>## [1] -2.048407</code></pre>
<p>We can also re-write the confidence interval formula into a slightly more
general forms as</p>
<p><span class="math display">\[\bar{x}_1 - \bar{x}_2 \mp t^*_{df}SE_{\bar{x}_1 - \bar{x}_2}\ \text{ OR }\ 
\bar{x}_1 - \bar{x}_2 \mp ME\]</span></p>
<p>where <span class="math inline">\(SE_{\bar{x}_1 - \bar{x}_2} = s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}\)</span> and
<span class="math inline">\(ME = t^*_{df}SE_{\bar{x}_1 - \bar{x}_2}\)</span>. The <em>SE</em> is available in the <code>lm</code> model <code>summary</code> for the line related to the difference in groups in the “Std. Error” column. In some situations, researchers will
report the <strong><em>standard error</em></strong> (SE) or <strong><em>margin of error</em></strong> (ME) as a method
of quantifying the uncertainty in a statistic. The SE is an estimate of the
standard deviation of the statistic (here <span class="math inline">\(\bar{x}_1 - \bar{x}_2\)</span>) and the ME
is an estimate of the precision of a statistic that can be used to directly
form a confidence interval. The ME depends on the choice of confidence level
although 95% is almost always selected.</p>
<p>To finish this example, R can be used to help you do calculations much
like a calculator except with much more power “under the hood”. You have to
make sure you are careful with using <code>( )</code> to group items and remember that
the asterisk (*) is used for multiplication. We need the pertinent
information which is available from the <code>favstats</code> output repeated below to
calculate the confidence interval “by hand”<a href="#fn50" class="footnote-ref" id="fnref50"><sup>50</sup></a> using R.</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="chapter2.html#cb176-1"></a><span class="kw">favstats</span>(Distance <span class="op">~</span><span class="st"> </span>Condition, <span class="dt">data =</span> dsample)</span></code></pre></div>
<pre><code>##   Condition min    Q1 median    Q3 max     mean       sd  n missing
## 1    casual  72 112.5    143 154.5 208 135.8000 39.36133 15       0
## 2   commute  60  88.5    113 123.0 168 109.8667 28.41244 15       0</code></pre>
<p>Start with typing the following command to calculate <span class="math inline">\(s_p\)</span> and store it in a
variable named <code>sp</code>:</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="chapter2.html#cb178-1"></a>sp &lt;-<span class="st"> </span><span class="kw">sqrt</span>(((<span class="dv">15-1</span>)<span class="op">*</span>(<span class="fl">39.36133</span><span class="op">^</span><span class="dv">2</span>)<span class="op">+</span>(<span class="dv">15-1</span>)<span class="op">*</span>(<span class="fl">28.4124</span><span class="op">^</span><span class="dv">2</span>))<span class="op">/</span>(<span class="dv">15</span><span class="op">+</span><span class="dv">15-2</span>))</span>
<span id="cb178-2"><a href="chapter2.html#cb178-2"></a>sp</span></code></pre></div>
<pre><code>## [1] 34.32622</code></pre>
<p>Then calculate the confidence interval that <code>confint</code> provided using:</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="chapter2.html#cb180-1"></a><span class="fl">109.8667-135.8</span> <span class="op">+</span><span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">28</span>)<span class="op">*</span>sp<span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">15</span><span class="op">+</span><span class="dv">1</span><span class="op">/</span><span class="dv">15</span>)</span></code></pre></div>
<pre><code>## [1] -51.6083698  -0.2582302</code></pre>
<p>Or using the information from the model summary:</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="chapter2.html#cb182-1"></a><span class="fl">-25.933</span> <span class="op">+</span><span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">28</span>)<span class="op">*</span><span class="fl">12.534</span></span></code></pre></div>
<pre><code>## [1] -51.6077351  -0.2582649</code></pre>
<p>The previous results all use <code>c(-1, 1)</code> times the margin of error to subtract and add
the ME to the difference in the sample means (<span class="math inline">\(109.8667-135.8\)</span>), which generates the
lower and then upper bounds of the confidence interval. If desired, we can also
use just the last portion of the calculation to find the margin of error,
which is 25.675 here.</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="chapter2.html#cb184-1"></a><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">28</span>)<span class="op">*</span>sp<span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">15</span><span class="op">+</span><span class="dv">1</span><span class="op">/</span><span class="dv">15</span>)</span></code></pre></div>
<pre><code>## [1] 25.67507</code></pre>
<p>For the entire <span class="math inline">\(n=1,636\)</span> data set for these two groups, the results are obtained using the following code. The estimated difference in the means is -3 cm (<em>commute</em> minus <em>casual</em>). The <span class="math inline">\(t\)</span>-based 95% confidence interval is from -5.89 to -0.11.</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="chapter2.html#cb186-1"></a>lm_all &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>ddsub)</span>
<span id="cb186-2"><a href="chapter2.html#cb186-2"></a><span class="kw">confint</span>(lm_all) <span class="co">#Parametric 95% CI</span></span></code></pre></div>
<pre><code>##                       2.5 %      97.5 %
## (Intercept)      115.520697 119.7013823
## Conditioncommute  -5.891248  -0.1149621</code></pre>
<p>The bootstrap 95% confidence interval is from -5.82 to -0.076. With this large data set, the differences between parametric and permutation approaches decrease and they essentially equivalent here. The bootstrap distribution (not displayed) for the differences in the sample means is relatively symmetric and centered around the estimated difference of -3 cm. So using all the observations we would be 95% confident that the true mean difference in overtake distances (<em>commute</em> - <em>casual</em>) is between -5.89 and -0.11 cm, providing additional information about the estimated difference in the sample means of -3 cm.</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="chapter2.html#cb188-1"></a>Tobs &lt;-<span class="st"> </span><span class="kw">coef</span>(lm_all)[<span class="dv">2</span>]; Tobs</span></code></pre></div>
<pre><code>## Conditioncommute 
##        -3.003105</code></pre>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="chapter2.html#cb190-1"></a>B &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb190-2"><a href="chapter2.html#cb190-2"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb190-3"><a href="chapter2.html#cb190-3"></a>Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</span>
<span id="cb190-4"><a href="chapter2.html#cb190-4"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</span>
<span id="cb190-5"><a href="chapter2.html#cb190-5"></a>  lmP &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span><span class="kw">resample</span>(ddsub))</span>
<span id="cb190-6"><a href="chapter2.html#cb190-6"></a>  Tstar[b] &lt;-<span class="st"> </span><span class="kw">coef</span>(lmP)[<span class="dv">2</span>]</span>
<span id="cb190-7"><a href="chapter2.html#cb190-7"></a>}</span></code></pre></div>
<!-- \newpage -->
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="chapter2.html#cb191-1"></a><span class="kw">qdata</span>(Tstar, <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span></code></pre></div>
<pre><code>##          quantile     p
## 2.5%  -5.81626474 0.025
## 97.5% -0.07606663 0.975</code></pre>
</div>
<div id="section2-10" class="section level2">
<h2><span class="header-section-number">2.10</span> Bootstrap confidence intervals for difference in GPAs</h2>
<p>We can now apply the new confidence interval methods on the STAT 217 grade data.
This time we start with the parametric 95% confidence interval “by hand” in R
and then use <code>lm</code> to verify our result. The <code>favstats</code> output provides
us with the required information to calculate the confidence interval, with the estimated difference in the sample mean GPAs of <span class="math inline">\(3.338-3.0886 = 0.2494\)</span>:</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="chapter2.html#cb193-1"></a><span class="kw">favstats</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217)</span></code></pre></div>
<pre><code>##   Sex  min  Q1 median   Q3 max     mean        sd  n missing
## 1   F 2.50 3.1  3.400 3.70   4 3.338378 0.4074549 37       0
## 2   M 1.96 2.8  3.175 3.46   4 3.088571 0.4151789 42       0</code></pre>
<p>The <span class="math inline">\(df\)</span> are <span class="math inline">\(37+42-2 = 77\)</span>. Using the SDs from the two groups and their sample
sizes, we can calculate <span class="math inline">\(s_p\)</span>:</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="chapter2.html#cb195-1"></a>sp &lt;-<span class="st"> </span><span class="kw">sqrt</span>(((<span class="dv">37-1</span>)<span class="op">*</span>(<span class="fl">0.4075</span><span class="op">^</span><span class="dv">2</span>)<span class="op">+</span>(<span class="dv">42-1</span>)<span class="op">*</span>(<span class="fl">0.41518</span><span class="op">^</span><span class="dv">2</span>))<span class="op">/</span>(<span class="dv">37</span><span class="op">+</span><span class="dv">42-2</span>))</span>
<span id="cb195-2"><a href="chapter2.html#cb195-2"></a>sp</span></code></pre></div>
<pre><code>## [1] 0.4116072</code></pre>
<p>The margin of error is:</p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="chapter2.html#cb197-1"></a><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">77</span>)<span class="op">*</span>sp<span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">37</span><span class="op">+</span><span class="dv">1</span><span class="op">/</span><span class="dv">42</span>)</span></code></pre></div>
<pre><code>## [1] 0.1847982</code></pre>
<div style="page-break-after: always;"></div>
<p>All together, the 95% confidence interval is:</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="chapter2.html#cb199-1"></a><span class="fl">3.338-3.0886</span><span class="op">+</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">77</span>)<span class="op">*</span>sp<span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">37</span><span class="op">+</span><span class="dv">1</span><span class="op">/</span><span class="dv">42</span>)</span></code></pre></div>
<pre><code>## [1] 0.0646018 0.4341982</code></pre>
<p>So we are 95% confident that the difference in the true mean GPAs between
females and males (females minus males) is between 0.065 and 0.434 GPA points.
We get a similar result from <code>confint</code> on <code>lm</code>, except that <code>lm</code> switched the direction of the comparison from what was done “by hand” above, with the estimated mean difference of -0.25 GPA points (male - female) and similarly switched CI:</p>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="chapter2.html#cb201-1"></a>lm_GPA &lt;-<span class="st"> </span><span class="kw">lm</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217)</span>
<span id="cb201-2"><a href="chapter2.html#cb201-2"></a><span class="kw">summary</span>(lm_GPA)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = GPA ~ Sex, data = s217)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.12857 -0.28857  0.06162  0.36162  0.91143 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  3.33838    0.06766  49.337  &lt; 2e-16
## SexM        -0.24981    0.09280  -2.692  0.00871
## 
## Residual standard error: 0.4116 on 77 degrees of freedom
## Multiple R-squared:  0.08601,    Adjusted R-squared:  0.07414 
## F-statistic: 7.246 on 1 and 77 DF,  p-value: 0.008713</code></pre>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="chapter2.html#cb203-1"></a><span class="kw">confint</span>(lm_GPA)</span></code></pre></div>
<pre><code>##                  2.5 %      97.5 %
## (Intercept)  3.2036416  3.47311517
## SexM        -0.4345955 -0.06501838</code></pre>
<p>Note that we can easily switch to 90% or 99% confidence intervals by simply
changing the percentile in <code>qt</code> or changing the <code>level</code> option in the
<code>confint</code> function.</p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="chapter2.html#cb205-1"></a><span class="kw">qt</span>(<span class="fl">0.95</span>, <span class="dt">df=</span><span class="dv">77</span>) <span class="co">#For 90% confidence and 77 df</span></span></code></pre></div>
<pre><code>## [1] 1.664885</code></pre>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="chapter2.html#cb207-1"></a><span class="kw">qt</span>(<span class="fl">0.995</span>, <span class="dt">df=</span><span class="dv">77</span>) <span class="co">#For 99% confidence and 77 df</span></span></code></pre></div>
<pre><code>## [1] 2.641198</code></pre>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="chapter2.html#cb209-1"></a><span class="kw">confint</span>(lm_GPA, <span class="dt">level=</span><span class="fl">0.9</span>) <span class="co">#90% confidence interval</span></span></code></pre></div>
<pre><code>##                    5 %        95 %
## (Intercept)  3.2257252  3.45103159
## SexM        -0.4043084 -0.09530553</code></pre>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="chapter2.html#cb211-1"></a><span class="kw">confint</span>(lm_GPA, <span class="dt">level=</span><span class="fl">0.99</span>) <span class="co">#99% confidence interval</span></span></code></pre></div>
<pre><code>##                  0.5 %       99.5 %
## (Intercept)  3.1596636  3.517093108
## SexM        -0.4949103 -0.004703598</code></pre>
<p>As a review of some basic ideas with confidence intervals make sure
you can answer the following questions:</p>
<ol style="list-style-type: decimal">
<li><p>What is the impact of increasing the confidence level in this situation?</p></li>
<li><p>What happens to the width of the confidence interval if the size of the
SE increases or decreases?</p></li>
<li><p>What about increasing the sample size – should that increase or decrease
the width of the interval?</p></li>
</ol>
<p>All the general results you learned before about impacts to widths of CIs hold
in this situation whether we are considering the parametric or bootstrap methods…</p>
<p>To finish this example, we will generate the comparable bootstrap 90%
confidence interval using the bootstrap distribution in
Figure <a href="chapter2.html#fig:Figure2-27">2.27</a>.</p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="chapter2.html#cb213-1"></a>Tobs &lt;-<span class="st"> </span><span class="kw">coef</span>(lm_GPA)[<span class="dv">2</span>]; Tobs</span></code></pre></div>
<pre><code>##       SexM 
## -0.2498069</code></pre>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="chapter2.html#cb215-1"></a>B &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb215-2"><a href="chapter2.html#cb215-2"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb215-3"><a href="chapter2.html#cb215-3"></a>Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</span>
<span id="cb215-4"><a href="chapter2.html#cb215-4"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</span>
<span id="cb215-5"><a href="chapter2.html#cb215-5"></a>  lmP &lt;-<span class="st"> </span><span class="kw">lm</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span><span class="kw">resample</span>(s217))</span>
<span id="cb215-6"><a href="chapter2.html#cb215-6"></a>  Tstar[b] &lt;-<span class="st"> </span><span class="kw">coef</span>(lmP)[<span class="dv">2</span>]</span>
<span id="cb215-7"><a href="chapter2.html#cb215-7"></a>}</span>
<span id="cb215-8"><a href="chapter2.html#cb215-8"></a></span>
<span id="cb215-9"><a href="chapter2.html#cb215-9"></a>quantiles &lt;-<span class="st"> </span><span class="kw">qdata</span>(Tstar, <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>))</span>
<span id="cb215-10"><a href="chapter2.html#cb215-10"></a>quantiles</span></code></pre></div>
<pre><code>##        quantile    p
## 5%  -0.39290566 0.05
## 95% -0.09622185 0.95</code></pre>
<p>The output tells us that the 90% confidence interval is from -0.393 to -0.096 GPA
points. The bootstrap distribution with the observed difference in the sample
means and these cut-offs is displayed in Figure <a href="chapter2.html#fig:Figure2-27">2.27</a> using
this code:</p>

<div class="figure"><span id="fig:Figure2-27"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-27-1.png" alt="Histogram and density curve of bootstrap distribution of difference in sample mean GPAs (male minus female) with observed difference (solid vertical line) and quantiles that delineate the 90% confidence intervals (dashed vertical lines)." width="960" />
<p class="caption">
Figure 2.27: Histogram and density curve of bootstrap distribution of difference in sample mean GPAs (male minus female) with observed difference (solid vertical line) and quantiles that delineate the 90% confidence intervals (dashed vertical lines).
</p>
</div>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="chapter2.html#cb217-1"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb217-2"><a href="chapter2.html#cb217-2"></a><span class="kw">hist</span>(Tstar,<span class="dt">labels=</span>T)</span>
<span id="cb217-3"><a href="chapter2.html#cb217-3"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lwd=</span><span class="dv">2</span>)</span>
<span id="cb217-4"><a href="chapter2.html#cb217-4"></a><span class="kw">abline</span>(<span class="dt">v=</span>quantiles<span class="op">$</span>quantile,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>,<span class="dt">lty=</span><span class="dv">2</span>)</span>
<span id="cb217-5"><a href="chapter2.html#cb217-5"></a><span class="kw">plot</span>(<span class="kw">density</span>(Tstar),<span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</span>
<span id="cb217-6"><a href="chapter2.html#cb217-6"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lwd=</span><span class="dv">2</span>)</span>
<span id="cb217-7"><a href="chapter2.html#cb217-7"></a><span class="kw">abline</span>(<span class="dt">v=</span>quantiles<span class="op">$</span>quantile,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>,<span class="dt">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<p>In the previous output, the parametric 90% confidence interval is from
-0.404 to -0.095, suggesting similar results again from the two approaches. Based on
the bootstrap CI, we can say that we are 90% confident that the difference in
the true mean GPAs for STAT 217 students is between -0.393 to -0.094 GPA points
(male minus females). This result would be usefully added to step 5 in the 6+ steps of the hypothesis testing protocol with an updated result of:</p>
<ol start="5" style="list-style-type: decimal">
<li><p>Report and discuss an estimate of the size of the differences, with confidence interval(s) if appropriate. </p>
<ul>
<li>Females were estimated to have a higher mean GPA by 0.25 points (<em>90% bootstrap confidence interval: 0.094 to 0.393</em>). This difference of 0.25 on a GPA scale does not seem like a very large difference in the means even though we were able to detect a difference in the groups.</li>
</ul></li>
</ol>
<p>Throughout the text, pay attention to the distinctions between parameters
and statistics, focusing on the differences between estimates based on the sample
and inferences for the population of interest in the form of the parameters of
interest. Remember that statistics are summaries of the sample information and
parameters are characteristics of populations (which we rarely know). And that
our inferences are limited to the population that we randomly sampled from, if
we randomly sampled.</p>
</div>
<div id="section2-11" class="section level2">
<h2><span class="header-section-number">2.11</span> Chapter summary</h2>
<p>In this chapter, we reviewed basic statistical inference methods in the context
of a two-sample mean problem using linear models and the <code>lm</code> function. You were introduced to using R to do enhanced visualizations (pirate-plots), permutation
testing, and generate bootstrap confidence intervals as well as obtaining
parametric <span class="math inline">\(t\)</span>-test and confidence intervals. You should
have learned how to use a <code>for</code> loop for doing the nonparametric inferences
and the <code>lm</code> and <code>confint</code> functions for generating parametric inferences. In the examples considered, the parametric and nonparametric methods provided similar
results, suggesting that the assumptions were not too violated for the parametric procedures. When parametric and nonparametric approaches
disagree, the nonparametric methods are likely to be more trustworthy since
they have less restrictive assumptions but can still make assumptions and can have problems.
</p>
<p>When the noted conditions are violated in a hypothesis testing situation, the
Type I error  rates can be inflated, meaning that we reject the null hypothesis
more often than we have allowed to occur by chance. Specifically, we could have
a situation where our assumed 5% significance level test might actually reject
the null when it is true 20% of the time. If this is occurring, we call a
procedure <strong><em>liberal</em></strong> (it rejects too easily) and if the procedure is liberal,
how could we trust a small p-value to be a “real” result and not just an
artifact of violating the assumptions of the procedure? Likewise, for
confidence intervals we hope that our 95% confidence level procedure, when
repeated, will contain the true parameter 95% of the time. If our assumptions
are violated, we might actually have an 80% confidence level procedure and it
makes it hard to trust the reported results for our observed data set.
Statistical inference relies on a belief in the methods underlying our
inferences. If we don’t trust our assumptions, we shouldn’t trust the
conclusions to perform the way we want them to. As sample sizes increase
and/or violations of conditions lessen, then the procedures will perform better.
In Chapter <a href="chapter3.html#chapter3">3</a>, some new tools for doing diagnostics are introduced
to help us assess how and how much those validity conditions are violated.</p>
<p>It is good to review how to report hypothesis test conclusions and compare those for when we have strong, moderate, or weak evidence. Suppose that we are doing parametric inferences with <code>lm</code> for differences between groups A and B, are extracting the <span class="math inline">\(t\)</span>-statistics, have 15 degrees of freedom, and obtain the following test statistics and p-values:</p>
<ul>
<li><p><span class="math inline">\(t_{15}=3.5\)</span>, p-value=0.0016: There is strong evidence against the null hypothesis of no difference in the true means of the response between A and B (<span class="math inline">\(t_{15}=3.5\)</span>, p-value=0.0016), so we would conclude that there is a difference in the true means.</p></li>
<li><p><span class="math inline">\(t_{15}=1.75\)</span>, p-value=0.0503: There is moderate evidence against the null hypothesis of no difference in the true means of the response between A and B (<span class="math inline">\(t_{15}=1.75\)</span>, p-value=0.0503), so we would conclude that there is likely<a href="#fn51" class="footnote-ref" id="fnref51"><sup>51</sup></a> a difference in the true means.</p></li>
<li><p><span class="math inline">\(t_{15}=0.75\)</span>, p-value=0.232: There is weak evidence against the null hypothesis of no difference in the true means of the response between A and B (<span class="math inline">\(t_{15}=1.75\)</span>, p-value=0.0503), so we would conclude that there is likely not a difference in the true means.</p></li>
</ul>
<p>The last conclusion also suggests an action to take when we encounter weak evidence against null hypotheses – we could potentially model the responses using the null model since we couldn’t prove it was wrong. We would take this action knowing that we could be wrong, but the “simpler” model that the null hypothesis suggests is often an attractive option in very complex models, such as what we are going to encounter in the coming chapters, especially in Chapters <a href="chapter5.html#chapter5">5</a> and <a href="chapter8.html#chapter8">8</a>.</p>
</div>
<div id="section2-12" class="section level2">
<h2><span class="header-section-number">2.12</span> Summary of important R code</h2>
<p>The main components of R code used in this chapter follow with components to
modify in lighter and/or ALL CAPS text, remembering that any R packages mentioned
need to be installed and loaded for this code to have a chance of working:</p>
<ul>
<li><p><strong>summary(<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li>Provides numerical summaries of all variables in the data set.
</li>
</ul></li>
<li><p><strong>summary(lm(<font color='red'>Y</font> ~ <font color='red'>X</font>,
data=<font color='red'>DATASETNAME</font>))</strong></p>
<ul>
<li>Provides estimate, SE, test statistic, and p-value for difference in second row of coefficient table. </li>
</ul></li>
<li><p><strong>confint(lm(<font color='red'>Y</font> ~ <font color='red'>X</font>,
data=<font color='red'>DATASETNAME</font>), level=0.95)</strong></p>
<ul>
<li>Provides 95%
confidence interval for difference in second row of output. </li>
</ul></li>
<li><p><strong>2<code>*</code>pt(abs(<font color='red'>Tobs</font>), df=<font color='red'>DF</font>, lower.tail=F)</strong></p>
<ul>
<li>Finds the two-sided test p-value for an observed 2-sample t-test
statistic of <code>Tobs</code>. </li>
</ul></li>
<li><p><strong>hist(<font color='red'>DATASETNAME$Y</font>)</strong></p>
<ul>
<li>Makes a histogram of a variable named <code>Y</code> from the data set of
interest.</li>
</ul></li>
<li><p><strong>boxplot(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li>Makes a boxplot of a variable named Y for groups in X from the data set.</li>
</ul></li>
<li><p><strong>pirateplot(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>, inf.method=“ci”, inf.disp=“line”)</strong></p>
<ul>
<li><p>Requires the <code>yarrr</code> package is loaded.</p></li>
<li><p>Makes a pirate-plot of a variable named Y for groups in X from the data set with estimated means and 95% confidence intervals for each group. </p></li>
<li><p>Add <code>theme=2</code> if the confidence intervals extend outside the density curves and you can’t see how far they extend.</p></li>
</ul></li>
<li><p><strong>mean(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>); sd(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li><p>This usage of <code>mean</code> and <code>sd</code> requires the <code>mosaic</code> package.</p></li>
<li><p>Provides the mean and sd of responses of Y for each group described in X.</p></li>
</ul></li>
<li><p><strong>favstats(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li>Provides numerical summaries of Y by groups described in X.</li>
</ul></li>
<li><p><strong>Tobs <code>&lt;-</code> coef(lm(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>))[2]; Tobs<br />
B <code>&lt;-</code> 1000<br />
Tstar <code>&lt;-</code> matrix(NA, nrow=B)<br />
for (b in (1:B)){<br />
lmP <code>&lt;-</code> lm(<font color='red'>Y</font>~shuffle(<font color='red'>X</font>), data=<font color='red'>DATASETNAME</font>)<br />
Tstar[b] <code>&lt;-</code> coef(lmP)[2]<br />
}</strong></p>
<ul>
<li>Code to run a <code>for</code> loop to generate 1000 permuted versions of the test
statistic using the <code>shuffle</code> function and keep track of the results in
<code>Tstar</code></li>
</ul></li>
<li><p><strong>pdata(Tstar, abs(<font color='red'>Tobs</font>), lower.tail=F)[[1]]</strong></p>
<ul>
<li>Finds the proportion of the permuted test statistics in Tstar that are
less than -|Tobs| or greater than |Tobs|, useful for finding the two-sided
test p-value. </li>
</ul></li>
</ul>
<!-- \newpage -->
<ul>
<li><p><strong>Tobs <code>&lt;-</code> coef(lm(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>))[2]; Tobs<br />
B <code>&lt;-</code> 1000<br />
Tstar <code>&lt;-</code> matrix(NA, nrow=B)<br />
for (b in (1:B)){<br />
lmP <code>&lt;-</code> lm(<font color='red'>Y</font>~<font color='red'>X</font>, data=resample(<font color='red'>DATASETNAME</font>))<br />
Tstar[b] <code>&lt;-</code> coef(lmP)[2]<br />
}</strong></p>
<ul>
<li>Code to run a <code>for</code> loop to generate 1000 bootstrapped versions of the
data set using the <code>resample</code> function and keep track of the results of
the statistic in <code>Tstar</code>.</li>
</ul></li>
<li><p><strong>qdata(Tstar, c(0.025, 0.975))</strong></p>
<ul>
<li>Provides the values that delineate the middle 95% of the results in the
bootstrap distribution (<code>Tstar</code>). </li>
</ul></li>
</ul>
<div style="page-break-after: always;"></div>
</div>
<div id="section2-13" class="section level2">
<h2><span class="header-section-number">2.13</span> Practice problems</h2>
<p>2.1. <strong>Overtake Distance Analysis</strong> The tests for the overtake distance data were performed with two-sided alternatives and so two-sided areas used to find the p-values. Suppose that the researchers expected that the average passing distance would be less (closer) for the commute clothing than for the casual clothing group. Repeat obtaining the permutation-based p-value for the one-sided test for either the full or smaller sample data set. Hint: Your p-value should be just about half of what it was before and in the direction of the alternative.</p>

<p>2.2. <strong>HELP Study Data Analysis</strong> Load the <code>HELPrct</code> data set from the <code>mosaicData</code> package <span class="citation">(Pruim, Kaplan, and Horton <a href="#ref-R-mosaicData" role="doc-biblioref">2018</a>)</span>
(you need to
install the <code>mosaicData</code> package once to be able to load it). The HELP study
was a clinical trial for adult inpatients recruited from a
detoxification unit. Patients with no primary care physician were randomly
assigned to receive a multidisciplinary assessment and a brief motivational
intervention or usual care and various outcomes were observed. Two of the
variables in the data set are <code>sex</code>, a factor with levels <em>male</em> and <em>female</em>
and <code>daysanysub</code> which is the time (in days) to first use of any substance
post-detox. We are interested in the difference in mean number of days to first
use of any substance post-detox between males and females. There are some
missing responses and the following code will produce <code>favstats</code> with the
missing values and then provide a data set that by
applying the <code>na.omit</code> function removes any observations with missing
values.
</p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="chapter2.html#cb218-1"></a><span class="kw">library</span>(mosaicData)</span>
<span id="cb218-2"><a href="chapter2.html#cb218-2"></a><span class="kw">data</span>(HELPrct)</span>
<span id="cb218-3"><a href="chapter2.html#cb218-3"></a>HELPrct2 &lt;-<span class="st"> </span>HELPrct[, <span class="kw">c</span>(<span class="st">&quot;daysanysub&quot;</span>, <span class="st">&quot;sex&quot;</span>)] <span class="co">#Just focus on two variables</span></span>
<span id="cb218-4"><a href="chapter2.html#cb218-4"></a>HELPrct3 &lt;-<span class="st"> </span><span class="kw">na.omit</span>(HELPrct2) <span class="co">#Removes subjects with missing values</span></span>
<span id="cb218-5"><a href="chapter2.html#cb218-5"></a><span class="kw">favstats</span>(daysanysub<span class="op">~</span>sex, <span class="dt">data=</span>HELPrct2)</span>
<span id="cb218-6"><a href="chapter2.html#cb218-6"></a><span class="kw">favstats</span>(daysanysub<span class="op">~</span>sex, <span class="dt">data=</span>HELPrct3)</span></code></pre></div>
<p>2.2.1. Based on the results provided, how many observations were missing for males
and females? Missing values here likely mean that the subjects didn’t use any
substances post-detox in the time of the study but might have at a later
date – the study just didn’t run long enough. This is called <strong><em>censoring</em></strong>.
What is the problem with the numerical summaries here if the missing responses
were all something larger than the largest observation?</p>
<p>2.2.2. Make a pirate-plot and a boxplot of <code>daysanysub</code> ~ <code>sex</code> using the
<code>HELPrct3</code> data set created above. Compare the distributions, recommending
parametric or nonparametric inferences.</p>
<p>2.2.3. Generate the permutation results and write out the 6+ steps of the
hypothesis test.</p>
<p>2.2.4. Interpret the p-value for these results.</p>
<p>2.2.5. Generate the parametric test results using <code>lm</code>, reporting the test-statistic,
its distribution under the null hypothesis, and compare the p-value to those
observed using the permutation approach.</p>
<p>2.2.6. Make and interpret a 95% bootstrap confidence interval for the difference
in the means.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Bland1995">
<p>Bland, J Martin, and Douglas G Altman. 1995. “Multiple Significance Tests: The Bonferroni Method.” <em>BMJ</em> 310 (6973): 170. <a href="https://doi.org/10.1136/bmj.310.6973.170">https://doi.org/10.1136/bmj.310.6973.170</a>.</p>
</div>
<div id="ref-Phillips2017">
<p>Phillips, Nathaniel. 2017a. <em>Yarrr: A Companion to the E-Book "Yarrr!: The Pirate’s Guide to R"</em>. <a href="https://CRAN.R-project.org/package=yarrr">https://CRAN.R-project.org/package=yarrr</a>.</p>
</div>
<div id="ref-R-yarrr">
<p>Phillips, Nathaniel. 2017b. <em>Yarrr: A Companion to the E-Book "Yarrr!: The Pirate’s Guide to R"</em>. <a href="https://CRAN.R-project.org/package=yarrr">https://CRAN.R-project.org/package=yarrr</a>.</p>
</div>
<div id="ref-R-mosaicData">
<p>Pruim, Randall, Daniel Kaplan, and Nicholas Horton. 2018. <em>MosaicData: Project Mosaic Data Sets</em>. <a href="https://CRAN.R-project.org/package=mosaicData">https://CRAN.R-project.org/package=mosaicData</a>.</p>
</div>
<div id="ref-R-mosaic">
<p>Pruim, Randall, Daniel T. Kaplan, and Nicholas J. Horton. 2019. <em>Mosaic: Project Mosaic Statistics and Mathematics Teaching Utilities</em>. <a href="https://CRAN.R-project.org/package=mosaic">https://CRAN.R-project.org/package=mosaic</a>.</p>
</div>
<div id="ref-Schneck2017">
<p>Schneck, Andreas. 2017. “Examining Publication Bias—a Simulation-Based Evaluation of Statistical Tests on Publication Bias.” <em>PeerJ</em> 5 (November): e4115. <a href="https://doi.org/10.7717/peerj.4115">https://doi.org/10.7717/peerj.4115</a>.</p>
</div>
<div id="ref-Smith2014">
<p>Smith, Michael L. 2014. “Honey Bee Sting Pain Index by Body Location.” <em>PeerJ</em> 2 (April): e338. <a href="https://doi.org/10.7717/peerj.338">https://doi.org/10.7717/peerj.338</a>.</p>
</div>
<div id="ref-Walker2014">
<p>Walker, Ian, Ian Garrard, and Felicity Jowitt. 2014. “The Influence of a Bicycle Commuter’s Appearance on Drivers’ Overtaking Proximities: An on-Road Test of Bicyclist Stereotypes, High-Visibility Clothing and Safety Aids in the United Kingdom.” <em>Accident Analysis U+0026 Prevention</em> 64: 69–77. <a href="https://doi.org/https://doi.org/10.1016/j.aap.2013.11.007">https://doi.org/https://doi.org/10.1016/j.aap.2013.11.007</a>.</p>
</div>
<div id="ref-Wasserstein2016">
<p>Wasserstein, Ronald L., and Nicole A. Lazar. 2016. “The Asa Statement on P-Values: Context, Process, and Purpose.” <em>The American Statistician</em> 70 (2): 129–33. <a href="https://doi.org/10.1080/00031305.2016.1154108">https://doi.org/10.1080/00031305.2016.1154108</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="12">
<li id="fn12"><p>Either try to remember “data is a plural word” or replace “data” with “things” in your sentence and consider whether it sounds right.<a href="chapter2.html#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>Of particular interest to the bicycle rider might be the “close” passes and we will revisit this as a categorical response with “close” and “not close” as its two categories later.<a href="chapter2.html#fnref13" class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p>Thanks to Ian Walker for allowing me to use and post these data.<a href="chapter2.html#fnref14" class="footnote-back">↩︎</a></p></li>
<li id="fn15"><p>As noted previously, we reserve the term “effect” for situations where random assignment  allows us to consider causality as the reason for the differences in the response variable among levels of the explanatory variable.<a href="chapter2.html#fnref15" class="footnote-back">↩︎</a></p></li>
<li id="fn18"><p>If you want to type this character in Rmarkdown, try <code>$\sim$</code> outside of codechunks.<a href="chapter2.html#fnref18" class="footnote-back">↩︎</a></p></li>
<li id="fn20"><p>The package and function are intentionally amusingly titled but are based on ideas in the beanplot in <span class="citation">Kampstra (<a href="#ref-Kampstra2008" role="doc-biblioref">2008</a>)</span> and provide what they call an <strong><em>RDI graphic</em></strong> - <strong><em>R</em></strong>aw data, <strong><em>D</em></strong>escriptive, and <strong><em>I</em></strong>nferential statistic in the same display.<a href="chapter2.html#fnref20" class="footnote-back">↩︎</a></p></li>
<li id="fn21"><p>The default version seems to get mis-interpreted as the box from a boxplot too easily. This display choice also matches our later plots for confidence intervals in term-plots.<a href="chapter2.html#fnref21" class="footnote-back">↩︎</a></p></li>
<li id="fn24"><p>Later we will shuffle other types of explanatory variables.<a href="chapter2.html#fnref24" class="footnote-back">↩︎</a></p></li>
<li id="fn25"><p>While not required, we often set our random number seed using the <code>set.seed</code> function so that when we re-run code with randomization in it we get the same results. <a href="chapter2.html#fnref25" class="footnote-back">↩︎</a></p></li>
<li id="fn27"><p>This is a bit like getting a new convertible sports car and driving it to the grocery store – there might be better ways to get groceries, but we probably would want to drive our new car as soon as we got it.<a href="chapter2.html#fnref27" class="footnote-back">↩︎</a></p></li>
<li id="fn28"><p>This will be formalized and explained more in the next chapter when we encounter more than two groups in these same models. For now, it is recommended to start with the sample means from <code>favstats</code> for the two groups and then use that to sort out which direction the differencing was done in the <code>lm</code> output.<a href="chapter2.html#fnref28" class="footnote-back">↩︎</a></p></li>
<li id="fn30"><p>In statistics, vectors are one dimensional lists of numeric elements – basically a column from a matrix of our tibble.<a href="chapter2.html#fnref30" class="footnote-back">↩︎</a></p></li>
<li id="fn34"><p>Both approaches are reasonable. By using both tails of the distribution we can incorporate potential differences in shape in both tails of the permutation distribution.<a href="chapter2.html#fnref34" class="footnote-back">↩︎</a></p></li>
<li id="fn35"><p>P-values of 1 are the only result that provide no evidence against the null hypothesis but this still doesn’t prove that the null hypothesis is true.<a href="chapter2.html#fnref35" class="footnote-back">↩︎</a></p></li>
<li id="fn37"><p>The <code>t.test</code> function with the <code>var.equal=T</code> option is the more direct route to calculating this statistic (here that would be <code>t.test(Distance~Condition, data=dsamp, var.equal=T)</code>), but since we can get the result of interest by fitting a linear model, we will use that approach.<a href="chapter2.html#fnref37" class="footnote-back">↩︎</a></p></li>
<li id="fn40"><p>At this level, it is critical to learn the tools and learn where they might provide inaccurate inferences. If you explore more advanced statistical resources, you will encounter methods that can allow you to obtain valid inferences in even more scenarios.<a href="chapter2.html#fnref40" class="footnote-back">↩︎</a></p></li>
<li id="fn41"><p>Only male and female were provided as options on the survey. These data were collected as part of a project to study learning of material using online versus paper versions of the book but we focus just on the gender differences in GPA here.<a href="chapter2.html#fnref41" class="footnote-back">↩︎</a></p></li>
<li id="fn42"><p>The data are provided and briefly discussed in the Practice Problems for Chapter <a href="chapter3.html#chapter3">3</a>.<a href="chapter2.html#fnref42" class="footnote-back">↩︎</a></p></li>
<li id="fn44"><p>An unbiased estimator  is a statistic that is on average equal to the population parameter.<a href="chapter2.html#fnref44" class="footnote-back">↩︎</a></p></li>
<li id="fn45"><p>Some perform bootstrap sampling in this situation by re-sampling within each of the groups. We will discuss using this technique in situations without clearly defined groups, so prefer to sample with replacement from the entire data set. It also directly corresponds to situations where the data came from one large sample and then the grouping variable of interest was measured on the <span class="math inline">\(n\)</span> subjects.<a href="chapter2.html#fnref45" class="footnote-back">↩︎</a></p></li>
<li id="fn47"><p>In any bootstrap sample, about 1/3 of the observations are not used at all.<a href="chapter2.html#fnref47" class="footnote-back">↩︎</a></p></li>
<li id="fn51"><p>Note that this modifier is added to note less certainty than when we encounter strong evidence against the null. Also note that someone else might decide that this more like weak evidence against the null and might choose to interpret it as in the “weak” case. In cases that are near boundaries for evidence levels, it becomes difficult to find a universal answer and it is best to report that the evidence is both not strong and not weak and is somewhere in between and let the reader decide what they think it means to them. This is complicated by often needing to make decisions about next steps based on p-values where we might choose to focus on the model with a difference or without it.<a href="chapter2.html#fnref51" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter3.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Greenwood_Book.pdf"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
