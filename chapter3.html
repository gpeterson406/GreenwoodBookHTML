<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 One-Way ANOVA | Intermediate Statistics with R</title>
  <meta name="description" content="Chapter 3 One-Way ANOVA | Intermediate Statistics with R" />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 One-Way ANOVA | Intermediate Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="gpeterson406/Greenwood_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 One-Way ANOVA | Intermediate Statistics with R" />
  
  
  

<meta name="author" content="Mark C Greenwood" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter2.html"/>
<link rel="next" href="chapter4.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="font-awesome.min.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intermediate Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#section1-1"><i class="fa fa-check"></i><b>1.1</b> Overview of methods</a></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#section1-2"><i class="fa fa-check"></i><b>1.2</b> Getting started in R</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#section1-3"><i class="fa fa-check"></i><b>1.3</b> Basic summary statistics, histograms, and boxplots using R</a></li>
<li class="chapter" data-level="1.4" data-path="chapter1.html"><a href="chapter1.html#section1-4"><i class="fa fa-check"></i><b>1.4</b> Chapter summary</a></li>
<li class="chapter" data-level="1.5" data-path="chapter1.html"><a href="chapter1.html#section1-5"><i class="fa fa-check"></i><b>1.5</b> Summary of important R code</a></li>
<li class="chapter" data-level="1.6" data-path="chapter1.html"><a href="chapter1.html#section1-6"><i class="fa fa-check"></i><b>1.6</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> (R)e-Introduction to statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#section2-1"><i class="fa fa-check"></i><b>2.1</b> Histograms, boxplots, and density curves</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#section2-2"><i class="fa fa-check"></i><b>2.2</b> Pirate-plots</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#section2-3"><i class="fa fa-check"></i><b>2.3</b> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#section2-4"><i class="fa fa-check"></i><b>2.4</b> Permutation testing for the two sample mean situation</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#section2-5"><i class="fa fa-check"></i><b>2.5</b> Hypothesis testing (general)</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#section2-6"><i class="fa fa-check"></i><b>2.6</b> Connecting randomization (nonparametric) and parametric tests</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#section2-7"><i class="fa fa-check"></i><b>2.7</b> Second example of permutation tests</a></li>
<li class="chapter" data-level="2.8" data-path="chapter2.html"><a href="chapter2.html#section2-8"><i class="fa fa-check"></i><b>2.8</b> Reproducibility Crisis: Moving beyond p &lt; 0.05, publication bias, and multiple testing issues</a></li>
<li class="chapter" data-level="2.9" data-path="chapter2.html"><a href="chapter2.html#section2-9"><i class="fa fa-check"></i><b>2.9</b> Confidence intervals and bootstrapping</a></li>
<li class="chapter" data-level="2.10" data-path="chapter2.html"><a href="chapter2.html#section2-10"><i class="fa fa-check"></i><b>2.10</b> Bootstrap confidence intervals for difference in GPAs</a></li>
<li class="chapter" data-level="2.11" data-path="chapter2.html"><a href="chapter2.html#section2-11"><i class="fa fa-check"></i><b>2.11</b> Chapter summary</a></li>
<li class="chapter" data-level="2.12" data-path="chapter2.html"><a href="chapter2.html#section2-12"><i class="fa fa-check"></i><b>2.12</b> Summary of important R code</a></li>
<li class="chapter" data-level="2.13" data-path="chapter2.html"><a href="chapter2.html#section2-13"><i class="fa fa-check"></i><b>2.13</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> One-Way ANOVA</a><ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#section3-1"><i class="fa fa-check"></i><b>3.1</b> Situation</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#section3-2"><i class="fa fa-check"></i><b>3.2</b> Linear model for One-Way ANOVA (cell means and reference-coding)</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#section3-3"><i class="fa fa-check"></i><b>3.3</b> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#section3-4"><i class="fa fa-check"></i><b>3.4</b> ANOVA model diagnostics including QQ-plots</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#section3-5"><i class="fa fa-check"></i><b>3.5</b> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#section3-6"><i class="fa fa-check"></i><b>3.6</b> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li class="chapter" data-level="3.7" data-path="chapter3.html"><a href="chapter3.html#section3-7"><i class="fa fa-check"></i><b>3.7</b> Pair-wise comparisons for the Overtake data</a></li>
<li class="chapter" data-level="3.8" data-path="chapter3.html"><a href="chapter3.html#section3-8"><i class="fa fa-check"></i><b>3.8</b> Chapter summary</a></li>
<li class="chapter" data-level="3.9" data-path="chapter3.html"><a href="chapter3.html#section3-9"><i class="fa fa-check"></i><b>3.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="3.10" data-path="chapter3.html"><a href="chapter3.html#section3-10"><i class="fa fa-check"></i><b>3.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Two-Way ANOVA</a><ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#section4-1"><i class="fa fa-check"></i><b>4.1</b> Situation</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#section4-2"><i class="fa fa-check"></i><b>4.2</b> Designing a two-way experiment and visualizing results</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#section4-3"><i class="fa fa-check"></i><b>4.3</b> Two-Way ANOVA models and hypothesis tests</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#section4-4"><i class="fa fa-check"></i><b>4.4</b> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#section4-5"><i class="fa fa-check"></i><b>4.5</b> Observational study example: The Psychology of Debt</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#section4-6"><i class="fa fa-check"></i><b>4.6</b> Pushing Two-Way ANOVA to the limit: Un-replicated designs and Estimability</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#section4-7"><i class="fa fa-check"></i><b>4.7</b> Chapter summary</a></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#section4-8"><i class="fa fa-check"></i><b>4.8</b> Summary of important R code</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#section4-9"><i class="fa fa-check"></i><b>4.9</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Chi-square tests</a><ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#section5-1"><i class="fa fa-check"></i><b>5.1</b> Situation, contingency tables, and tableplots</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#section5-2"><i class="fa fa-check"></i><b>5.2</b> Homogeneity test hypotheses</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#section5-3"><i class="fa fa-check"></i><b>5.3</b> Independence test hypotheses</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#section5-4"><i class="fa fa-check"></i><b>5.4</b> Models for R by C tables</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#section5-5"><i class="fa fa-check"></i><b>5.5</b> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#section5-6"><i class="fa fa-check"></i><b>5.6</b> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#section5-7"><i class="fa fa-check"></i><b>5.7</b> Examining residuals for the source of differences</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#section5-8"><i class="fa fa-check"></i><b>5.8</b> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#section5-9"><i class="fa fa-check"></i><b>5.9</b> Political party and voting results: Complete analysis</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#section5-10"><i class="fa fa-check"></i><b>5.10</b> Is cheating and lying related in students?</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#section5-11"><i class="fa fa-check"></i><b>5.11</b> Analyzing a stratified random sample of California schools</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#section5-12"><i class="fa fa-check"></i><b>5.12</b> Chapter summary</a></li>
<li class="chapter" data-level="5.13" data-path="chapter5.html"><a href="chapter5.html#section5-13"><i class="fa fa-check"></i><b>5.13</b> Summary of important R commands</a></li>
<li class="chapter" data-level="5.14" data-path="chapter5.html"><a href="chapter5.html#section5-14"><i class="fa fa-check"></i><b>5.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Correlation and Simple Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#section6-1"><i class="fa fa-check"></i><b>6.1</b> Relationships between two quantitative variables</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#section6-2"><i class="fa fa-check"></i><b>6.2</b> Estimating the correlation coefficient</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#section6-3"><i class="fa fa-check"></i><b>6.3</b> Relationships between variables by groups</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#section6-4"><i class="fa fa-check"></i><b>6.4</b> Inference for the correlation coefficient</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#section6-5"><i class="fa fa-check"></i><b>6.5</b> Are tree diameters related to tree heights?</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#section6-6"><i class="fa fa-check"></i><b>6.6</b> Describing relationships with a regression model</a></li>
<li class="chapter" data-level="6.7" data-path="chapter6.html"><a href="chapter6.html#section6-7"><i class="fa fa-check"></i><b>6.7</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.8" data-path="chapter6.html"><a href="chapter6.html#section6-8"><i class="fa fa-check"></i><b>6.8</b> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li class="chapter" data-level="6.9" data-path="chapter6.html"><a href="chapter6.html#section6-9"><i class="fa fa-check"></i><b>6.9</b> Outliers: leverage and influence</a></li>
<li class="chapter" data-level="6.10" data-path="chapter6.html"><a href="chapter6.html#section6-10"><i class="fa fa-check"></i><b>6.10</b> Residual diagnostics – setting the stage for inference</a></li>
<li class="chapter" data-level="6.11" data-path="chapter6.html"><a href="chapter6.html#section6-11"><i class="fa fa-check"></i><b>6.11</b> Old Faithful discharge and waiting times</a></li>
<li class="chapter" data-level="6.12" data-path="chapter6.html"><a href="chapter6.html#section6-12"><i class="fa fa-check"></i><b>6.12</b> Chapter summary</a></li>
<li class="chapter" data-level="6.13" data-path="chapter6.html"><a href="chapter6.html#section6-13"><i class="fa fa-check"></i><b>6.13</b> Summary of important R code</a></li>
<li class="chapter" data-level="6.14" data-path="chapter6.html"><a href="chapter6.html#section6-14"><i class="fa fa-check"></i><b>6.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Simple linear regression inference</a><ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#section7-1"><i class="fa fa-check"></i><b>7.1</b> Model</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#section7-2"><i class="fa fa-check"></i><b>7.2</b> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#section7-3"><i class="fa fa-check"></i><b>7.3</b> Bozeman temperature trend</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#section7-4"><i class="fa fa-check"></i><b>7.4</b> Randomization-based inferences for the slope coefficient</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#section7-5"><i class="fa fa-check"></i><b>7.5</b> Transformations part I: Linearizing relationships</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#section7-6"><i class="fa fa-check"></i><b>7.6</b> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#section7-7"><i class="fa fa-check"></i><b>7.7</b> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li class="chapter" data-level="7.8" data-path="chapter7.html"><a href="chapter7.html#section7-8"><i class="fa fa-check"></i><b>7.8</b> Chapter summary</a></li>
<li class="chapter" data-level="7.9" data-path="chapter7.html"><a href="chapter7.html#section7-9"><i class="fa fa-check"></i><b>7.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="7.10" data-path="chapter7.html"><a href="chapter7.html#section7-10"><i class="fa fa-check"></i><b>7.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#section8-1"><i class="fa fa-check"></i><b>8.1</b> Going from SLR to MLR</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#section8-2"><i class="fa fa-check"></i><b>8.2</b> Validity conditions in MLR</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#section8-3"><i class="fa fa-check"></i><b>8.3</b> Interpretation of MLR terms</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#section8-4"><i class="fa fa-check"></i><b>8.4</b> Comparing multiple regression models</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#section8-5"><i class="fa fa-check"></i><b>8.5</b> General recommendations for MLR interpretations and VIFs</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#section8-6"><i class="fa fa-check"></i><b>8.6</b> MLR inference: Parameter inferences using the t-distribution</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#section8-7"><i class="fa fa-check"></i><b>8.7</b> Overall F-test in multiple linear regression</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#section8-8"><i class="fa fa-check"></i><b>8.8</b> Case study: First year college GPA and SATs</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#section8-9"><i class="fa fa-check"></i><b>8.9</b> Different intercepts for different groups: MLR with indicator variables</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#section8-10"><i class="fa fa-check"></i><b>8.10</b> Additive MLR with more than two groups: Headache example</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#section8-11"><i class="fa fa-check"></i><b>8.11</b> Different slopes and different intercepts</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#section8-12"><i class="fa fa-check"></i><b>8.12</b> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#section8-13"><i class="fa fa-check"></i><b>8.13</b> AICs for model selection</a></li>
<li class="chapter" data-level="8.14" data-path="chapter8.html"><a href="chapter8.html#section8-14"><i class="fa fa-check"></i><b>8.14</b> Case study: Forced expiratory volume model selection using AICs</a></li>
<li class="chapter" data-level="8.15" data-path="chapter8.html"><a href="chapter8.html#section8-15"><i class="fa fa-check"></i><b>8.15</b> Chapter summary</a></li>
<li class="chapter" data-level="8.16" data-path="chapter8.html"><a href="chapter8.html#section8-16"><i class="fa fa-check"></i><b>8.16</b> Summary of important R code</a></li>
<li class="chapter" data-level="8.17" data-path="chapter8.html"><a href="chapter8.html#section8-17"><i class="fa fa-check"></i><b>8.17</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Case studies</a><ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#section9-1"><i class="fa fa-check"></i><b>9.1</b> Overview of material covered</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#section9-2"><i class="fa fa-check"></i><b>9.2</b> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#section9-3"><i class="fa fa-check"></i><b>9.3</b> Ants learn to rely on more informative attributes during decision-making</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#section9-4"><i class="fa fa-check"></i><b>9.4</b> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#section9-5"><i class="fa fa-check"></i><b>9.5</b> What do didgeridoos really do about sleepiness?</a></li>
<li class="chapter" data-level="9.6" data-path="chapter9.html"><a href="chapter9.html#section9-6"><i class="fa fa-check"></i><b>9.6</b> General summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Intermediate Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter3" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> One-Way ANOVA</h1>
<div id="section3-1" class="section level2">
<h2><span class="header-section-number">3.1</span> Situation</h2>
<p>In Chapter <a href="chapter2.html#chapter2">2</a>, tools for comparing the means of two groups
were considered. More generally, these methods are used for a quantitative
response and a categorical explanatory variable (group) which had two and only
two levels. The complete overtake distance data set actually contained seven groups
(Figure <a href="chapter3.html#fig:Figure3-1">3.1</a>) with the outfit for each commute randomly assigned. In a
situation with more than two groups, we have two choices. First, we could rely
on our two group comparisons, performing tests for every possible pair
(<em>commute</em> vs <em>casual</em>, <em>casual</em> vs <em>highviz</em>, <em>commute</em> vs <em>highviz</em>, …, <em>polite</em> vs <em>racer</em>), which would entail 21 different comparisons. But this would engage multiple testing issues and inflation of Type I error rates if not accounted for in some fashion. We would also end up with 21 p-values that answer detailed questions but none that addresses a simple but initially useful question – is there a difference somewhere among the pairs of groups or, under the null hypothesis, are all the true group means the same? In this chapter, we will learn a new method, called
<strong><em>Analysis of Variance</em></strong>, <strong><em>ANOVA</em></strong>, or sometimes <strong><em>AOV</em></strong> that directly assesses evidence against the null hypothesis of no difference and then possibly leading to the ability to conclude that there is some overall difference in the means among the groups. This version
of an ANOVA is called a <strong><em>One-Way ANOVA</em></strong> since there is just
one<a href="#fn52" class="footnote-ref" id="fnref52"><sup>52</sup></a> grouping variable. After we perform our One-Way ANOVA test for
overall evidence of some difference, we will revisit the comparisons similar to those
considered in Chapter <a href="chapter2.html#chapter2">2</a> to get more details on specific differences
among <em>all</em> the pairs of groups – what we call <strong><em>pair-wise comparisons</em></strong>.
We will augment
our previous methods for comparing two groups with an adjusted method for pairwise comparisons to make our
results valid called <strong><em>Tukey’s Honest Significant Difference</em></strong>.</p>
<p>To make this more concrete, we return to the original overtake data, making
a pirate-plot (Figure <a href="chapter3.html#fig:Figure3-1">3.1</a>) as well as
summarizing the overtake distances by the seven groups using <code>favstats</code>.</p>

<div class="figure"><span id="fig:Figure3-1"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-1-1.png" alt="Pirate-plot of the overtake distances for the seven groups with group mean (bold lines with boxes indicating 95% confidence intervals) and the overall sample mean (dashed line) of 117.1 cm added." width="960" />
<p class="caption">
Figure 3.1: Pirate-plot of the overtake distances for the seven groups with group mean (bold lines with boxes indicating 95% confidence intervals) and the overall sample mean (dashed line) of 117.1 cm added.
</p>
</div>
<pre><code>##   Condition min    Q1 median  Q3 max     mean       sd   n missing
## 1    casual  17 100.0    117 134 245 117.6110 29.86954 779       0
## 2   commute   8  98.0    116 132 222 114.6079 29.63166 857       0
## 3     hiviz  12 101.0    117 134 237 118.4383 29.03384 737       0
## 4    novice   2 100.5    118 133 274 116.9405 29.03812 807       0
## 5    police  34 104.0    119 138 253 122.1215 29.73662 790       0
## 6    polite   2  95.0    114 133 225 114.0518 31.23684 868       0
## 7     racer  28  98.0    117 135 231 116.7559 30.60059 852       0</code></pre>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="chapter3.html#cb220-1"></a><span class="kw">library</span>(mosaic)</span>
<span id="cb220-2"><a href="chapter3.html#cb220-2"></a><span class="kw">library</span>(readr)</span>
<span id="cb220-3"><a href="chapter3.html#cb220-3"></a><span class="kw">library</span>(yarrr)</span>
<span id="cb220-4"><a href="chapter3.html#cb220-4"></a>dd &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/Walker2014_mod.csv&quot;</span>)</span>
<span id="cb220-5"><a href="chapter3.html#cb220-5"></a>dd<span class="op">$</span>Condition &lt;-<span class="st"> </span><span class="kw">factor</span>(dd<span class="op">$</span>Condition)</span>
<span id="cb220-6"><a href="chapter3.html#cb220-6"></a></span>
<span id="cb220-7"><a href="chapter3.html#cb220-7"></a><span class="kw">pirateplot</span>(Distance<span class="op">~</span>Condition,<span class="dt">data=</span>dd, <span class="dt">inf.method=</span><span class="st">&quot;ci&quot;</span>, <span class="dt">inf.disp=</span><span class="st">&quot;line&quot;</span>)</span>
<span id="cb220-8"><a href="chapter3.html#cb220-8"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">mean</span>(dd<span class="op">$</span>Distance), <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;green&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>) <span class="co"># Adds overall mean to plot</span></span>
<span id="cb220-9"><a href="chapter3.html#cb220-9"></a><span class="kw">favstats</span>(Distance<span class="op">~</span>Condition,<span class="dt">data=</span>dd)</span></code></pre></div>
<p>There are slight differences in the sample sizes in the seven groups with between <span class="math inline">\(737\)</span> and <span class="math inline">\(868\)</span> observations, providing a
data set has a total sample size of <span class="math inline">\(N=5,690\)</span>. The sample means vary from 114.05 to 122.12 cm. In
Chapter <a href="chapter2.html#chapter2">2</a>, we found moderate evidence regarding the difference in
<em>commute</em> and <em>casual</em>. It is less clear whether we might find evidence of a
difference between, say, <em>commute</em> and <em>novice</em> groups since we are comparing
means of 114.05 and 116.94 cm. All the distributions appear to have similar shapes that are generally symmetric and bell-shaped and have relatively similar variability. The <em>police</em> vest group of observations seems to have highest sample mean, but there are many open questions about what differences might really exist here and there are many comparisons that could be considered.</p>
</div>
<div id="section3-2" class="section level2">
<h2><span class="header-section-number">3.2</span> Linear model for One-Way ANOVA (cell means and reference-coding)</h2>
<p>We introduced the statistical model <span class="math inline">\(y_{ij} = \mu_j+\varepsilon_{ij}\)</span> in
Chapter <a href="chapter2.html#chapter2">2</a> for the situation with <span class="math inline">\(j = 1 \text{ or } 2\)</span> to denote
a situation where there were two groups and, for the model that is consistent
with the alternative hypothesis, the means differed. Now there are seven groups
and the previous model can be extended to this new situation by allowing <span class="math inline">\(j\)</span>
to be 1, 2, 3, …, 7.  As before, the linear model assumes that the responses follow a normal
distribution with the model defining the mean of the normal distributions and all observations have the
same variance. <strong><em>Linear models</em></strong> assume that the parameters for the mean in the model enter linearly. This
last condition is hard to explain at this level of material – it is sufficient
to know that there are models where the parameters enter the model nonlinearly and
that they are beyond the scope of this function and this material and you won’t run into them in most statistical models. By employing this general “linear” modeling methodology, we will be able to use the same general modeling framework for the methods
in Chapters <a href="chapter3.html#chapter3">3</a>, <a href="chapter4.html#chapter4">4</a>, <a href="chapter6.html#chapter6">6</a>,
<a href="chapter7.html#chapter7">7</a>, and <a href="chapter8.html#chapter8">8</a>.</p>
<p>As in Chapter <a href="chapter2.html#chapter2">2</a>, the null hypothesis defines a
situation (and model) where all the groups have the same mean. Specifically,
the <strong><em>null hypothesis</em></strong> in the general situation with <span class="math inline">\(J\)</span> groups
(<span class="math inline">\(J\ge 2\)</span>) is to have all the <span class="math inline">\(\underline{\text{true}}\)</span> group means equal,</p>
<p><span class="math display">\[H_0:\mu_1 = \ldots = \mu_J.\]</span></p>
<p>This defines a model where all the groups have the same mean so it can be
defined in terms of a single mean, <span class="math inline">\(\mu\)</span>, for the <span class="math inline">\(i^{th}\)</span> observation from
the <span class="math inline">\(j^{th}\)</span> group as <span class="math inline">\(y_{ij} = \mu+\varepsilon_{ij}\)</span>. This is not the model
that most researchers want to be the final description of their study as it
implies no difference in the groups. There is more caution required to specify
the alternative hypothesis with more than two groups. The
<strong><em>alternative hypothesis</em></strong> needs to be the logical negation of this null
hypothesis of all groups having equal means; to make the null hypothesis
false, we only need one group to differ but more than one group could differ
from the others. Essentially, there are many ways to “violate” the null
hypothesis so we choose some delicate wording for the alternative hypothesis
when there are more than 2 groups. Specifically, we state the alternative as</p>
<p><span class="math display">\[H_A: \text{ Not all } \mu_j \text{ are equal}\]</span></p>
<p>or, in words, <strong>at least one of the true means differs among the J groups</strong>.
You might be attracted to trying to say that all means are different in the
alternative but we do not put this strict a requirement in place to reject the
null hypothesis. The alternative model

allows all the true group means to
differ but does require that they are actually all different with the model written as</p>
<p><span class="math display">\[y_{ij} = {\color{red}{\mu_j}}+\varepsilon_{ij}.\]</span></p>
<p>This linear model

states that the response for the <span class="math inline">\(i^{th}\)</span> observation in
the <span class="math inline">\(j^{th}\)</span> group, <span class="math inline">\(\mathbf{y_{ij}}\)</span>, is modeled with a group <span class="math inline">\(j\)</span>
(<span class="math inline">\(j=1, \ldots, J\)</span>) population mean, <span class="math inline">\(\mu_j\)</span>, and a random error for each subject
in each group, <span class="math inline">\(\varepsilon_{ij}\)</span>, that we assume follows a normal distribution and
that all the random errors have the same variance, <span class="math inline">\(\sigma^2\)</span>. We can write the assumption about the random errors, often called the <strong><em>normality assumption</em></strong>,
as <span class="math inline">\(\varepsilon_{ij} \sim N(0,\sigma^2)\)</span>. There is a second way to write out this
model that allows extension to more complex models discussed below, so we
need a name for this version of the model. The model written in terms of the
<span class="math inline">\({\color{red}{\mu_j}}\text{&#39;s}\)</span> is called the
<b><font color='red'>cell means model</font></b> and is the
easier version of this model to understand.
</p>
<p>One of the reasons we learned about pirate-plots is that
it helps us visually consider all the aspects of this model.

In Figure <a href="chapter3.html#fig:Figure3-1">3.1</a>,
we can see the bold horizontal lines that provide the estimated (sample) group means.
The bigger the differences in the sample means (especially relative to the variability around the means), the more evidence we will find
against the null hypothesis. You can also see the null model on the plot
that assumes all the groups have the same mean as displayed in the
dashed horizontal line
at 117.1 cm (the R code below shows the overall mean of <em>Distance</em> is 117.1). While
the hypotheses focus on the means, the model also contains assumptions about the
distribution of the responses – specifically that the distributions are normal
and that all the groups have the same variability, which do not appear to be clearly violated in this situation.
</p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="chapter3.html#cb221-1"></a><span class="kw">mean</span>(dd<span class="op">$</span>Distance)</span></code></pre></div>
<pre><code>## [1] 117.126</code></pre>
<p>There is a second way to write out the One-Way ANOVA model

that provides a framework
for extensions to more complex models described in Chapter <a href="chapter4.html#chapter4">4</a> and
beyond. The other <strong><em>parameterization</em></strong> (way of writing out or defining) of the
model is called the <b><font color='purple'>reference-coded model</font></b> since it
writes out the model in terms of a


<strong><em>baseline group</em></strong> and deviations from that baseline or reference level. The
reference-coded model for the <span class="math inline">\(i^{th}\)</span> subject in the <span class="math inline">\(j^{th}\)</span> group is
<span class="math inline">\(y_{ij} ={\color{purple}{\boldsymbol{\alpha + \tau_j}}}+\varepsilon_{ij}\)</span> where
<span class="math inline">\(\color{purple}{\boldsymbol{\alpha}}\)</span> (“alpha”) is the true mean for the
baseline group (usually first alphabetically) and the <span class="math inline">\(\color{purple}{\boldsymbol{\tau_j}}\)</span>
(tau <span class="math inline">\(j\)</span>) are the deviations from the baseline group for group <span class="math inline">\(j\)</span>. The deviation
for the baseline group, <span class="math inline">\(\color{purple}{\boldsymbol{\tau_1}}\)</span>, is always set to 0
so there are really just deviations for groups 2 through <span class="math inline">\(J\)</span>. The equivalence
between the reference-coded and cell means models can be seen by considering the mean for the first, second,
and <span class="math inline">\(J^{th}\)</span> groups in both models:</p>
<p><span class="math display">\[\begin{array}{lccc}
&amp; \textbf{Cell means:} &amp;&amp; \textbf{Reference-coded:}\\
\textbf{Group } 1: &amp; \color{red}{\mu_1} &amp;&amp; \color{purple}{\boldsymbol{\alpha}} \\
\textbf{Group } 2: &amp; \color{red}{\mu_2} &amp;&amp; \color{purple}{\boldsymbol{\alpha + \tau_2}} \\
\ldots &amp; \ldots &amp;&amp; \ldots \\
\textbf{Group } J: &amp; \color{red}{\mu_J} &amp;&amp; \color{purple}{\boldsymbol{\alpha +\tau_J}}
\end{array}\]</span></p>
<p>The hypotheses for the reference-coded model are similar to those in the
cell means coding except that they are defined in terms of the deviations,
<span class="math inline">\({\color{purple}{\boldsymbol{\tau_j}}}\)</span>. The null hypothesis is that there is
no deviation from the baseline for any group – that all the <span class="math inline">\({\color{purple}{\boldsymbol{\tau_j\text{&#39;s}}}}=0\)</span>,</p>
<p><span class="math display">\[\boldsymbol{H_0: \tau_2=\ldots=\tau_J=0}.\]</span></p>
<p>The alternative hypothesis is that at least one of the deviations is not 0,</p>
<p><span class="math display">\[\boldsymbol{H_A:} \textbf{ Not all } \boldsymbol{\tau_j} \textbf{ equal } \bf{0}.\]</span></p>
<p>In this chapter, you are welcome to use either version (unless we instruct you
otherwise) but we have to use the reference-coding in subsequent chapters. The
next task is to learn how to use R’s linear model, <code>lm</code>, function to get
estimates of the parameters<a href="#fn53" class="footnote-ref" id="fnref53"><sup>53</sup></a> in each model, but first a quick review of these
new ideas:</p>
<p><b><font color='red'>Cell Means Version</font></b></p>
<ul>
<li><p><span class="math inline">\(H_0: {\color{red}{\mu_1= \ldots = \mu_J}}\)</span>        
    <span class="math inline">\(H_A: {\color{red}{\text{ Not all } \mu_j \text{ equal}}}\)</span></p></li>
<li><p>Null hypothesis in words: No difference in the true means among the groups.</p></li>
<li><p>Null model: <span class="math inline">\(y_{ij} = \mu+\varepsilon_{ij}\)</span> </p></li>
<li><p>Alternative hypothesis in words: At least one of the true means differs among
the groups.</p></li>
<li><p>Alternative model: <span class="math inline">\(y_{ij} = \color{red}{\mu_j}+\varepsilon_{ij}.\)</span>
</p></li>
</ul>
<p><b><font color='purple'>Reference-coded Version</font></b></p>
<ul>
<li><p><span class="math inline">\(H_0: \color{purple}{\boldsymbol{\tau_2 = \ldots = \tau_J = 0}}\)</span>
        
<span class="math inline">\(H_A: \color{purple}{\text{ Not all } \tau_j \text{ equal 0}}\)</span></p></li>
<li><p>Null hypothesis in words: No deviation of the true mean for any groups from the
baseline group.</p></li>
<li><p>Null model: <span class="math inline">\(y_{ij} =\boldsymbol{\alpha} +\varepsilon_{ij}\)</span>
</p></li>
<li><p>Alternative hypothesis in words: At least one of the true deviations is
different from 0 or that at least one group has a different true mean than the
baseline group.</p></li>
<li><p>Alternative model: <span class="math inline">\(y_{ij} =\color{purple}{\boldsymbol{\alpha + \tau_j}}+\varepsilon_{ij}\)</span> </p></li>
</ul>
<p>In order to estimate the models discussed above, the <code>lm</code> function is used<a href="#fn54" class="footnote-ref" id="fnref54"><sup>54</sup></a>. The <code>lm</code> function continues to
use the same format as previous functions and in Chapter <a href="chapter2.html#chapter2">2</a> , <code>lm(Y~X, data=datasetname)</code>.
It ends up that <code>lm</code> generates the reference-coded version of the
model by default (The developers of R thought it was that important!).


But we want to start with the
cell means version of the model,

so we have to override the standard technique
and add a “<code>-1</code>” to the formula interface to tell R that we want to the
cell means coding. Generally, this looks like <code>lm(Y~X-1, data=datasetname).</code>

Once we fit a model in R, the <code>summary</code> function run on the model provides a
useful “summary” of the model coefficients and a suite of other potentially
interesting information. For the moment, we will focus on the estimated model
coefficients, so only those lines are provided. When fitting the cell means version
of the One-Way ANOVA model,
you will find a row of output for each group relating estimating the <span class="math inline">\(\mu_j\text{&#39;s}\)</span>.
The output contains columns for an estimate (<code>Estimate</code>), standard error
(<code>Std. Error</code>), <span class="math inline">\(t\)</span>-value (<code>t value</code>), and p-value (<code>Pr(&gt;|t|)</code>). We’ll
explore which of these are of interest in these models below, but focus
on the estimates of the parameters that the function provides in the first
column (“Estimate”) of the coefficient table and compare these results to what was found using <code>favstats</code>.</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="chapter3.html#cb223-1"></a>lm1 &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span>Condition<span class="dv">-1</span>, <span class="dt">data=</span>dd)</span>
<span id="cb223-2"><a href="chapter3.html#cb223-2"></a><span class="kw">summary</span>(lm1)<span class="op">$</span>coefficients</span></code></pre></div>
<pre><code>##                  Estimate Std. Error  t value Pr(&gt;|t|)
## Conditioncasual  117.6110   1.071873 109.7248        0
## Conditioncommute 114.6079   1.021931 112.1484        0
## Conditionhiviz   118.4383   1.101992 107.4765        0
## Conditionnovice  116.9405   1.053114 111.0426        0
## Conditionpolice  122.1215   1.064384 114.7344        0
## Conditionpolite  114.0518   1.015435 112.3182        0
## Conditionracer   116.7559   1.024925 113.9164        0</code></pre>
<p>In general, we denote estimated parameters   with a hat over the parameter of
interest to show that it is an estimate. For the true mean of group <span class="math inline">\(j\)</span>,
<span class="math inline">\(\mu_j\)</span>, we estimate it with <span class="math inline">\(\widehat{\mu}_j\)</span>, which is just the sample mean for group
<span class="math inline">\(j\)</span>, <span class="math inline">\(\bar{x}_j\)</span>. The model suggests an estimate for each observation that we denote
as <span class="math inline">\(\widehat{y}_{ij}\)</span> that we will also call a <strong><em>fitted value</em></strong> based on the model
being considered. The
same estimate is used for all observations in the each group in this model. R tries to help you to
sort out which row of output corresponds to which group by appending the group name
with the variable name. Here, the variable name was <code>Condition</code> and the first group
alphabetically was <em>casual</em>, so R provides a row labeled <code>Conditioncasual</code>
with an estimate of 117.61. The sample means from the seven groups can be seen to
directly match the <code>favstats</code> results presented previously.</p>
<p>The reference-coded version of the same model is more complicated but ends up

giving the same results once we understand what it is doing. It uses a different
parameterization to accomplish this, so has different model output. Here is the model
summary:</p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="chapter3.html#cb225-1"></a>lm2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>dd)</span>
<span id="cb225-2"><a href="chapter3.html#cb225-2"></a><span class="kw">summary</span>(lm2)<span class="op">$</span>coefficients</span></code></pre></div>
<pre><code>##                     Estimate Std. Error     t value    Pr(&gt;|t|)
## (Intercept)      117.6110398   1.071873 109.7247845 0.000000000
## Conditioncommute  -3.0031051   1.480964  -2.0278039 0.042626835
## Conditionhiviz     0.8272234   1.537302   0.5381008 0.590528548
## Conditionnovice   -0.6705193   1.502651  -0.4462242 0.655452292
## Conditionpolice    4.5104792   1.510571   2.9859423 0.002839115
## Conditionpolite   -3.5591965   1.476489  -2.4105807 0.015958695
## Conditionracer    -0.8551713   1.483032  -0.5766371 0.564207492</code></pre>
<p>The estimated model coefficients are <span class="math inline">\(\widehat{\alpha} = 117.61\)</span> cm,
<span class="math inline">\(\widehat{\tau}_2 =-3.00\)</span> cm, <span class="math inline">\(\widehat{\tau}_3=0.83\)</span> cm, and so on up to <span class="math inline">\(\widehat{\tau}_7=-0.86\)</span> cm, where R selected group 1
for <em>casual</em>, 2 for <em>commute</em>, 3 for <em>hiviz</em>, all the way up to group 7 for <em>racer</em>. The way you can figure
out the baseline group (group 1 is <em>casual</em> here) is to see which category label
is <em>not present</em> in the reference-coded output. <strong>The baseline level is typically the first group
label alphabetically</strong>, but you should always check this<a href="#fn55" class="footnote-ref" id="fnref55"><sup>55</sup></a>. Based on these definitions,
there are interpretations available for each coefficient. For <span class="math inline">\(\widehat{\alpha} = 117.61\)</span> cm, this is an estimate of the mean overtake distance for the <em>casual</em> outfit group.
<span class="math inline">\(\widehat{\tau}_2 =-3.00\)</span> cm is the deviation of the <em>commute</em> group’s mean from
the <em>causal</em> group’s mean (specifically, it is <span class="math inline">\(3.00\)</span> cm lower and was a quantity we explored in detail in Chapter <a href="chapter2.html#chapter2">2</a> when we just focused on comparing <em>casual</em> and <em>commute</em> groups).
<span class="math inline">\(\widehat{\tau}_3=0.83\)</span> cm tells us that the <em>hiviz</em> group mean distance is 0.83 cm higher than the <em>casual</em> group mean and <span class="math inline">\(\widehat{\tau}_7=-0.86\)</span> says that the <em>racer</em> sample mean was 0.86 cm lower than for the <em>casual</em> group. These
interpretations are interesting as they directly relate to comparisons of groups with the baseline and lead directly to
reconstructing the estimated means for each group by combining the baseline and
a pertinent deviation as shown in Table <a href="chapter3.html#tab:Table3-1">3.1</a>.</p>

<table>
<caption><span id="tab:Table3-1">Table 3.1: </span> Constructing group mean estimates from the reference-coded linear model estimates.</caption>
<colgroup>
<col width="12%" />
<col width="47%" />
<col width="40%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Group</th>
<th align="left">Formula</th>
<th align="left">Estimates</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">casual</td>
<td align="left"><span class="math inline">\(\widehat{\alpha}\)</span></td>
<td align="left"><strong>117.61</strong> cm</td>
</tr>
<tr class="even">
<td align="left">commute</td>
<td align="left"><span class="math inline">\(\widehat{\alpha}+\widehat{\tau}_2\)</span></td>
<td align="left">117.61 - 3.00 = <strong>114.61</strong> cm</td>
</tr>
<tr class="odd">
<td align="left">hiviz</td>
<td align="left"><span class="math inline">\(\widehat{\alpha}+\widehat{\tau}_3\)</span></td>
<td align="left">117.61 + 0.83 = <strong>118.44</strong> cm</td>
</tr>
<tr class="even">
<td align="left">novice</td>
<td align="left"><span class="math inline">\(\widehat{\alpha}+\widehat{\tau}_4\)</span></td>
<td align="left">117.61 - 0.67 = <strong>116.94</strong> cm</td>
</tr>
<tr class="odd">
<td align="left">police</td>
<td align="left"><span class="math inline">\(\widehat{\alpha}+\widehat{\tau}_5\)</span></td>
<td align="left">117.61 + 4.51 = <strong>122.12</strong> cm</td>
</tr>
<tr class="even">
<td align="left">polite</td>
<td align="left"><span class="math inline">\(\widehat{\alpha}+\widehat{\tau}_6\)</span></td>
<td align="left">117.61 - 3.56 = <strong>114.05</strong> cm</td>
</tr>
<tr class="odd">
<td align="left">racer</td>
<td align="left"><span class="math inline">\(\widehat{\alpha}+\widehat{\tau}_7\)</span></td>
<td align="left">117.61 - 0.86 = <strong>116.75</strong> cm</td>
</tr>
</tbody>
</table>
<p>We can also visualize the results of our linear models using what are called
<strong><em>term-plots</em></strong> or <strong><em>effect-plots</em></strong>
 
(from the <code>effects</code> package; <span class="citation">(Fox et al. <a href="#ref-R-effects" role="doc-biblioref">2019</a>)</span>)

as displayed in Figure <a href="chapter3.html#fig:Figure3-2">3.2</a>. We don’t want to use the word
“effect” for these model components unless we have random assignment in the study

design so we generically call these <strong><em>term-plots</em></strong> as they display terms or
components from the model in hopefully useful ways to aid in model interpretation
even in the presence of complicated model parameterizations. The word “effect” has a causal connotation that we want to avoid as much as possible in non-causal (so non-randomly assigned) situations. Term-plots take an estimated model and show you its estimates along with 95% confidence
intervals generated by the linear model. These confidence intervals may differ from the confidence intervals in the pirate-plots since the pirate-plots make them for each group separately and term-plots are combining information across groups via the estimated model and then doing inferences for individual group means. To make term-plots, you need to install and
load the <code>effects</code> package and then use <code>plot(allEffects(...))</code> functions
together on the <code>lm</code> object called <code>lm2</code> that was estimated above. You can
find the correspondence between the displayed means and the estimates that were
constructed in Table <a href="chapter3.html#tab:Table3-1">3.1</a>.
</p>

<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb227-1"><a href="chapter3.html#cb227-1"></a><span class="kw">library</span>(effects)</span>
<span id="cb227-2"><a href="chapter3.html#cb227-2"></a><span class="kw">plot</span>(<span class="kw">allEffects</span>(lm2))</span></code></pre></div>
<div class="figure"><span id="fig:Figure3-2"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-2-1.png" alt="Plot of the estimated group mean distances from the reference-coded model for the overtake data from the effects package." width="672" />
<p class="caption">
Figure 3.2: Plot of the estimated group mean distances from the reference-coded model for the overtake data from the <code>effects</code> package.
</p>
</div>
<p>In order to assess overall evidence against having the same means for the all groups (vs having at least one mean different from the others), we
compare either of the previous models (cell means or reference-coded) to a null
model based on the null hypothesis of <span class="math inline">\(H_0: \mu_1 = \ldots = \mu_J\)</span>, which implies a
model of <span class="math inline">\(\color{red}{y_{ij} = \mu+\varepsilon_{ij}}\)</span> in the cell means version
where <span class="math inline">\({\color{red}{\mu}}\)</span> is a common mean for all the observations. We will call
this the <b><font color='red'>mean-only</font></b> model since it only has a single mean
in it. In the reference-coded version of the model, we have a null hypothesis of
<span class="math inline">\(H_0: \tau_2 = \ldots = \tau_J = 0\)</span>, so the “mean-only” model is

<span class="math inline">\(\color{purple}{y_{ij} =\boldsymbol{\alpha}+\varepsilon_{ij}}\)</span> with
<span class="math inline">\(\color{purple}{\boldsymbol{\alpha}}\)</span> having the same definition as
<span class="math inline">\(\color{red}{\mu}\)</span> for the cell means model – it forces a common value for the
mean for all the groups. Moving from the <em>reference-coded</em> model to the <em>mean-only</em>
model is also an example of a situation where we move from a “full” model


to a
“reduced” model by setting some coefficients in the “full” model to 0 and, by doing
this, get a simpler or “reduced” model.

Simple models can be good as they are easier
to interpret, but having a model for <span class="math inline">\(J\)</span> groups that suggests no difference in the
groups is not a very exciting result
in most, but not all, situations<a href="#fn56" class="footnote-ref" id="fnref56"><sup>56</sup></a>. In order for R to provide
results for the mean-only model, we remove the grouping variable, <code>Condition</code>, from
the model formula and just include a “1”. The <code>(Intercept)</code> row of the output
provides the estimate for the mean-only model as a reduced model from either the
cell means or reference-coded models when we assume that the mean is the same
for all groups:</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="chapter3.html#cb228-1"></a>lm3 &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span><span class="dv">1</span>, <span class="dt">data=</span>dd)</span>
<span id="cb228-2"><a href="chapter3.html#cb228-2"></a><span class="kw">summary</span>(lm3)<span class="op">$</span>coefficients</span></code></pre></div>
<pre><code>##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  117.126  0.3977533 294.469        0</code></pre>
<p>This model provides an estimate of the common mean for all observations of
<span class="math inline">\(117.13 = \widehat{\mu} = \widehat{\alpha}\)</span> cm. This value also is the dashed horizontal
line in the pirate-plot in Figure <a href="chapter3.html#fig:Figure3-1">3.1</a>. Some people
call this mean-only model estimate the “grand” or “overall” mean and notationally is represented as <span class="math inline">\(\bar{\bar{y}}\)</span>. </p>
</div>
<div id="section3-3" class="section level2">
<h2><span class="header-section-number">3.3</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</h2>
<p>The previous discussion showed two ways of parameterizing models for the
One-Way ANOVA model and getting estimates from output but still hasn’t
addressed how to assess evidence related to whether the observed differences
in the means among the groups is “real”. In this section, we develop what is
called the <strong><em>ANOVA F-test</em></strong>

that provides a method of aggregating the
differences among the means of 2 or more groups and testing (assessing evidence against) our null hypothesis
of no difference in the means vs the alternative. In order to develop the test,
some additional notation is needed. The sample size in each group is denoted
<span class="math inline">\(n_j\)</span> and the total sample size is
<span class="math inline">\(\boldsymbol{N=\Sigma n_j = n_1 + n_2 + \ldots + n_J}\)</span> where <span class="math inline">\(\Sigma\)</span>
(capital sigma) means “add up over whatever follows”. An estimated
<strong><em>residual</em></strong> (<span class="math inline">\(e_{ij}\)</span>) is the difference between an observation, <span class="math inline">\(y_{ij}\)</span>,
and the model estimate, <span class="math inline">\(\widehat{y}_{ij} = \widehat{\mu}_j\)</span>, for that observation,
<span class="math inline">\(y_{ij}-\widehat{y}_{ij} = e_{ij}\)</span>. It is basically what is left over that the mean
part of the model (<span class="math inline">\(\widehat{\mu}_{j}\)</span>) does not explain. It is also a window
into how “good” the model might be because it reflects what the model was unable to explain. </p>
<p>Consider the four different fake results for a situation with four groups (<span class="math inline">\(J=4\)</span>)
displayed in Figure <a href="chapter3.html#fig:Figure3-3">3.3</a>. Which of the different results shows
the most and least evidence of differences in the means? In trying to answer
this, think about both how different the means are (obviously important) and
how variable the results are around the mean. These situations were created to
have the same means in Scenarios 1 and 2 as well as matching means in Scenarios
3 and 4. In Scenarios 1 and 2, the differences in the means is smaller than in
the other two results. But Scenario 2 should provide more evidence of what
little difference is present than Scenario 1 because it has less variability
around the means. The best situation for finding group differences here is
Scenario 4 since it has the largest difference in the means and the least
variability around those means. Our test statistic somehow needs to allow a
comparison of the variability in the means to the overall variability to help
us get results that reflect that Scenario 4 has the strongest evidence of a
difference (most variability in the means and least variability around those means) and Scenario 1 would have the least evidence (least variability in the means and most variability around those means).</p>

<div class="figure"><span id="fig:Figure3-3"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-3-1.png" alt="Demonstration of different amounts of difference in means relative to variability. Scenarios have the same means in rows and same variance around means in columns of plot. Confidence intervals not reported in the pirate-plots." width="576" />
<p class="caption">
Figure 3.3: Demonstration of different amounts of difference in means relative to variability. Scenarios have the same means in rows and same variance around means in columns of plot. Confidence intervals not reported in the pirate-plots.
</p>
</div>
<p>The statistic that allows the comparison of relative amounts of variation is called
the <strong><em>ANOVA F-statistic</em></strong>. It is developed using <strong><em>sums of squares</em></strong>

which are measures of total variation like those that are used in the numerator of the
standard deviation (<span class="math inline">\(\Sigma_1^N(y_i-\bar{y})^2\)</span>) that took all the observations,
subtracted the mean, squared the differences, and then added up the results
over all the observations to generate a measure of total variability. With
multiple groups, we will focus on decomposing that total variability
(<strong><em>Total Sums of Squares</em></strong>) into variability among the means (we’ll call this
<strong><em>Explanatory Variable</em></strong> <span class="math inline">\(\mathbf{A}\textbf{&#39;s}\)</span> <strong><em>Sums of Squares</em></strong>) and
variability in the residuals 
or errors (<strong><em>Error Sums of Squares</em></strong>). We define each of these quantities in
the One-Way ANOVA situation as follows:</p>
<ul>
<li><p><span class="math inline">\(\textbf{SS}_{\textbf{Total}} =\)</span> Total Sums of Squares
<span class="math inline">\(= \Sigma^J_{j=1}\Sigma^{n_j}_{i=1}(y_{ij}-\bar{\bar{y}})^2\)</span></p>
<ul>
<li><p>This is the total variation in the responses around the <strong><em>grand mean</em></strong> (<span class="math inline">\(\bar{\bar{y}}\)</span>, the estimated mean for all the
observations and available from the mean-only model).</p></li>
<li><p>By summing over all <span class="math inline">\(n_j\)</span> observations in each group, <span class="math inline">\(\Sigma^{n_j}_{i=1}(\ )\)</span>,
and then adding those results up across the groups, <span class="math inline">\(\Sigma^J_{j=1}(\ )\)</span>,
we accumulate the variation across all <span class="math inline">\(N\)</span> observations.</p></li>
<li><p><strong>Note</strong>: this is the residual variation if the null model is used, so there
is no further decomposition possible for that model.</p></li>
<li><p>This is also equivalent to the numerator of the sample variance,
<span class="math inline">\(\Sigma^{N}_{1}(y_{i}-\bar{y})^2\)</span> which is what you get when you ignore
the information on the potential differences in the groups.</p></li>
</ul></li>
<li><p><span class="math inline">\(\textbf{SS}_{\textbf{A}} =\)</span> Explanatory Variable <em>A</em>’s Sums of Squares
<span class="math inline">\(=\Sigma^J_{j=1}\Sigma^{n_j}_{i=1}(\bar{y}_{j}-\bar{\bar{y}})^2 =\Sigma^J_{j=1}n_j(\bar{y}_{j}-\bar{\bar{y}})^2\)</span></p>
<ul>
<li><p>This is the variation in the group means around the grand mean based on
the explanatory variable <span class="math inline">\(A\)</span>.</p></li>
<li><p>This is also called sums of squares for the treatment, regression, or model.</p></li>
</ul></li>
<li><p><span class="math inline">\(\textbf{SS}_\textbf{E} =\)</span> Error (Residual) Sums of Squares
<span class="math inline">\(=\Sigma^J_{j=1}\Sigma^{n_j}_{i=1}(y_{ij}-\bar{y}_j)^2 = \Sigma^J_{j=1}\Sigma^{n_j}_{i=1}(e_{ij})^2\)</span></p>
<ul>
<li><p>This is the variation in the responses around the group means.</p></li>
<li><p>Also called the sums of squares for the residuals, especially when using the second
version of the formula, which shows that it is just the squared residuals added
up across all the observations.</p></li>
</ul></li>
</ul>
<p>The possibly surprising result given the mass of notation just presented is that
the total sums of squares is <strong>ALWAYS</strong> equal to the sum of explanatory variable
<span class="math inline">\(A\text{&#39;s}\)</span> sum of squares and the error sums of squares,</p>
<p><span class="math display">\[\textbf{SS}_{\textbf{Total}} \mathbf{=} \textbf{SS}_\textbf{A} \mathbf{+} \textbf{SS}_\textbf{E}.\]</span></p>
<p>This result is called the <strong><em>sums of squares decomposition formula</em></strong>.

The equality implies that if the <span class="math inline">\(\textbf{SS}_\textbf{A}\)</span> goes up, then the
<span class="math inline">\(\textbf{SS}_\textbf{E}\)</span> must go down if <span class="math inline">\(\textbf{SS}_{\textbf{Total}}\)</span> remains
the same. We use these results to build our test statistic and organize this information in
what is called an <strong><em>ANOVA table</em></strong>.

The ANOVA table is generated using the
<code>anova</code> function applied to the reference-coded model, <code>lm2</code>:

</p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="chapter3.html#cb230-1"></a>lm2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>dd)</span>
<span id="cb230-2"><a href="chapter3.html#cb230-2"></a><span class="kw">anova</span>(lm2)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: Distance
##             Df  Sum Sq Mean Sq F value    Pr(&gt;F)
## Condition    6   34948  5824.7  6.5081 7.392e-07
## Residuals 5683 5086298   895.0</code></pre>
<p>Note that the ANOVA table has a row labelled <code>Condition</code>, which contains information
for the grouping variable (we’ll generally refer to this as explanatory variable
<span class="math inline">\(A\)</span> but here it is the outfit group that was randomly assigned), and a row
labeled <code>Residuals</code>, which is synonymous with “Error”. The Sums of Squares
(SS) are available in the <code>Sum Sq</code> column. It doesn’t show a row for “Total” but
the <span class="math inline">\(\textbf{SS}_{\textbf{Total}} \mathbf{=} \textbf{SS}_\textbf{A} \mathbf{+} \textbf{SS}_\textbf{E} = 5,121,246\)</span>.</p>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="chapter3.html#cb232-1"></a><span class="dv">34948</span> <span class="op">+</span><span class="st"> </span><span class="dv">5086298</span></span></code></pre></div>
<pre><code>## [1] 5121246</code></pre>

<div class="figure"><span id="fig:Figure3-4"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-4-1.png" alt="Plot of means and 95% confidence intervals for the three groups for the real overtake data (a) and three different permutations of the outfit group labels to the same responses in (b), (c), and (d). Note that SSTotal is always the same but the different amounts of variation associated with the means (SSA) or the errors (SSE) changes in permutation." width="960" />
<p class="caption">
Figure 3.4: Plot of means and 95% confidence intervals for the three groups for the real overtake data (a) and three different permutations of the outfit group labels to the same responses in (b), (c), and (d). Note that <code>SSTotal</code> is always the same but the different amounts of variation associated with the means (<code>SSA</code>) or the errors (<code>SSE</code>) changes in permutation.
</p>
</div>
<p>It may be easiest to understand the <em>sums of squares decomposition</em> by connecting
it to our permutation ideas.


In a permutation situation, the total variation
(<span class="math inline">\(SS_\text{Total}\)</span>) cannot change – it is the same responses varying
around the same grand mean. However, the amount of variation attributed to variation
among the means and in the residuals can change if we change which observations go
with which group. In Figure <a href="chapter3.html#fig:Figure3-4">3.4</a> (panel a), the means, sums of
squares, and 95% confidence intervals for each mean are displayed for the seven
groups from the original overtake data. Three permuted versions
of the data set are summarized in panels (b), (c), and (d). The <span class="math inline">\(\text{SS}_A\)</span> is 34948
in the real data set and between 857 and 4539 in the permuted data sets.
If you had
to pick among the plots for the one with the most evidence of a difference in the
means, you hopefully would pick panel (a). This visual “unusualness” suggests
that this observed result is unusual relative to the possibilities under
permutations, which are, again, the possibilities tied to having the null
hypothesis being true. But note that the differences here are not that great
between these three permuted data sets and the real one. It is likely that at
least some might have selected panel (d) as also looking like it shows
some evidence of differences, although the variation in the means in the real data set is clearly more pronounced than in this or the other permutations.</p>
<p>One way to think about <span class="math inline">\(\textbf{SS}_\textbf{A}\)</span> is that it is a function that
converts the variation in the group means into a single value. This makes it a
reasonable test statistic in a permutation testing context.

By comparing the
observed <span class="math inline">\(\text{SS}_A =\)</span> 34948 to the permutation results of
857, 3828, and 4539 we see
that the observed result is much more extreme than the three alternate versions.
In contrast to our previous test statistics where positive and negative
differences were possible, <span class="math inline">\(\text{SS}_A\)</span> is always positive with a value of 0
corresponding to no variation in the means. The larger the <span class="math inline">\(\text{SS}_A\)</span>, the more
variation there is in the means. The permutation p-value for the alternative
hypothesis of <strong>some</strong> (not of greater or less than!) difference in the true
means of the groups will involve counting the number of permuted <span class="math inline">\(SS_A^*\)</span> results
that are as large or larger than what we observed.
</p>
<p>To do a permutation test,

we need to be able to calculate and extract the
<span class="math inline">\(\text{SS}_A\)</span> value. In the ANOVA table, it is the second number in the first row;
we can use the bracket, <code>[,]</code>, referencing to extract that
number from the ANOVA table that <code>anova</code> produces with
<code>anova(lm(Distance~Condition, data=dd))[1, 2]</code>. We’ll store the observed value
of <span class="math inline">\(\text{SS}_A\)</span> in <code>Tobs</code>, reusing some ideas from Chapter <a href="chapter2.html#chapter2">2</a>.
</p>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="chapter3.html#cb234-1"></a>Tobs &lt;-<span class="st"> </span><span class="kw">anova</span>(<span class="kw">lm</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>dd))[<span class="dv">1</span>,<span class="dv">2</span>]; Tobs</span></code></pre></div>
<pre><code>## [1] 34948.43</code></pre>
<p>The following code performs the permutations <code>B=1,000</code> times using the
<code>shuffle</code> function, builds up a vector of results in <code>Tobs</code>, and then makes
a plot of the resulting permutation distribution:</p>

<div class="figure"><span id="fig:Figure3-5"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-5-1.png" alt="Histogram and density curve of permutation distribution of \(\text{SS}_A\) with the observed value of \(\text{SS}_A\) displayed as a bold, vertical line. The proportion of results that are as large or larger than the observed value of \(\text{SS}_A\) provides an estimate of the p-value." width="960" />
<p class="caption">
Figure 3.5: Histogram and density curve of permutation distribution of <span class="math inline">\(\text{SS}_A\)</span> with the observed value of <span class="math inline">\(\text{SS}_A\)</span> displayed as a bold, vertical line. The proportion of results that are as large or larger than the observed value of <span class="math inline">\(\text{SS}_A\)</span> provides an estimate of the p-value.
</p>
</div>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="chapter3.html#cb236-1"></a>B &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb236-2"><a href="chapter3.html#cb236-2"></a>Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</span>
<span id="cb236-3"><a href="chapter3.html#cb236-3"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</span>
<span id="cb236-4"><a href="chapter3.html#cb236-4"></a>  Tstar[b] &lt;-<span class="st"> </span><span class="kw">anova</span>(<span class="kw">lm</span>(Distance<span class="op">~</span><span class="kw">shuffle</span>(Condition), <span class="dt">data=</span>dd))[<span class="dv">1</span>,<span class="dv">2</span>]</span>
<span id="cb236-5"><a href="chapter3.html#cb236-5"></a>  }</span>
<span id="cb236-6"><a href="chapter3.html#cb236-6"></a><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">300</span>))</span>
<span id="cb236-7"><a href="chapter3.html#cb236-7"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb236-8"><a href="chapter3.html#cb236-8"></a><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</span>
<span id="cb236-9"><a href="chapter3.html#cb236-9"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span></code></pre></div>
<p>The right-skewed distribution (Figure <a href="chapter3.html#fig:Figure3-5">3.5</a>) contains the
distribution of <span class="math inline">\(\text{SS}^*_A\text{&#39;s}\)</span> under permutations (where
all the groups are assumed to be equivalent under the null hypothesis). The observed result is larger than all of the <span class="math inline">\(\text{SS}^*_A\text{&#39;s}\)</span>. The proportion
of permuted results that exceed the observed value is found using <code>pdata</code>
as before, except only for the area to the right of the observed result. We know
that <code>Tobs</code> will always be positive so no absolute values are required here.
</p>
<div class="sourceCode" id="cb237"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb237-1"><a href="chapter3.html#cb237-1"></a><span class="kw">pdata</span>(Tstar, Tobs, <span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<p>Because there were no permutations that exceeded the observed value, the p-value should be reported as p-value &lt; 0.001 (less than 1 in 1,000) and not 0. This suggests very strong evidence
against the null hypothesis of no difference in the true means. We would interpret
this p-value as saying that there is less than a 0.1% chance of getting a <span class="math inline">\(\text{SS}_A\)</span>
as large or larger than we observed, given that the null hypothesis is true.
</p>
<p>It ends up that some nice parametric statistical results
are available (if our assumptions are met) for the ratio of estimated variances,
the estimated variances are called <strong><em>Mean Squares</em></strong>. 

To turn sums of squares into mean square (variance) estimates,
we divide the sums of squares by the amount of free information available. For
example, remember the typical variance estimator introductory statistics,
<span class="math inline">\(\Sigma^N_1(y_i-\bar{y})^2/(N-1)\)</span>? Your instructor probably spent some time trying various
approaches to explaining why the denominator is the sample size minus 1. The most useful explanation for our
purposes moving forward is that we “lose” one piece of information to estimate
the mean and there are <span class="math inline">\(N\)</span> deviations around the single mean so we divide by
<span class="math inline">\(N-1\)</span>. The main point is that the sums of squares were divided by something and
we got an estimator for the variance, in that situation for the observations overall.</p>
<p>Now consider <span class="math inline">\(\text{SS}_E = \Sigma^J_{j=1}\Sigma^{n_j}_{i=1}(y_{ij}-\bar{y}_j)^2\)</span>
which still has <span class="math inline">\(N\)</span> deviations but it varies around the <span class="math inline">\(J\)</span> means, so the</p>
<p><span class="math display">\[\textbf{Mean Square Error} = \text{MS}_E = \text{SS}_E/(N-J).\]</span></p>
<p>Basically, we lose <span class="math inline">\(J\)</span> pieces of information in this calculation because we have
to estimate <span class="math inline">\(J\)</span> means. The similar calculation of the <strong><em>Mean Square for variable</em></strong> <span class="math inline">\(\mathbf{A}\)</span>
(<span class="math inline">\(\text{MS}_A\)</span>) is harder to see in the formula
(<span class="math inline">\(\text{SS}_A = \Sigma^J_{j=1}n_j(\bar{y}_i-\bar{\bar{y}})^2\)</span>), but the same
reasoning can be used to understand the denominator for forming <span class="math inline">\(\text{MS}_A\)</span>:
there are <span class="math inline">\(J\)</span> means that vary around the grand mean so</p>
<p><span class="math display">\[\text{MS}_A = \text{SS}_A/(J-1).\]</span></p>
<p>In summary, the two mean squares are simply:</p>
<ul>
<li><p><span class="math inline">\(\text{MS}_A = \text{SS}_A/(J-1)\)</span>, which estimates the variance of the group
means around the grand mean.</p></li>
<li><p><span class="math inline">\(\text{MS}_{\text{Error}} = \text{SS}_{\text{Error}}/(N-J)\)</span>, which estimates
the variation of the errors around the group means.</p></li>
</ul>
<!-- \newpage -->
<p>These results are put together using a ratio to define the <strong><em>ANOVA F-statistic</em></strong>
(also called the <strong><em>F-ratio</em></strong>) as: </p>
<p><span class="math display">\[F=\text{MS}_A/\text{MS}_{\text{Error}}.\]</span></p>
<p>If the variability in the means is “similar” to the variability in the residuals,
the statistic would have a value around 1. If that variability is similar then
there would be no evidence of a difference in the means. If the <span class="math inline">\(\text{MS}_A\)</span> is much
larger than the <span class="math inline">\(\text{MS}_E\)</span>, the <span class="math inline">\(F\)</span>-statistic will provide evidence against
the null hypothesis. The “size” of the <span class="math inline">\(F\)</span>-statistic is formalized by finding the
p-value. The <span class="math inline">\(F\)</span>-statistic, if assumptions discussed below are not violated and we assume
the null hypothesis is true, follows what is called an <span class="math inline">\(F\)</span>-distribution.

The
<strong><em>F-distribution</em></strong> is a right-skewed distribution whose shape is defined by what
are called the <strong><em>numerator degrees of freedom</em></strong> (<span class="math inline">\(J-1\)</span>) and the
<strong><em>denominator degrees of freedom</em></strong> (<span class="math inline">\(N-J\)</span>). These names correspond to the values
that we used to calculate the mean squares and where in the <span class="math inline">\(F\)</span>-ratio each mean
square was used; <span class="math inline">\(F\)</span>-distributions are denoted by their degrees of freedom using
the convention of <span class="math inline">\(F\)</span> (<em>numerator df</em>, <em>denominator df</em>). Some examples of
different <span class="math inline">\(F\)</span>-distributions are displayed for you in Figure <a href="chapter3.html#fig:Figure3-6">3.6</a>. </p>
<p>The characteristics of the F-distribution can be summarized as:</p>
<ul>
<li><p>Right skewed,</p></li>
<li><p>Nonzero probabilities for values greater than 0,</p></li>
<li><p>Its shape changes depending on the <strong>numerator DF</strong> and <strong>denominator DF</strong>, and</p></li>
<li><p><strong>Always use the right-tailed area for p-values.</strong></p></li>
</ul>

<div class="figure"><span id="fig:Figure3-6"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-6-1.png" alt="Density curves of four different \(F\)-distributions. Upper left is an \(F(6, 5683)\), upper right is \(F(2, 10)\), lower left is \(F(6, 10)\), and lower right is \(F(3, 20)\). P-values are found using the areas to the right of the observed \(F\)-statistic value in all F-distributions. " width="960" />
<p class="caption">
Figure 3.6: Density curves of four different <span class="math inline">\(F\)</span>-distributions. Upper left is an <span class="math inline">\(F(6, 5683)\)</span>, upper right is <span class="math inline">\(F(2, 10)\)</span>, lower left is <span class="math inline">\(F(6, 10)\)</span>, and lower right is <span class="math inline">\(F(3, 20)\)</span>. P-values are found using the areas to the right of the observed <span class="math inline">\(F\)</span>-statistic value in all F-distributions. 
</p>
</div>
<p>Now we are ready to discuss an ANOVA table since we know about each of its
components. Note the general format of the ANOVA table is in Table <a href="chapter3.html#tab:Table3-2">3.2</a><a href="#fn57" class="footnote-ref" id="fnref57"><sup>57</sup></a>:
</p>
<div style="page-break-after: always;"></div>

<table>
<caption><span id="tab:Table3-2">Table 3.2: </span> General One-Way ANOVA table.</caption>
<colgroup>
<col width="13%" />
<col width="7%" />
<col width="18%" />
<col width="21%" />
<col width="19%" />
<col width="19%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Source  </th>
<th align="left">DF </th>
<th align="left">Sums of<br />
Squares</th>
<th align="left">Mean Squares</th>
<th align="left">F-ratio</th>
<th align="left">P-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Variable A</td>
<td align="left"><span class="math inline">\(J-1\)</span></td>
<td align="left"><span class="math inline">\(\text{SS}_A\)</span></td>
<td align="left"><span class="math inline">\(\text{MS}_A=\text{SS}_A/(J-1)\)</span></td>
<td align="left"><span class="math inline">\(F=\text{MS}_A/\text{MS}_E\)</span></td>
<td align="left">Right tail of <span class="math inline">\(F(J-1,N-J)\)</span></td>
</tr>
<tr class="even">
<td align="left">Residuals</td>
<td align="left"><span class="math inline">\(N-J\)</span></td>
<td align="left"><span class="math inline">\(\text{SS}_E\)</span></td>
<td align="left"><span class="math inline">\(\text{MS}_E = \text{SS}_E/(N-J)\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="left"><span class="math inline">\(N-1\)</span></td>
<td align="left"><span class="math inline">\(\text{SS}_{\text{Total}}\)</span></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p>The table is oriented to help you reconstruct the <span class="math inline">\(F\)</span>-ratio from each of its
components. The output from R is similar although it does not provide the last row
and sometimes switches the order of columns in different functions we will use. The R version of the table for the type
of outfit effect (<code>Condition</code>) with <span class="math inline">\(J=7\)</span> levels and <span class="math inline">\(N=5,690\)</span> observations, repeated
from above, is:</p>
<div class="sourceCode" id="cb239"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb239-1"><a href="chapter3.html#cb239-1"></a><span class="kw">anova</span>(lm2)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: Distance
##             Df  Sum Sq Mean Sq F value       Pr(&gt;F)
## Condition    6   34948  5824.7  6.5081 0.0000007392
## Residuals 5683 5086298   895.0</code></pre>
<div style="page-break-after: always;"></div>
<p>The p-value from the <span class="math inline">\(F\)</span>-distribution is 0.0000007 so we can report it<a href="#fn58" class="footnote-ref" id="fnref58"><sup>58</sup></a> as a p-value &lt; 0.0001.   We can
verify this result using the observed <span class="math inline">\(F\)</span>-statistic of 6.51
(which came from taking the ratio of the two mean squares,
F=5824.74/895)
which follows an <span class="math inline">\(F(6, 5683)\)</span> distribution if the null hypothesis is true and some
other assumptions are met. Using the <code>pf</code> function provides us with areas in the
specified <span class="math inline">\(F\)</span>-distribution with the <code>df1</code> provided to the function as the
numerator <em>df</em> and <code>df2</code> as the denominator <em>df</em> and <code>lower.tail=F</code> reflecting
our desire for a right tailed area. 
</p>
<div class="sourceCode" id="cb241"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb241-1"><a href="chapter3.html#cb241-1"></a><span class="kw">pf</span>(<span class="fl">6.51</span>, <span class="dt">df1=</span><span class="dv">6</span>, <span class="dt">df2=</span><span class="dv">5683</span>, <span class="dt">lower.tail=</span>F)</span></code></pre></div>
<pre><code>## [1] 0.0000007353832</code></pre>
<p>The result from the <span class="math inline">\(F\)</span>-distribution using this parametric procedure is similar to
the p-value obtained using permutations with the test statistic of the
<span class="math inline">\(\text{SS}_A\)</span>, which was &lt; 0.0001. The <span class="math inline">\(F\)</span>-statistic obviously is another
potential test statistic to use as a test statistic in a permutation approach,
now that we know about it. We should check that we get similar results from it
with permutations as we did from using <span class="math inline">\(\text{SS}_A\)</span> as a permutation-test test
statistic. The following code generates the permutation distribution

for the
<span class="math inline">\(F\)</span>-statistic (Figure <a href="chapter3.html#fig:Figure3-7">3.7</a>) and assesses how unusual the observed
<span class="math inline">\(F\)</span>-statistic of 6.51 was in this permutation distribution.
The only change in the code involves moving from extracting <span class="math inline">\(\text{SS}_A\)</span> to
extracting the <span class="math inline">\(F\)</span>-ratio which is in the 4<sup>th</sup> column of the <code>anova</code>
output:</p>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb243-1"><a href="chapter3.html#cb243-1"></a>Tobs &lt;-<span class="st"> </span><span class="kw">anova</span>(<span class="kw">lm</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>dd))[<span class="dv">1</span>,<span class="dv">4</span>]; Tobs</span></code></pre></div>
<pre><code>## [1] 6.508071</code></pre>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="chapter3.html#cb245-1"></a>B &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb245-2"><a href="chapter3.html#cb245-2"></a>Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</span>
<span id="cb245-3"><a href="chapter3.html#cb245-3"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</span>
<span id="cb245-4"><a href="chapter3.html#cb245-4"></a>  Tstar[b] &lt;-<span class="st"> </span><span class="kw">anova</span>(<span class="kw">lm</span>(Distance<span class="op">~</span><span class="kw">shuffle</span>(Condition), <span class="dt">data=</span>dd))[<span class="dv">1</span>,<span class="dv">4</span>]</span>
<span id="cb245-5"><a href="chapter3.html#cb245-5"></a>}</span>
<span id="cb245-6"><a href="chapter3.html#cb245-6"></a></span>
<span id="cb245-7"><a href="chapter3.html#cb245-7"></a><span class="kw">pdata</span>(Tstar, Tobs, <span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="chapter3.html#cb247-1"></a><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)</span>
<span id="cb247-2"><a href="chapter3.html#cb247-2"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb247-3"><a href="chapter3.html#cb247-3"></a><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</span>
<span id="cb247-4"><a href="chapter3.html#cb247-4"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span></code></pre></div>

<div class="figure"><span id="fig:Figure3-7"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-7-1.png" alt="Histogram and density curve of the permutation distribution of the F-statistic with bold, vertical line for the observed value of the test statistic of 6.51." width="960" />
<p class="caption">
Figure 3.7: Histogram and density curve of the permutation distribution of the F-statistic with bold, vertical line for the observed value of the test statistic of 6.51.
</p>
</div>
<p>The permutation-based p-value is again at less than 1 in 1,000, which matches the other
results closely. The first conclusion is that using a test statistic of either
the <span class="math inline">\(F\)</span>-statistic or the <span class="math inline">\(\text{SS}_A\)</span> provide similar permutation results.
However, we tend to favor using the <span class="math inline">\(F\)</span>-statistic because it is more commonly used
in reporting ANOVA results, not because it is any better in a permutation context .</p>
<p>It is also interesting to compare the permutation distribution for the
<span class="math inline">\(F\)</span>-statistic and the parametric <span class="math inline">\(F(6, 6583)\)</span> distribution
(Figure <a href="chapter3.html#fig:Figure3-8">3.8</a>). They do not match perfectly but are quite similar.
Some the differences around 0 are due to the behavior of the method used to create
the density curve and are not really a problem for the methods. The similarity in
the two curves explains why both methods would give similar p-value results for almost any test statistic value. In some
situations, the correspondence will not be quite so close.</p>

<div class="figure"><span id="fig:Figure3-8"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-8-1.png" alt="Comparison of \(F(6, 6583)\) (dashed line) and permutation distribution (solid line)." width="480" />
<p class="caption">
Figure 3.8: Comparison of <span class="math inline">\(F(6, 6583)\)</span> (dashed line) and permutation distribution (solid line).
</p>
</div>
<p>So how can we rectify this result (p-value &lt; 0.0001) and the
Chapter <a href="chapter2.html#chapter2">2</a> result that reported moderate evidence against the null hypothesis of no difference between <em>commute</em>
and <em>casual</em> with a <span class="math inline">\(\text{p-value}\approx 0.04\)</span>? I selected the two groups
to compare in Chapter <a href="chapter2.html#chapter2">2</a> because they were somewhat far apart but not too far apart. I could have selected <em>police</em> and <em>polite</em> as they are furthest apart and just focused on that difference. “Cherry-picking” a comparison when many are present, especially one that is most different, without accounting for this choice creates a
false sense of the real situation and inflates the Type I error rate because of
the selection<a href="#fn59" class="footnote-ref" id="fnref59"><sup>59</sup></a>.


If the entire suite of pairwise comparisons are considered, this
result may lose some of its luster. In other words, if we consider the suite of
21 pair-wise differences (and the tests) implicit in comparing all of them,
we may need really strong evidence against the null in at least some of the pairs to suggest overall differences. In this situation,
the <em>hiviz</em> and <em>casual</em>
groups are not that different from each other so their difference does not
contribute much to the overall <span class="math inline">\(F\)</span>-test. In Section <a href="chapter3.html#section3-6">3.6</a>, we will
revisit this topic and consider a method that is
statistically valid for performing all possible pair-wise comparisons that is also
consistent with our overall test results.</p>
</div>
<div id="section3-4" class="section level2">
<h2><span class="header-section-number">3.4</span> ANOVA model diagnostics including QQ-plots</h2>
<p>The requirements for a One-Way ANOVA <span class="math inline">\(F\)</span>-test are similar to those discussed in
Chapter <a href="chapter2.html#chapter2">2</a>, except that there are now <span class="math inline">\(J\)</span> groups instead of only 2.
Specifically, the linear model assumes:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Independent observations</strong>,</p></li>
<li><p><strong>Equal variances</strong>, and</p></li>
<li><p><strong>Normal distributions</strong>.</p></li>
</ol>
<p>For assessing equal variances across the groups, it is best to use plots to assess this. We can use pirate-plots to compare the spreads of the
groups, which were provided in Figure <a href="chapter3.html#fig:Figure3-1">3.1</a>. The spreads (both in terms of extrema and rest of the distributions) should look relatively similar across the groups for you to suggest that there is not evidence of a
problem with this assumption. You should start with noting how clear or big the
violation of the conditions might be but remember that there will always be some
differences in the variation among groups even if the true variability is exactly
equal in the populations. In addition to our direct plotting, there are some
diagnostic plots available from the <code>lm</code> function that can help us more
clearly assess potential violations of the assumptions.
</p>
<p>We can obtain a suite of four diagnostic plots by using the <code>plot</code> function on
any linear model object that we have fit. To get all the plots together in four
panels we need to add the <code>par(mfrow=c(2,2))</code> command to tell R to make a graph
with 4 panels<a href="#fn60" class="footnote-ref" id="fnref60"><sup>60</sup></a>.</p>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb248-1"><a href="chapter3.html#cb248-1"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb248-2"><a href="chapter3.html#cb248-2"></a><span class="kw">plot</span>(lm2, <span class="dt">pch=</span><span class="dv">16</span>)</span></code></pre></div>
<p>There are two plots in Figure <a href="chapter3.html#fig:Figure3-9">3.9</a> with useful information for assessing the
equal variance assumption. The “Residuals vs Fitted” panel in the top left panel displays the residuals <span class="math inline">\((e_{ij} = y_{ij}-\widehat{y}_{ij})\)</span> on the y-axis and the fitted values
<span class="math inline">\((\widehat{y}_{ij})\)</span> on the x-axis.

This allows you to see if the variability of the
observations differs across the groups as a function of the mean of the groups,
because all the observations in the same group get the same fitted value – the
mean of the group. In this plot, the points seem to have fairly similar spreads
at the fitted values for the seven groups with fitted values at 114 up to 122 cm.
The “Scale-Location” plot in the lower left panel has the same x-axis of fitted values but the
y-axis contains the square-root of the absolute value of the standardized
residuals.

The standardization scales the residuals to have a variance
of 1 so help you in other displays to get a sense of how many standard deviations
you are away from the mean in the residual distribution. The absolute value transforms all the residuals into a magnitude
scale (removing direction) and the square-root helps you see differences in
variability more accurately. The visual assessment is
similar in the two plots – you want to consider whether it appears that the
groups have somewhat similar or noticeably different amounts of variability. If
you see a clear funnel shape (narrow (less variability) on the left or right and wide (more variability) at the right or left) in the Residuals vs Fitted

and/or an increase or decrease
in the height of the upper edge of points in the Scale-Location plot that may indicate a
violation of the constant variance assumption.

Remember that some variation
across the groups is expected, does not suggest a violation of a validity conditions, and means that you can proceed with trusting your inferences, but large differences in the spread are problematic for all the procedures that involve linear models. When discussing
these results, you want to discuss how clearly the differences in variation are
and whether that shows a <em>clear</em> violation of the condition of equal variance
for all observations. Like in hypothesis testing, you can never prove that an assumption is true based on a plot “looking OK”, but you can say that there is no
clear evidence that the condition is violated!
</p>

<div class="figure"><span id="fig:Figure3-9"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-9-1.png" alt="Default diagnostic plots for the full overtake data linear model." width="960" />
<p class="caption">
Figure 3.9: Default diagnostic plots for the full overtake data linear model.
</p>
</div>
<p>The linear model also assumes that all the random errors (<span class="math inline">\(\varepsilon_{ij}\)</span>) follow a
normal distribution. To gain insight into the validity of this assumption, we
can explore the original observations as displayed in the pirate-plots, mentally
subtracting off the differences in the means and focusing on the shapes of the
distributions of observations in each group. Each group should look approximately normal to avoid a concern on this assumption. These plots are especially good for
assessing whether there is a skew or are outliers present in each group.   If either skew or clear outliers are present,
by definition, the normality assumption is violated. But our assumption is
about the distribution of all the errors after removing the differences
in the means and so we want an overall assessment technique to understand how
reasonable our assumption might be overall for our model. The residuals from the entire

model provide us with estimates of the random errors and if the normality
assumption is met, then the residuals all-together should approximately follow a
normal distribution.  The <strong><em>Normal QQ-Plot</em></strong> in the upper right panel of
Figure <a href="chapter3.html#fig:Figure3-9">3.9</a> also provides a direct visual assessment of how well our
residuals match what we would expect from a normal distribution. Outliers, skew,
heavy and light-tailed aspects of distributions (all violations of normality)
show up in this plot once you learn to read it – which is our next task. To
make it easier to read QQ-plots, it is nice to start with just considering
histograms and/or density plots of the residuals and to see how that maps into
this new display. We can obtain the residuals from the linear model using the
<code>residuals</code> function  on any linear model object. Figure <a href="chapter3.html#fig:Figure3-10">3.10</a> makes both a histogram and density curve of these residuals. It shows that they have a subtle right skew present (right half of the distribution is a little more spread out than the left, so the skew is to the right) once we accounted for the different means in the groups but there are no apparent outliers.</p>

<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb249-1"><a href="chapter3.html#cb249-1"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb249-2"><a href="chapter3.html#cb249-2"></a>eij &lt;-<span class="st"> </span><span class="kw">residuals</span>(lm2)</span>
<span id="cb249-3"><a href="chapter3.html#cb249-3"></a><span class="kw">hist</span>(eij, <span class="dt">main=</span><span class="st">&quot;Histogram of residuals&quot;</span>)</span>
<span id="cb249-4"><a href="chapter3.html#cb249-4"></a><span class="kw">plot</span>(<span class="kw">density</span>(eij), <span class="dt">main=</span><span class="st">&quot;Density plot of residuals&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Density&quot;</span>,</span>
<span id="cb249-5"><a href="chapter3.html#cb249-5"></a>     <span class="dt">xlab=</span><span class="st">&quot;Residuals&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure3-10"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-10-1.png" alt="Histogram and density curve of the linear model raw residuals from the overtake data linear model." width="960" />
<p class="caption">
Figure 3.10: Histogram and density curve of the linear model raw residuals from the overtake data linear model.
</p>
</div>
<p>A Quantile-Quantile plot (<strong><em>QQ-plot</em></strong>)

shows the “match” of an observed
distribution with a theoretical distribution, almost always the normal
distribution. They are also known as Quantile Comparison, Normal Probability,
or Normal Q-Q plots, with the last two names being specific to comparing
results to a normal distribution. In this version<a href="#fn61" class="footnote-ref" id="fnref61"><sup>61</sup></a>, the QQ-plots display the value of
observed percentiles in the residual distribution on the y-axis versus the
percentiles of a theoretical normal distribution on the x-axis. If the
observed <strong>distribution of the residuals matches the shape of the normal distribution, then the plotted points should follow a 1-1 relationship.</strong>
If the points follow the displayed straight line then that suggests that the
residuals have a similar shape to a normal distribution. Some variation is
expected around the line and some patterns of deviation are worse
than others for our models, so you need to go beyond saying “it does not match
a normal distribution”. It is best to be specific about the type of deviation
you are detecting and how clear or obvious that deviation is. And to do that, we need to practice interpreting some
QQ-plots.</p>

<div class="figure"><span id="fig:Figure3-11"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-11-1.png" alt="QQ-plot of residuals from overtake data linear model." width="576" />
<p class="caption">
Figure 3.11: QQ-plot of residuals from overtake data linear model.
</p>
</div>
<p>The QQ-plot of the linear model residuals from Figure <a href="chapter3.html#fig:Figure3-9">3.9</a> is extracted and enhanced a little to make Figure <a href="chapter3.html#fig:Figure3-11">3.11</a> so we
can just focus on it.

We know from looking at the histogram that this is a
(very) slightly right skewed distribution. Either version of the QQ-plots we will work with place the observed residuals on the y-axis and the expected results for a normal distribution on the x-axis. In some plots, the <strong><em>standardized</em></strong><a href="#fn62" class="footnote-ref" id="fnref62"><sup>62</sup></a> <strong><em>residuals</em></strong> are used (Figure <a href="chapter3.html#fig:Figure3-9">3.9</a>) and in others the raw residuals are used (Figure <a href="chapter3.html#fig:Figure3-11">3.11</a>) to compare the residual distribution to a normal one. Both the upper and lower tails (upper tail in the upper right and the lower tail in the lower right of the plot) show some separation from the 1-1 line. The separation in the upper tail is more clear and these positive residuals are higher than the line “predicts” if the distribution had been normal. Being higher than the line in the right tail means being bigger than expected and so more spread out in that direction than a normal distribution should be. The left tail for the negative residuals also shows some separation from the line to have more extreme (here more negative) than expected, suggesting a little extra spread in the lower tail than suggested by a normal distribution. If the two sides had been similarly far from the 1-1 line, then we would have a symmetric and <strong><em>heavy-tailed</em></strong> distribution. Here, the slight difference in the two sides suggests that the right tail is more spread out than the left and we should be concerned about a minor violation of the normality assumption. If the distribution had followed the
normal distribution here, there would be no clear pattern of deviation from the 1-1 line (not all points need to be on the line!) and the standardized residuals would not have quite so many extreme results (over 5 in both tails). Note that the diagnostic plots will label a few points (3 by default) that might be of interest for further exploration. These identifications are not to be used for any other purpose – this is not the software identifying outliers or other problematic points – that is your responsibility to assess using these plots. For example, the point “2709” is identified in Figures <a href="chapter3.html#fig:Figure3-9">3.9</a> and <a href="chapter3.html#fig:Figure3-11">3.11</a> (the 2709<sup>th</sup>
observation in the data set) as a potentially interesting point that falls in the far right-tail of positive residuals with a raw residual of almost 160 cm. This is a great opportunity to review what residuals are and how they are calculated for this observation. First, we can extract the row for this observation and find that it was a <em>novice</em> vest observation with a distance of 274 cm (that is almost 9 feet). The fitted value for this observation can be obtained using the <code>fitted</code> function on the estimated <code>lm</code> – which here is just the sample mean of the group of the observations (<em>novice</em>) of 116.94 cm. The residual is stored in the 2,709<sup>th</sup> value of <code>eij</code> or can be calculated by taking 274 minus the fitted value of 116.94. Given the large magnitude of this passing distance (it was the maximum distance observed in the <code>Distance</code> variable), it is not too surprising that it ends up as the largest positive residual.  </p>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb250-1"><a href="chapter3.html#cb250-1"></a>dd[<span class="dv">2709</span>,<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>)]</span></code></pre></div>
<pre><code>## # A tibble: 1 x 2
##   Condition Distance
##   &lt;fct&gt;        &lt;dbl&gt;
## 1 novice         274</code></pre>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb252-1"><a href="chapter3.html#cb252-1"></a><span class="kw">fitted</span>(lm2)[<span class="dv">2709</span>]</span></code></pre></div>
<pre><code>##     2709 
## 116.9405</code></pre>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="chapter3.html#cb254-1"></a>eij[<span class="dv">2709</span>]</span></code></pre></div>
<pre><code>##     2709 
## 157.0595</code></pre>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb256-1"><a href="chapter3.html#cb256-1"></a><span class="dv">274</span><span class="fl">-116.9405</span></span></code></pre></div>
<pre><code>## [1] 157.0595</code></pre>
<p>Generally, when both tails deviate on the same side of the line (forming a
sort of quadratic curve, especially in more extreme cases), that indicates
a skewed residual distribution (the one above has a very minor skew so this does not occur) and presence of a skew is evidence of a violation of the normality assumption. To see some different potential shapes in QQ-plots, six different
data sets are displayed in Figures <a href="chapter3.html#fig:Figure3-12">3.12</a> and <a href="chapter3.html#fig:Figure3-13">3.13</a>.
In each row, a QQ-plot and associated density curve are displayed. If the points
form a pattern where all are above the 1-1 line in the lower and upper tails as in
Figure <a href="chapter3.html#fig:Figure3-12">3.12</a>(a), then the pattern is a right skew, more
extreme and easy to see than in the previous real data set.   If the points form a pattern where they are below the 1-1 line in both
tails as in Figure <a href="chapter3.html#fig:Figure3-12">3.12</a>(c), then the pattern is identified as a
left skew. Skewed residual distributions (either direction) are problematic for
models that assume normally distributed responses but not necessarily for our
permutation approaches if all the groups have similar skewed shapes. The other
problematic pattern is to have more spread than a normal curve as in
Figure <a href="chapter3.html#fig:Figure3-12">3.12</a>(e) and (f). This shows up with the points being
below the line in the left tail (more extreme negative than expected by the normal)
and the points being above the line for the right tail (more extreme positive
than the normal predicts).  We call these distributions <strong><em>heavy-tailed</em></strong>
which can manifest as distributions with outliers in both tails or just a bit
more spread out than a normal distribution. Heavy-tailed residual distributions
can be problematic for our models as the variation is greater than what the normal
distribution can account for and our methods might under-estimate the
variability in the results. The opposite pattern with the left tail above the
line and the right tail below the line suggests less spread (<strong><em>light-tailed</em></strong>)
than a normal as in Figure <a href="chapter3.html#fig:Figure3-12">3.12</a>(g) and (h). This pattern is
relatively harmless and you can proceed with methods that assume normality safely
as they will just be a little conservative. For any of the patterns, you would
note a potential violation of the normality assumption and then proceed to
describe the type of violation and how clear or extreme it seems to be. </p>

<div class="figure"><span id="fig:Figure3-12"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-12-1.png" alt="QQ-plots and density curves of four simulated distributions with different shapes." width="576" />
<p class="caption">
Figure 3.12: QQ-plots and density curves of four simulated distributions with different shapes.
</p>
</div>
<p>Finally, to help you calibrate expectations for data that are actually normally
distributed, two data sets simulated from normal distributions are displayed in
Figure <a href="chapter3.html#fig:Figure3-13">3.13</a>. Note how neither follows the line exactly but
that the overall pattern matches fairly well. <strong>You have to allow for some variation from the line in real data sets</strong> and focus on when there are really
noticeable issues in the distribution of the residuals such as those
displayed above. Again, you will never be able to prove that you have normally
distributed residuals even if the residuals are all exactly on the line, but if
you see QQ-plots as in Figure <a href="chapter3.html#fig:Figure3-12">3.12</a> you can determine that there is clear evidence of violations of the normality assumption.
  </p>

<div class="figure"><span id="fig:Figure3-13"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-13-1.png" alt="Two more simulated data sets, both generated from normal distributions." width="576" />
<p class="caption">
Figure 3.13: Two more simulated data sets, both generated from normal distributions.
</p>
</div>
<p>The last issues with assessing the assumptions in an ANOVA relates to
situations where the methods are more or less <strong><em>resistant</em></strong><a href="#fn63" class="footnote-ref" id="fnref63"><sup>63</sup></a> to violations of assumptions.

In simulation studies of the performance of the <span class="math inline">\(F\)</span>-test, researchers
have found that the
parametric ANOVA <span class="math inline">\(F\)</span>-test is more resistant to violations of the assumptions of
the normality and equal variance assumptions if the design is balanced.

A <strong><em>balanced design</em></strong> occurs when each group is measured the same number of
times. The resistance decreases as the data set becomes less balanced, as the
sample sizes in the groups are more different, so having close to balance is
preferred to a more imbalanced situation if there is a choice available. There
is some intuition available here – it makes some sense that you would have better
results in comparing groups if the information available is similar in all the
groups and none are relatively under-represented. We can check the number of
observations in each group to see if they are equal or similar using the
<code>tally</code> function from the <code>mosaic</code> package. This function is useful for
being able to get counts of observations, especially for cross-classifying
observations on two variables that is used in Chapter <a href="chapter5.html#chapter5">5</a>. For just
a single variable, we use <code>tally(~x, data=...)</code>:
</p>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb258-1"><a href="chapter3.html#cb258-1"></a><span class="kw">library</span>(mosaic)</span>
<span id="cb258-2"><a href="chapter3.html#cb258-2"></a><span class="kw">tally</span>(<span class="op">~</span>Condition, <span class="dt">data=</span>dd)</span></code></pre></div>
<pre><code>## Condition
##  casual commute   hiviz  novice  police  polite   racer 
##     779     857     737     807     790     868     852</code></pre>
<p>So the sample sizes do vary among the groups and the design is not
balanced, but all the sample sizes are between 737 and 868 so it is (in percentage terms at least) not too far from balanced. It is better then having, say, 50 in one group and 1,200 in another. This
tells us that the <span class="math inline">\(F\)</span>-test should have some resistance to violations of
assumptions. We also get more resistance to violation of assumptions as our sample sizes increase. With such as large data set here and only minor concerns with the normality assumption, the inferences generated for the means should be trustworthy and we will get similar results from parametric and nonparametric procedures. If we had only 15 observations per group and a slightly skewed residual distribution, then we might want to appeal to the permutation approach to have more trustworthy results, even if the design were balanced.</p>
</div>
<div id="section3-5" class="section level2">
<h2><span class="header-section-number">3.5</span> Guinea pig tooth growth One-Way ANOVA example</h2>
<p>A second example of the One-way ANOVA methods involves a study of length of
odontoblasts (cells that are responsible for tooth growth) in 60 Guinea Pigs
(measured in microns) from <span class="citation">Crampton (<a href="#ref-Crampton1947" role="doc-biblioref">1947</a>)</span> and is available in base R using <code>data(ToothGrowth)</code>. <span class="math inline">\(N=60\)</span> Guinea Pigs were obtained
from a local breeder and each received one of three dosages (0.5, 1, or
2 mg/day) of Vitamin C via one of two delivery methods, Orange Juice (<em>OJ</em>) or
ascorbic acid (the stuff in vitamin C capsules, called <span class="math inline">\(\text{VC}\)</span> below) as the source
of Vitamin C in their diets. Each guinea pig was randomly assigned to receive
one of the six different treatment combinations possible
(OJ at 0.5 mg, OJ at 1 mg, OJ at 2 mg, VC at 0.5 mg, VC at 1 mg, and VC at 2 mg).
The animals were treated similarly otherwise and we can assume lived in
separate cages and only one observation was taken for each guinea pig, so we
can assume the observations are independent<a href="#fn64" class="footnote-ref" id="fnref64"><sup>64</sup></a>. We need to create a variable that
combines the levels of delivery type (OJ, VC) and the dosages (0.5, 1, and 2)
to use our One-Way ANOVA on the six levels.  The <code>interaction</code> function can be
used create a new variable that is based on combinations of the levels of other
variables. Here a new variable is created in the <code>ToothGrowth</code> tibble that
we called <code>Treat</code> using the <code>interaction</code> function that provides a six-level grouping variable for our One-Way
ANOVA to compare the combinations of treatments. To get a sense of the pattern of
observations in the data set, the counts in <code>supp</code> (supplement type) and
<code>dose</code> are provided and then the counts in the new categorical explanatory variable, <code>Treat</code>.</p>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb260-1"><a href="chapter3.html#cb260-1"></a><span class="kw">data</span>(ToothGrowth) <span class="co">#Available in Base R</span></span>
<span id="cb260-2"><a href="chapter3.html#cb260-2"></a><span class="kw">library</span>(tibble)</span>
<span id="cb260-3"><a href="chapter3.html#cb260-3"></a>ToothGrowth &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(ToothGrowth) <span class="co">#Convert data.frame to tibble</span></span>
<span id="cb260-4"><a href="chapter3.html#cb260-4"></a><span class="kw">library</span>(mosaic)</span></code></pre></div>
<!-- \newpage -->
<div class="sourceCode" id="cb261"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb261-1"><a href="chapter3.html#cb261-1"></a><span class="kw">tally</span>(<span class="op">~</span>supp, <span class="dt">data=</span>ToothGrowth) <span class="co">#Supplement Type (VC or OJ)</span></span></code></pre></div>
<pre><code>## supp
## OJ VC 
## 30 30</code></pre>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb263-1"><a href="chapter3.html#cb263-1"></a><span class="kw">tally</span>(<span class="op">~</span>dose, <span class="dt">data=</span>ToothGrowth) <span class="co">#Dosage level</span></span></code></pre></div>
<pre><code>## dose
## 0.5   1   2 
##  20  20  20</code></pre>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb265-1"><a href="chapter3.html#cb265-1"></a><span class="co">#Creates a new variable Treat with 6 levels</span></span>
<span id="cb265-2"><a href="chapter3.html#cb265-2"></a>ToothGrowth<span class="op">$</span>Treat &lt;-<span class="st"> </span><span class="kw">with</span>(ToothGrowth, <span class="kw">interaction</span>(supp, dose)) </span>
<span id="cb265-3"><a href="chapter3.html#cb265-3"></a></span>
<span id="cb265-4"><a href="chapter3.html#cb265-4"></a><span class="co">#New variable that combines supplement type and dosage</span></span>
<span id="cb265-5"><a href="chapter3.html#cb265-5"></a><span class="kw">tally</span>(<span class="op">~</span>Treat, <span class="dt">data=</span>ToothGrowth) </span></code></pre></div>
<pre><code>## Treat
## OJ.0.5 VC.0.5   OJ.1   VC.1   OJ.2   VC.2 
##     10     10     10     10     10     10</code></pre>
<p>The <code>tally</code> function helps us to check for balance;

this is a balanced design
because the same number of guinea pigs (<span class="math inline">\(n_j=10 \text{ for } j=1, 2,\ldots, 6\)</span>)
were measured in each treatment combination.</p>
<p>With the variable <code>Treat</code> prepared, the first task is to visualize the results
using pirate-plots<a href="#fn65" class="footnote-ref" id="fnref65"><sup>65</sup></a> (Figure <a href="chapter3.html#fig:Figure3-14">3.14</a>) and generate some summary statistics for
each group using <code>favstats</code>.</p>
<div class="sourceCode" id="cb267"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb267-1"><a href="chapter3.html#cb267-1"></a><span class="kw">favstats</span>(len<span class="op">~</span>Treat, <span class="dt">data=</span>ToothGrowth)</span></code></pre></div>
<pre><code>##    Treat  min     Q1 median     Q3  max  mean       sd  n missing
## 1 OJ.0.5  8.2  9.700  12.25 16.175 21.5 13.23 4.459709 10       0
## 2 VC.0.5  4.2  5.950   7.15 10.900 11.5  7.98 2.746634 10       0
## 3   OJ.1 14.5 20.300  23.45 25.650 27.3 22.70 3.910953 10       0
## 4   VC.1 13.6 15.275  16.50 17.300 22.5 16.77 2.515309 10       0
## 5   OJ.2 22.4 24.575  25.95 27.075 30.9 26.06 2.655058 10       0
## 6   VC.2 18.5 23.375  25.95 28.800 33.9 26.14 4.797731 10       0</code></pre>

<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb269-1"><a href="chapter3.html#cb269-1"></a><span class="kw">pirateplot</span>(len<span class="op">~</span>Treat, <span class="dt">data=</span>ToothGrowth, <span class="dt">inf.method=</span><span class="st">&quot;ci&quot;</span>, <span class="dt">inf.disp=</span><span class="st">&quot;line&quot;</span>,</span>
<span id="cb269-2"><a href="chapter3.html#cb269-2"></a>           <span class="dt">ylab=</span><span class="st">&quot;Odontoblast Growth in microns&quot;</span>, <span class="dt">point.o=</span>.<span class="dv">7</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure3-14"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-14-1.png" alt="Pirate-plot of odontoblast growth responses for the six treatment level combinations." width="960" />
<p class="caption">
Figure 3.14: Pirate-plot of odontoblast growth responses for the six treatment level combinations.
</p>
</div>
<p>Figure <a href="chapter3.html#fig:Figure3-14">3.14</a> suggests that the mean tooth growth increases with
the dosage level and that <em>OJ</em> might lead to higher growth rates than <em>VC</em> except
at a dosage of 2 mg/day. The variability around the means looks to be small
relative to the differences among the means, so we should expect a small
p-value from our <span class="math inline">\(F\)</span>-test. The design is balanced as noted above (<span class="math inline">\(n_j=10\)</span>
for all six groups) so the methods are some what resistant to impacts from
potential non-normality and non-constant variance but we should still
assess the patterns in the plots, especially with smaller sample sizes in each group.

There is some suggestion of non-constant variance in the plots but this will be explored
further below when we can remove the difference in the means and combine all
the residuals together.

There might be some skew in the responses in some of
the groups (for example in <em>OJ.0.5</em> a right skew may be present and in <em>OJ.1</em> a left skew) but there are only 10 observations per group so
visual evidence of skew in the pirate-plots
could be generated by impacts of very few of the observations. This actually highlights an issue with residual explorations: when the sample sizes are small, our assumptions matter more than when the sample sizes are large, but when the sample sizes are small, we don’t have much information to assess the assumptions and come to a clear conclusion.</p>
<p>Now we can apply our 6+ steps for performing a hypothesis test

with these observations.</p>
<ol start="0" style="list-style-type: decimal">
<li><p>The research question is about differences in odontoblast growth across these combinations of treatments and they seem to have collected data that allow this to explored. A pirate-plot would be a good start to displaying the results and understanding all the combinations of the predictor variable.</p></li>
<li><p><strong>Hypotheses</strong>:</p>
<p><span class="math inline">\(\boldsymbol{H_0: \mu_{\text{OJ}0.5} = \mu_{\text{VC}0.5} = \mu_{\text{OJ}1} = \mu_{\text{VC}1} = \mu_{\text{OJ}2} = \mu_{\text{VC}2}}\)</span></p>
<p><strong>vs</strong></p>
<p><span class="math inline">\(\boldsymbol{H_A:}\textbf{ Not all } \boldsymbol{\mu_j} \textbf{ equal}\)</span></p>
<ul>
<li><p>The null hypothesis could also be written in reference-coding as below
since OJ.0.5 is chosen as the baseline group (discussed below).</p>
<ul>
<li><span class="math inline">\(\boldsymbol{H_0:\tau_{\text{VC}0.5}=\tau_{\text{OJ}1}=\tau_{\text{VC}1}=\tau_{\text{OJ}2}=\tau_{\text{VC}2}=0}\)</span></li>
</ul></li>
<li><p>The alternative hypothesis can be left a bit less specific:</p>
<ul>
<li><span class="math inline">\(\boldsymbol{H_A:} \textbf{ Not all } \boldsymbol{\tau_j} \textbf{ equal 0}\)</span> for <span class="math inline">\(j = 2, \ldots, 6\)</span></li>
</ul></li>
</ul></li>
<li><p><strong>Plot the data and assess validity conditions</strong>:
</p>
<ul>
<li><p>Independence:</p>
<ul>
<li><p>This is where the separate cages note above is important. Suppose that
there were cages that contained multiple animals and they competed for
food or could share illness or levels of activity. The animals in one
cage might be systematically different from the others and this
“clustering” of observations would present a potential violation of the
independence assumption.</p>
<p>If the experiment had the animals in separate
cages, there is no clear dependency in the design of the study and we can
assume<a href="#fn66" class="footnote-ref" id="fnref66"><sup>66</sup></a> that there is no problem with
this assumption.</p></li>
</ul></li>
<li><p>Constant variance:</p>
<ul>
<li>There is some indication of a difference in the
variability among the groups in the pirate-plots but the sample
size was small in each group. We need to fit the linear model to get
the other diagnostic plots to make an overall assessment.</li>
</ul>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb270-1"><a href="chapter3.html#cb270-1"></a>m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(len<span class="op">~</span>Treat, <span class="dt">data=</span>ToothGrowth)</span>
<span id="cb270-2"><a href="chapter3.html#cb270-2"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb270-3"><a href="chapter3.html#cb270-3"></a><span class="kw">plot</span>(m2,<span class="dt">pch=</span><span class="dv">16</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure3-15"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-15-1.png" alt="Diagnostic plots for the odontoblast growth model." width="960" />
<p class="caption">
Figure 3.15: Diagnostic plots for the odontoblast growth model.
</p>
</div>
<ul>
<li><p>The Residuals vs Fitted panel in Figure <a href="chapter3.html#fig:Figure3-15">3.15</a> shows some
difference in the spreads but the spread is not that different among the groups.</p></li>
<li><p>The Scale-Location plot also shows just a little less variability in the group
with the smallest fitted value but the spread of the groups looks fairly similar in
this alternative presentation related to assessing equal variance.</p></li>
<li><p>Put together, the evidence for non-constant variance is not that strong
and we can proceed comfortably that there is at least not a clear issue
with this assumption. Because of the balanced design, we also get a little more resistance to violation of the equal variance assumption.</p></li>
</ul></li>
<li><p>Normality of residuals: </p>
<ul>
<li>The Normal Q-Q plot shows a small deviation in the lower tail but nothing that
we wouldn’t expect from a normal distribution. So there is no evidence of a problem
with the normality assumption based on the upper right panel of Figure <a href="chapter3.html#fig:Figure3-15">3.15</a>. Because of the balanced design, we also get a little more resistance to violation of the normality assumption.</li>
</ul></li>
</ul></li>
<li><p><strong>Calculate the test statistic and find the p-value</strong>:</p>
<ul>
<li>The ANOVA table for our model follows, providing an <span class="math inline">\(F\)</span>-statistic of 41.557:</li>
</ul>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb271-1"><a href="chapter3.html#cb271-1"></a>m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(len<span class="op">~</span>Treat, <span class="dt">data=</span>ToothGrowth)</span>
<span id="cb271-2"><a href="chapter3.html#cb271-2"></a><span class="kw">anova</span>(m2)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: len
##           Df  Sum Sq Mean Sq F value    Pr(&gt;F)
## Treat      5 2740.10  548.02  41.557 &lt; 2.2e-16
## Residuals 54  712.11   13.19</code></pre>
<ul>
<li><p>There are two options here, especially since it seems that our assumptions about
variance and normality are not violated (note that we do not say “met” – we just have
no clear evidence against them). The parametric and nonparametric approaches should
provide similar results here.</p></li>
<li><p>The parametric approach is easiest – the p-value comes from the previous
ANOVA table as <code>&lt; 2e-16</code>. First, note that this is in scientific notation
that is a compact way of saying that the p-value here is <span class="math inline">\(2.2*10^{-16}\)</span> or
0.00000000000000022.
When you see <code>2.2e-16</code> in R output, it also means
that the calculation is at the numerical precision limits of the computer.
What R is really trying to report is that this is a very small number.
<strong>When you encounter p-values that are smaller than 0.0001, you should just report that the p-value &lt; 0.0001.</strong> Do not report that it is 0 as this gives
the false impression that there is no chance of the result occurring when
it is just a really small probability. This p-value came from an <span class="math inline">\(F(5,54)\)</span>
distribution (this is the distribution of the test statistic if the null hypothesis
is true) with an <span class="math inline">\(F\)</span>-statistic of 41.56.</p></li>
<li><p>The nonparametric approach is not too hard so we can compare the two approaches here as well:</p></li>
</ul>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb273-1"><a href="chapter3.html#cb273-1"></a>Tobs &lt;-<span class="st"> </span><span class="kw">anova</span>(<span class="kw">lm</span>(len<span class="op">~</span>Treat, <span class="dt">data=</span>ToothGrowth))[<span class="dv">1</span>,<span class="dv">4</span>]; Tobs</span></code></pre></div>
<pre><code>## [1] 41.55718</code></pre>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb275-1"><a href="chapter3.html#cb275-1"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb275-2"><a href="chapter3.html#cb275-2"></a>B &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb275-3"><a href="chapter3.html#cb275-3"></a>Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</span>
<span id="cb275-4"><a href="chapter3.html#cb275-4"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</span>
<span id="cb275-5"><a href="chapter3.html#cb275-5"></a>  Tstar[b] &lt;-<span class="st"> </span><span class="kw">anova</span>(<span class="kw">lm</span>(len<span class="op">~</span><span class="kw">shuffle</span>(Treat), <span class="dt">data=</span>ToothGrowth))[<span class="dv">1</span>,<span class="dv">4</span>]</span>
<span id="cb275-6"><a href="chapter3.html#cb275-6"></a>}</span>
<span id="cb275-7"><a href="chapter3.html#cb275-7"></a><span class="kw">pdata</span>(Tstar, Tobs, <span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb277-1"><a href="chapter3.html#cb277-1"></a><span class="kw">hist</span>(Tstar, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,Tobs<span class="op">+</span><span class="dv">3</span>))</span>
<span id="cb277-2"><a href="chapter3.html#cb277-2"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span>
<span id="cb277-3"><a href="chapter3.html#cb277-3"></a><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,Tobs<span class="op">+</span><span class="dv">3</span>), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</span>
<span id="cb277-4"><a href="chapter3.html#cb277-4"></a><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure3-16"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-16-1.png" alt="Histogram and density curve of permutation distribution for $F$-statistic for odontoblast growth data. Observed test statistic in bold, vertical line at 41.56." width="480" />
<p class="caption">
Figure 3.16: Histogram and density curve of permutation distribution for <span class="math inline">\(F\)</span>-statistic for odontoblast growth data. Observed test statistic in bold, vertical line at 41.56.
</p>
</div>
<ul>
<li><strong>The permutation p-value was reported as 0.
This should be reported as
p-value &lt; 0.001</strong> since we did 1,000 permutations and found that none of the
permuted <span class="math inline">\(F\)</span>-statistics, <span class="math inline">\(F^*\)</span>, were larger than the observed <span class="math inline">\(F\)</span>-statistic
of 41.56. The permuted results do not exceed 6 as seen in Figure
<a href="chapter3.html#fig:Figure3-16">3.16</a>, so the observed result is <em>really unusual</em> relative
to the null hypothesis. As suggested previously, the parametric and
nonparametric approaches should be similar here and they were.</li>
</ul></li>
<li><p><strong>Write a conclusion:</strong></p>
<ul>
<li><p>There is strong evidence (<span class="math inline">\(F=41.56\)</span>, permutation p-value &lt; 0.001) against the null hypothesis that the different treatments
(combinations of OJ/VC and dosage levels) have the same <strong>true</strong>
mean odontoblast growth for <strong>these</strong> guinea pigs, so we would conclude that the treatments <strong>cause</strong> at least one of the combinations to have a different true mean.</p>
<ul>
<li><p>We can make the causal statement of the treatment causing differences
because the treatments were randomly assigned but these inferences only
apply to these guinea pigs since they were not randomly selected from a
larger population.</p></li>
<li><p>Remember that we are making inferences to the population or true
means and not the sample means and want to make that clear in any
conclusion. When there is not a random sample from a population it is
more natural to discuss the true means since we can’t extend to the
population values.</p></li>
<li><p>The alternative is that there is some difference in the true means
– be sure to make the wording clear that you aren’t saying that all
the means differ. In fact, if you look back at Figure
<a href="chapter3.html#fig:Figure3-14">3.14</a>, the means for the 2 mg dosages look almost the
same so we will have a tough time arguing that all groups differ. The
<span class="math inline">\(F\)</span>-test is about finding evidence of some difference <em>somewhere</em> among the true means. The next section will
provide some additional tools to get more specific about the source of
those detected differences and allow us to get at estimates of the differences we observed to complete our interpretation.</p></li>
</ul></li>
</ul></li>
<li><p><strong>Discuss size of differences:</strong></p>
<ul>
<li><p>It appears that increasing dose levels are related to increased odontoblast growth and that the differences in dose effects change based on the type of delivery method. The difference between 7 and 26 microns for the average length of the cells could be quite interesting to the researchers. This result is harder for me to judge and likely for you than the average distances of cars to bikes but the differences could be very interesting to these researchers.</p></li>
<li><p>The “size” discussion can be further augmented by estimated pair-wise differences using methods discussed below.</p></li>
</ul></li>
<li><p><strong>Scope of inference:</strong></p>
<ul>
<li><p>We can make a causal statement of the treatment causing differences in the responses because the treatments were randomly assigned but these inferences only
apply to these guinea pigs since they were not randomly selected from a
larger population.</p>
<ul>
<li>Remember that we are making inferences to the population or true
means and not the sample means and want to make that clear. When there is not a random sample from a population it is often
more natural to discuss the true means since we can’t extend the results to the population values.</li>
</ul></li>
</ul></li>
</ol>
<p>Before we leave this example, we should revisit our model estimates and
interpretations. The default model parameterization uses reference-coding.
Running the model <code>summary</code> function on <code>m2</code> provides the estimated
coefficients:</p>
<div class="sourceCode" id="cb278"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb278-1"><a href="chapter3.html#cb278-1"></a><span class="kw">summary</span>(m2)<span class="op">$</span>coefficients</span></code></pre></div>
<!-- \newpage -->
<pre><code>##             Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)    13.23   1.148353 11.520847 3.602548e-16
## TreatVC.0.5    -5.25   1.624017 -3.232726 2.092470e-03
## TreatOJ.1       9.47   1.624017  5.831222 3.175641e-07
## TreatVC.1       3.54   1.624017  2.179781 3.365317e-02
## TreatOJ.2      12.83   1.624017  7.900166 1.429712e-10
## TreatVC.2      12.91   1.624017  7.949427 1.190410e-10</code></pre>
<p>For some practice with the reference-coding used in these models, let’s find
the estimates (fitted values) for observations for a couple of the groups. To work with the
parameters, you need to start with determining the baseline category that was
used by considering which level is not displayed in the output. The
<code>levels</code> function can list the groups in a categorical variable and their
coding in the data set. The first level is usually the baseline category but
you should check this in the model summary as well.</p>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="chapter3.html#cb280-1"></a><span class="kw">levels</span>(ToothGrowth<span class="op">$</span>Treat)</span></code></pre></div>
<pre><code>## [1] &quot;OJ.0.5&quot; &quot;VC.0.5&quot; &quot;OJ.1&quot;   &quot;VC.1&quot;   &quot;OJ.2&quot;   &quot;VC.2&quot;</code></pre>
<p>There is a <code>VC.0.5</code> in the second row of the model summary, but there is no row for
<code>0J.0.5</code> and so this must be the baseline category. That means that the fitted value
or model estimate for the <em>OJ</em> at 0.5 mg/day group is the same as the <code>(Intercept)</code> row
or <span class="math inline">\(\widehat{\alpha}\)</span>, estimating a mean tooth growth of 13.23 microns when the pigs get OJ
at a 0.5 mg/day dosage level. You should always start with working on the baseline level
in a reference-coded model. To get estimates for any other group, then you can use the
<code>(Intercept)</code> estimate and add the deviation (which could be negative) for the group of interest. For
<code>VC.0.5</code>, the estimated mean tooth growth is
<span class="math inline">\(\widehat{\alpha} + \widehat{\tau}_2 = \widehat{\alpha} + \widehat{\tau}_{\text{VC}0.5}=13.23 + (-5.25)=7.98\)</span>
microns. It is also potentially interesting to directly interpret the estimated difference
(or deviation) between <code>OJ.0.5</code> (the baseline) and <code>VC.0.5</code> (group 2) that
is <span class="math inline">\(\widehat{\tau}_{\text{VC}0.5}= -5.25\)</span>: we estimate that the mean tooth growth in
<code>VC.0.5</code> is 5.25 microns shorter than it is in <code>OJ.0.5</code>. This and many other
direct comparisons of groups are likely of interest to researchers involved in
studying the impacts of these supplements on tooth growth and the next section
will show us how to do that (correctly!).</p>
<p>The reference-coding is still going to feel a little uncomfortable so the comparison
to the cell means model and exploring the effect plot can help to reinforce
that both models patch together the same estimated means for each group. For
example, we can find our estimate of 7.98 microns for the VC0.5 group in the
output and Figure <a href="chapter3.html#fig:Figure3-17">3.17</a>. Also note that Figure
<a href="chapter3.html#fig:Figure3-17">3.17</a> is the same whether you plot the results from
<code>m2</code> or <code>m3</code>.</p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="chapter3.html#cb282-1"></a>m3 &lt;-<span class="st"> </span><span class="kw">lm</span>(len<span class="op">~</span>Treat<span class="dv">-1</span>, <span class="dt">data=</span>ToothGrowth)</span>
<span id="cb282-2"><a href="chapter3.html#cb282-2"></a><span class="kw">summary</span>(m3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = len ~ Treat - 1, data = ToothGrowth)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
##  -8.20  -2.72  -0.27   2.65   8.27 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## TreatOJ.0.5   13.230      1.148  11.521 3.60e-16
## TreatVC.0.5    7.980      1.148   6.949 4.98e-09
## TreatOJ.1     22.700      1.148  19.767  &lt; 2e-16
## TreatVC.1     16.770      1.148  14.604  &lt; 2e-16
## TreatOJ.2     26.060      1.148  22.693  &lt; 2e-16
## TreatVC.2     26.140      1.148  22.763  &lt; 2e-16
## 
## Residual standard error: 3.631 on 54 degrees of freedom
## Multiple R-squared:  0.9712, Adjusted R-squared:  0.968 
## F-statistic:   303 on 6 and 54 DF,  p-value: &lt; 2.2e-16</code></pre>

<div class="figure"><span id="fig:Figure3-17"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-17-1.png" alt="Effect plot of the One-Way ANOVA model for the odontoblast growth data." width="384" />
<p class="caption">
Figure 3.17: Effect plot of the One-Way ANOVA model for the odontoblast growth data.
</p>
</div>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb284-1"><a href="chapter3.html#cb284-1"></a><span class="kw">plot</span>(<span class="kw">allEffects</span>(m2))</span></code></pre></div>

</div>
<div id="section3-6" class="section level2">
<h2><span class="header-section-number">3.6</span> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</h2>

<p>With evidence against all the true means being equal and concluding that not all are equal, many researchers
want to explore which groups show evidence of differing from one another. This
provides information on the source of the overall difference that was
detected and detailed information on which groups differed from one another.
Because this is a shot-gun/unfocused sort of approach, some people think it
is an over-used procedure. Others feel that
it is an important method of addressing detailed questions about group
comparisons in a valid and safe way. For example, we might want to know if OJ is
different from VC <em>at the 0.5 mg/day</em> dosage level and these methods will allow
us to get an answer to this sort of question.
It also will test for differences between the OJ.0.5 and VC.2 groups
and every other pair of levels that you can construct (15 total!). This method actually
takes us back to the methods in Chapter <a href="chapter2.html#chapter2">2</a> where we compared the means of two
groups except that we need to deal with potentially many pair-wise comparisons,
making an adjustment to account for that inflation in Type I errors

that occurs due to many tests being performed at the same time. A commonly used method to make all the pair-wise comparisons that includes a correction for doing this is called <strong><em>Tukey’s Honest Significant Difference</em></strong> (Tukey’s HSD) 
method<a href="#fn67" class="footnote-ref" id="fnref67"><sup>67</sup></a>. The name suggests that not using it could lead to a
dishonest answer and that it will give you an honest result. It is more that if you don’t
do some sort of correction for all the tests you are performing, you might find some <strong><em>spurious</em></strong><a href="#fn68" class="footnote-ref" id="fnref68"><sup>68</sup></a> results. There are other methods that could be used
to do a similar correction and also provide “honest” inferences; we are just going to learn
one of them. Tukey’s method employs a different correction from the Bonferroni method discussed in Chapter <a href="chapter2.html#chapter2">2</a> but also controls the <strong><em>family-wise error rate</em></strong> 
across all the pairs being compared.</p>
<p>In pair-wise comparisons between all the pairs of means in a One-Way ANOVA, the number of
tests is based on the number of pairs. We can calculate the number of tests using
<span class="math inline">\(J\)</span> choose 2, <span class="math inline">\(\begin{pmatrix}J\\2\end{pmatrix}\)</span>, to get the number of unique pairs of
size 2 that we can make out of <span class="math inline">\(J\)</span> individual treatment levels. We don’t need to
explore the combinatorics formula for this, as the <code>choose</code> function in R can give us the
answers:</p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="chapter3.html#cb285-1"></a><span class="kw">choose</span>(<span class="dv">3</span>,<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 3</code></pre>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="chapter3.html#cb287-1"></a><span class="kw">choose</span>(<span class="dv">4</span>,<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 6</code></pre>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="chapter3.html#cb289-1"></a><span class="kw">choose</span>(<span class="dv">5</span>,<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 10</code></pre>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb291-1"><a href="chapter3.html#cb291-1"></a><span class="kw">choose</span>(<span class="dv">6</span>,<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 15</code></pre>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="chapter3.html#cb293-1"></a><span class="kw">choose</span>(<span class="dv">7</span>,<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 21</code></pre>
<p>So if you have three groups (the smallest number where we have to worry about more than one pair), there are three unique pairs to
compare. For six groups, like in the Guinea Pig study, we have to consider 15 tests to compare all the unique pairs of groups and with seven groups, there are 21 tests. Once there are more than two groups to compare, it seems like we
should be worried about inflated family-wise error rates. Fortunately, the
Tukey’s HSD method controls the family-wise error rate at your specified level
(say 0.05) across any number of pair-wise comparisons. This means that the
overall rate of at least one Type I error across all the tests is controlled at the specified
significance level, often 5%. To do this, each test must use a slightly more
conservative cut-off than if just one test is performed and the procedure helps
us figure out how much more conservative we need to be.</p>
<p>Tukey’s HSD starts  with focusing on the difference between the groups with the
largest and smallest means (<span class="math inline">\(\bar{y}_{max}-\bar{y}_{min}\)</span>). If
<span class="math inline">\((\bar{y}_{max}-\bar{y}_{min}) \le \text{Margin of Error}\)</span>
for the difference in the means, then all other pairwise differences, say
<span class="math inline">\(\vert \bar{y}_j - \bar{y}_{j&#39;}\vert\)</span>, for two groups <span class="math inline">\(j\)</span> and <span class="math inline">\(j&#39;\)</span>, will be less
than or equal to that margin of error. This also means that any confidence
intervals for any difference in the means will contain 0. Tukey’s HSD selects a
critical value so that (<span class="math inline">\(\bar{y}_{max}-\bar{y}_{min}\)</span>) will be less than the margin of
error in 95% of data sets drawn from populations with a common mean. This implies
that in 95% of data sets in which all the population means are the same, all
confidence intervals for differences in pairs of means will contain 0. Tukey’s
HSD provides confidence intervals for the difference in true means between
groups <span class="math inline">\(j\)</span> and <span class="math inline">\(j&#39;\)</span>, <span class="math inline">\(\mu_j-\mu_{j&#39;}\)</span>, for all pairs where <span class="math inline">\(j \ne j&#39;\)</span>, using</p>
<p><span class="math display">\[(\bar{y}_j - \bar{y}_{j&#39;}) \mp \frac{q^*}{\sqrt{2}}\sqrt{\text{MS}_E\left(\frac{1}{n_j}+
\frac{1}{n_{j&#39;}}\right)}\]</span></p>
<p>where
<span class="math inline">\(\frac{q^*}{\sqrt{2}}\sqrt{\text{MS}_E\left(\frac{1}{n_j}+\frac{1}{n_{j&#39;}}\right)}\)</span>
is the margin of error for the intervals. The distribution used
to find the multiplier, <span class="math inline">\(q^*\)</span>, for the confidence intervals is available in the
<code>qtukey</code> function and generally provides a slightly larger multiplier than the
regular <span class="math inline">\(t^*\)</span> from our two-sample <span class="math inline">\(t\)</span>-based confidence interval discussed in
Chapter <a href="chapter2.html#chapter2">2</a>. The formula otherwise is very similar to the one used in Chapter <a href="chapter2.html#chapter2">2</a> with the SE for the difference in the means based on a measure of residual variance (here <span class="math inline">\(MS_E\)</span>) times <span class="math inline">\(\left(\frac{1}{n_j}+\frac{1}{n_{j&#39;}}\right)\)</span> which weights the results based on the relative sample sizes in the groups.</p>
<p>We will use the <code>confint</code>, <code>cld</code>, and <code>plot</code> functions
applied to output from the <code>glht</code> function (all from the <code>multcomp</code> package;
<span class="citation">Hothorn, Bretz, and Westfall (<a href="#ref-Hothorn2008" role="doc-biblioref">2008</a>)</span>, <span class="citation">(Hothorn, Bretz, and Westfall <a href="#ref-R-multcomp" role="doc-biblioref">2019</a>)</span>) to get the required comparisons from our
ANOVA model.

Unfortunately, its code format is a little complicated – but there are
just two places to modify the code: include the model name and after <code>mcp</code>
(stands for <em>multiple comparison procedure</em>) in the <code>linfct</code> option, you need to include the
explanatory variable name as <code>VARIABLENAME="Tukey"</code>. The last part is to get the
Tukey HSD multiple comparisons run on our explanatory variable<a href="#fn69" class="footnote-ref" id="fnref69"><sup>69</sup></a>. Once we obtain the
intervals using the <code>confint</code> function or using <code>plot</code> applied to the stored results, we can use them to test <span class="math inline">\(H_0: \mu_j = \mu_{j&#39;} \text{ vs } H_A: \mu_j \ne \mu_{j&#39;}\)</span>
by assessing whether 0 is in the confidence interval for each pair. If 0 is in the
interval, then there is weak evidence against the null hypothesis for that pair, so we do not detect a difference in that pair and do not conclude that there is a difference. If 0 is not in the
interval, then we have strong evidence against <span class="math inline">\(H_0\)</span> for that pair, detect a difference, and conclude that there is a difference in that pair <em>at the specified family-wise significance
level</em>. You will see a switch to using the
word “detection” to describe null hypotheses that we find strong evidence against as it
can help to compactly write up these complicated results. The following code provides the numerical
and graphical<a href="#fn70" class="footnote-ref" id="fnref70"><sup>70</sup></a> results of applying Tukey’s HSD
to the linear model for the Guinea Pig data:</p>

<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb295-1"><a href="chapter3.html#cb295-1"></a><span class="kw">library</span>(multcomp)</span>
<span id="cb295-2"><a href="chapter3.html#cb295-2"></a>Tm2 &lt;-<span class="st"> </span><span class="kw">glht</span>(m2, <span class="dt">linfct =</span> <span class="kw">mcp</span>(<span class="dt">Treat =</span> <span class="st">&quot;Tukey&quot;</span>))</span>
<span id="cb295-3"><a href="chapter3.html#cb295-3"></a><span class="kw">confint</span>(Tm2)</span></code></pre></div>
<pre><code>## 
##   Simultaneous Confidence Intervals
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: lm(formula = len ~ Treat, data = ToothGrowth)
## 
## Quantile = 2.955
## 95% family-wise confidence level
##  
## 
## Linear Hypotheses:
##                      Estimate lwr      upr     
## VC.0.5 - OJ.0.5 == 0  -5.2500 -10.0490  -0.4510
## OJ.1 - OJ.0.5 == 0     9.4700   4.6710  14.2690
## VC.1 - OJ.0.5 == 0     3.5400  -1.2590   8.3390
## OJ.2 - OJ.0.5 == 0    12.8300   8.0310  17.6290
## VC.2 - OJ.0.5 == 0    12.9100   8.1110  17.7090
## OJ.1 - VC.0.5 == 0    14.7200   9.9210  19.5190
## VC.1 - VC.0.5 == 0     8.7900   3.9910  13.5890
## OJ.2 - VC.0.5 == 0    18.0800  13.2810  22.8790
## VC.2 - VC.0.5 == 0    18.1600  13.3610  22.9590
## VC.1 - OJ.1 == 0      -5.9300 -10.7290  -1.1310
## OJ.2 - OJ.1 == 0       3.3600  -1.4390   8.1590
## VC.2 - OJ.1 == 0       3.4400  -1.3590   8.2390
## OJ.2 - VC.1 == 0       9.2900   4.4910  14.0890
## VC.2 - VC.1 == 0       9.3700   4.5710  14.1690
## VC.2 - OJ.2 == 0       0.0800  -4.7190   4.8790</code></pre>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="chapter3.html#cb297-1"></a>old.par &lt;-<span class="st"> </span><span class="kw">par</span>(<span class="dt">mai=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>)) <span class="co">#Makes room on the plot for the group names</span></span>
<span id="cb297-2"><a href="chapter3.html#cb297-2"></a><span class="kw">plot</span>(Tm2)</span></code></pre></div>
<div class="figure"><span id="fig:Figure3-18"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-18-1.png" alt="Graphical display of pair-wise comparisons from Tukey’s HSD for the Guinea Pig data. Any confidence intervals that do not contain 0 provide strong evidence against the null hypothesis of no difference in the true means for that pair of groups." width="960" />
<p class="caption">
Figure 3.18: Graphical display of pair-wise comparisons from Tukey’s HSD for the Guinea Pig data. Any confidence intervals that do not contain 0 provide strong evidence against the null hypothesis of no difference in the true means for that pair of groups.
</p>
</div>
<p>Figure <a href="chapter3.html#fig:Figure3-18">3.18</a> contains confidence intervals for the difference in
the means for all 15 pairs of groups. For example, the first row in the plot contains
the confidence interval for comparing VC.0.5 and OJ.0.5 (VC.0.5 <strong>minus</strong> OJ.0.5). In the numerical output, you can find that this 95%
family-wise confidence interval goes from -10.05 to -0.45 microns (<code>lwr</code> and
<code>upr</code> in the numerical output provide the CI endpoints). This interval does not
contain 0 since its upper end point is -0.45 microns and so we can now say that
there is strong evidence against the null hypothesis of no difference in this pair and that we detect that OJ and VC have different true mean growth rates at the 0.5 mg
dosage level. We can go further and say that we are 95% confident that the difference
in the true mean tooth growth between VC.0.5 and OJ.0.5 (VC.0.5-OJ.0.5) is between
-10.05 and -0.45 microns, after adjusting for comparing all the pairs of groups. The center of this CI is -5.25 which is <span class="math inline">\(\widehat{\tau}_2\)</span> and the estimate difference between VC.0.5 and the baseline category of OJ.0.5. That means we can get an un-adjusted 95% confidence interval from the <code>confint</code> function to compare to this adjusted CI. The interval that does not account for all the comparisons goes from -8.51 to -1.99 microns (second row out <code>confint</code> output), showing the increased width needed in Tukey’s interval to control the family-wise error rate when many pairs are being compared. With 14 other intervals, we obviously can’t give them all this much attention…</p>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="chapter3.html#cb298-1"></a><span class="kw">confint</span>(m2)</span></code></pre></div>
<pre><code>##                  2.5 %    97.5 %
## (Intercept) 10.9276907 15.532309
## TreatVC.0.5 -8.5059571 -1.994043
## TreatOJ.1    6.2140429 12.725957
## TreatVC.1    0.2840429  6.795957
## TreatOJ.2    9.5740429 16.085957
## TreatVC.2    9.6540429 16.165957</code></pre>
<p>If you put all these pair-wise tests together, you can generate an overall
interpretation of Tukey’s HSD results that discusses sets of groups that
are not detectably different from one another and those groups that were
distinguished from other sets of groups. To do this, start with listing
out the groups that are not detectably different (CIs contain 0), which,
here, only occurs for four of the pairs. The CIs that contain 0 are for the
pairs VC.1 and OJ.0.5, OJ.2 and OJ.1, VC.2 and OJ.1, and, finally, VC.2 and
OJ.2. So VC.2, OJ.1, and OJ.2 are all not detectably different from each
other and VC.1 and OJ.0.5 are also not detectably different. If you look carefully, VC.0.5 is detected as different from every other group. So there are basically
three sets of groups that can be grouped together as “similar”: VC.2, OJ.1,
and OJ.2; VC.1 and OJ.0.5; and VC.0.5. Sometimes groups overlap with some
levels not being detectably different from other levels that belong to
different groups and the story is not as clear as it is in this case. An
example of this sort of overlap is seen in the next section.</p>
<p>There is a method that many researchers use to more efficiently generate and
report these sorts of results that is called a <strong><em>compact letter display</em></strong> 
(CLD, <span class="citation">Piepho (<a href="#ref-Piepho2004" role="doc-biblioref">2004</a>)</span>)<a href="#fn71" class="footnote-ref" id="fnref71"><sup>71</sup></a>. The <code>cld</code> function can be applied to the results from
<code>glht</code> to generate the CLD that we can use to provide a “simple” summary of
the sets of groups. In this discussion, we define a <strong>set as a union of different
groups that can contain one or more members</strong> and the member of these groups are
the different treatment levels.</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="chapter3.html#cb300-1"></a><span class="kw">cld</span>(Tm2)</span></code></pre></div>
<pre><code>## OJ.0.5 VC.0.5   OJ.1   VC.1   OJ.2   VC.2 
##    &quot;b&quot;    &quot;a&quot;    &quot;c&quot;    &quot;b&quot;    &quot;c&quot;    &quot;c&quot;</code></pre>
<!-- \newpage -->
<p>Groups with the same letter are not detectably different (are in the same set)
and groups that are detectably different get different letters (are in
different sets). Groups can have more than one letter to reflect “overlap”
between the sets of groups and sometimes a set of groups contains only a
single treatment level (VC.0.5 is a set of size 1). Note
that if the groups have the same letter, this does not mean they are the same,
just that there is <strong>insufficient evidence to declare a difference for that pair</strong>. If we consider
the previous output for the CLD, the “a” set contains VC.0.5, the “b” set contains
OJ.0.5 and VC.1, and the “c” set contains OJ.1, OJ.2, and VC.2. These are exactly
the groups of treatment levels that we obtained by going through all fifteen
pairwise results.</p>
<p>One benefit of this work is that the CLD letters can be added to a plot (such as the pirate-plot) to
help fully report the results and understand the sorts of differences Tukey’s
HSD detected. The code with <code>text</code>  involves placing text on the figure. In the <code>text</code> function, the x and y axis locations are specified (x-axis goes from 1 to 6 for the 6 categories) as well as the text to add (the CLD here). Some trial and error for locations may be needed to get the letters to be easily seen in a given pirate-plot. Figure <a href="chapter3.html#fig:Figure3-19">3.19</a> enhances the discussion by showing that the
“<b><font color='blue'>a</font></b>” group with VC.0.5 had the lowest average tooth
growth, the “<b><font color='red'>b</font></b>” group had intermediate tooth growth
for treatments OJ.0.5 and VC.1, and the highest growth rates came from
OJ.1, OJ.2, and VC.2. Even though VC.2 had the highest average growth rate,
we are not able to prove that its true mean is any higher
than the other groups labeled with “<b><font color='green'>c</font></b>”. Hopefully the
ease of getting to the story of the Tukey’s HSD results from a plot like this
explains why it is common to report results using these methods instead of
reporting 15 confidence intervals for all the pair-wise differences, either in a table or the plot.</p>

<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="chapter3.html#cb302-1"></a><span class="co">#Options theme=2,inf.f.o = 0,point.o = .5 added to focus on CLD</span></span>
<span id="cb302-2"><a href="chapter3.html#cb302-2"></a><span class="kw">pirateplot</span>(len<span class="op">~</span>Treat, <span class="dt">data=</span>ToothGrowth, <span class="dt">ylab=</span><span class="st">&quot;Growth (microns)&quot;</span>, <span class="dt">inf.method=</span><span class="st">&quot;ci&quot;</span>, <span class="dt">inf.disp=</span><span class="st">&quot;line&quot;</span>,</span>
<span id="cb302-3"><a href="chapter3.html#cb302-3"></a>           <span class="dt">theme=</span><span class="dv">2</span>, <span class="dt">inf.f.o =</span> <span class="fl">0.3</span>, <span class="dt">point.o =</span> <span class="fl">.5</span>) </span>
<span id="cb302-4"><a href="chapter3.html#cb302-4"></a><span class="kw">text</span>(<span class="dt">x=</span><span class="dv">2</span>,<span class="dt">y=</span><span class="dv">10</span>,<span class="st">&quot;a&quot;</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">cex=</span><span class="fl">1.5</span>) <span class="co">#CLD added</span></span>
<span id="cb302-5"><a href="chapter3.html#cb302-5"></a><span class="kw">text</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">4</span>),<span class="dt">y=</span><span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">18</span>),<span class="st">&quot;b&quot;</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">cex=</span><span class="fl">1.5</span>)</span>
<span id="cb302-6"><a href="chapter3.html#cb302-6"></a><span class="kw">text</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">6</span>),<span class="dt">y=</span><span class="kw">c</span>(<span class="dv">25</span>,<span class="dv">28</span>,<span class="dv">28</span>),<span class="st">&quot;c&quot;</span>,<span class="dt">col=</span><span class="st">&quot;green&quot;</span>,<span class="dt">cex=</span><span class="fl">1.5</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure3-19"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-19-1.png" alt="Pirate-plot of odontoblast growth by group with Tukey’s HSD compact letter display. Note some extra pirate-plot options are used to enhance focus on the CLD results." width="960" />
<p class="caption">
Figure 3.19: Pirate-plot of odontoblast growth by group with Tukey’s HSD compact letter display. Note some extra pirate-plot options are used to enhance focus on the CLD results.
</p>
</div>
<p>There are just a couple of other details to mention on this set of methods. First,
note that we interpret the set of confidence intervals simultaneously: We are
95% confident that <strong>ALL</strong> the intervals contain the respective differences in
the true means (this is a <strong><em>family-wise interpretation</em></strong>). These intervals are
adjusted from our regular two-sample <span class="math inline">\(t\)</span> intervals that came from <code>lm</code> from Chapter <a href="chapter2.html#chapter2">2</a>
to allow this stronger interpretation. Specifically,
they are wider. Second, if sample sizes are unequal in the groups, Tukey’s HSD
is conservative and provides a family-wise error rate that is lower than the
<em>nominal</em> (or specified) level. In other words, it fails less often than expected
and the intervals provided are a little wider than needed, containing all the
pairwise differences at higher than the nominal confidence level of (typically)
95%. Third, this is a parametric approach and violations of normality and
constant variance will push the method in the other direction, potentially
making the technique dangerously liberal. Nonparametric approaches to this
problem are also possible, but will not be considered here.</p>
<p>Tukey’s HSD results can also be displayed as p-values for each pair-wise test result. This is a little less common but can allow you to directly assess the strength of evidence for a particular pair instead of using the detected/not result that the family-wise CIs provide. But the family-wise CIs are useful for exploring the size of the differences in the pairs and we need to simplify things to detect/not in these situations because there are so many tests. But if you want to see the Tukey HSD p-values, you can use</p>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb303-1"><a href="chapter3.html#cb303-1"></a><span class="kw">summary</span>(Tm2)</span></code></pre></div>
<pre><code>## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: lm(formula = len ~ Treat, data = ToothGrowth)
## 
## Linear Hypotheses:
##                      Estimate Std. Error t value Pr(&gt;|t|)
## VC.0.5 - OJ.0.5 == 0   -5.250      1.624  -3.233  0.02424
## OJ.1 - OJ.0.5 == 0      9.470      1.624   5.831  &lt; 0.001
## VC.1 - OJ.0.5 == 0      3.540      1.624   2.180  0.26411
## OJ.2 - OJ.0.5 == 0     12.830      1.624   7.900  &lt; 0.001
## VC.2 - OJ.0.5 == 0     12.910      1.624   7.949  &lt; 0.001
## OJ.1 - VC.0.5 == 0     14.720      1.624   9.064  &lt; 0.001
## VC.1 - VC.0.5 == 0      8.790      1.624   5.413  &lt; 0.001
## OJ.2 - VC.0.5 == 0     18.080      1.624  11.133  &lt; 0.001
## VC.2 - VC.0.5 == 0     18.160      1.624  11.182  &lt; 0.001
## VC.1 - OJ.1 == 0       -5.930      1.624  -3.651  0.00739
## OJ.2 - OJ.1 == 0        3.360      1.624   2.069  0.31868
## VC.2 - OJ.1 == 0        3.440      1.624   2.118  0.29372
## OJ.2 - VC.1 == 0        9.290      1.624   5.720  &lt; 0.001
## VC.2 - VC.1 == 0        9.370      1.624   5.770  &lt; 0.001
## VC.2 - OJ.2 == 0        0.080      1.624   0.049  1.00000
## (Adjusted p values reported -- single-step method)</code></pre>
<p>These reinforce the strong evidence for many of the pairs and less strong evidence for four pairs that were not detected to be different. So these p-values provide another method to employ to report the Tukey’s HSD results – you would only need to report and explore the confidence intervals or the p-values, not both.</p>
<p>Tukey’s HSD does not require you
to find a small p-value from your overall <span class="math inline">\(F\)</span>-test to employ the methods
but if you apply it to situations with p-values larger than your
<em>a priori</em> significance level,
you are unlikely to find any pairs that are detected as being different. Some
statisticians suggest that you shouldn’t employ follow-up tests such as Tukey’s
HSD when there is not sufficient evidence to reject the overall null hypothesis.</p>
</div>
<div id="section3-7" class="section level2">
<h2><span class="header-section-number">3.7</span> Pair-wise comparisons for the Overtake data</h2>
<p>In our previous work with the overtake data, the overall ANOVA test
led to a conclusion that there is some difference in the true means across the
seven groups with a p-value &lt; 0.001 giving very strong evidence against the null hypothesis of them all being equal. The original authors followed up their overall <span class="math inline">\(F\)</span>-test with comparing every pair of outfits using one of the other methods for multiple testing adjustments available in the <code>p.adjust</code> function and detected differences between the <em>police</em> outfit and all others except for <em>hiviz</em> and no other pairs had p-values less than 0.05 using their approach. We will employ the Tukey’s HSD approach to address the same exploration and get basically the same results as they obtained, as well as estimated differences in the means in all the pairs of groups.</p>
<div style="page-break-after: always;"></div>
<p>The code is similar<a href="#fn72" class="footnote-ref" id="fnref72"><sup>72</sup></a> to the previous example focusing on the <code>Condition</code> variable for the 21 pairs to compare. To make these results easier to read and generally to make all the results with seven groups easier to understand, we can sort the levels of the explanatory based on the values in the response, using something like the the means or medians of the responses for the groups. This does not change the analyses (the <span class="math inline">\(F\)</span>-statistic and all pair-wise comparisons are the same), it just sorts them to be easier to discuss. Note that it might change the baseline group so would impact the reference-coded model even though the fitted values are the same. Specifically, we can use the <code>reorder</code> function  based on the mean using something like <code>reorder(FACTORVARIABLE, RESPONSEVARIABLE, FUN=mean)</code>. Unfortunately the <code>reorder</code> function doesn’t have a <code>data=...</code> option, so we will let the function know where to find the two variables with a wrapper around it of <code>with(DATASETNAME, reorder(...))</code>; this approach saves us from having to use <code>dd$...</code> to reference each variable. I like to put this “reordered” factor into a new variable so I can always go back to the other version if I want it. The code creates <code>Condition2</code> here and checking the levels for it and the original <code>Condition</code> variable show the change in the order of the levels of the two factor variables:</p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="chapter3.html#cb305-1"></a>dd<span class="op">$</span>Condition2 &lt;-<span class="st"> </span><span class="kw">with</span>(dd, <span class="kw">reorder</span>(Condition, Distance, mean))</span>
<span id="cb305-2"><a href="chapter3.html#cb305-2"></a><span class="kw">levels</span>(dd<span class="op">$</span>Condition)</span></code></pre></div>
<pre><code>## [1] &quot;casual&quot;  &quot;commute&quot; &quot;hiviz&quot;   &quot;novice&quot;  &quot;police&quot;  &quot;polite&quot;  &quot;racer&quot;</code></pre>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="chapter3.html#cb307-1"></a><span class="kw">levels</span>(dd<span class="op">$</span>Condition2)</span></code></pre></div>
<pre><code>## [1] &quot;polite&quot;  &quot;commute&quot; &quot;racer&quot;   &quot;novice&quot;  &quot;casual&quot;  &quot;hiviz&quot;   &quot;police&quot;</code></pre>
<p>And to verify that this worked, we can compare the means based on <code>Condition</code> and <code>Condition2</code>, and now it is even more clear which groups have the smallest and largest mean passing distances:</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="chapter3.html#cb309-1"></a><span class="kw">mean</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>dd)</span></code></pre></div>
<pre><code>##   casual  commute    hiviz   novice   police   polite    racer 
## 117.6110 114.6079 118.4383 116.9405 122.1215 114.0518 116.7559</code></pre>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="chapter3.html#cb311-1"></a><span class="kw">mean</span>(Distance<span class="op">~</span>Condition2, <span class="dt">data=</span>dd)</span></code></pre></div>
<pre><code>##   polite  commute    racer   novice   casual    hiviz   police 
## 114.0518 114.6079 116.7559 116.9405 117.6110 118.4383 122.1215</code></pre>
<p>In Figure <a href="chapter3.html#fig:Figure3-20">3.20</a>, the 95% family-wise confidence intervals are displayed. There are only five pairs that have confidence intervals that do not contain 0 and all contain comparisons of the <em>police</em> group with others. So there is a detectable difference between <em>police</em> and <em>polite</em>, <em>commute</em>, <em>racer</em>, <em>novice</em>, and <em>casual</em>. The <em>police</em> versus <em>casual</em> comparison is hard to see whether 0 is in the interval or not in the plot, but the confidence interval goes from 0.06 to 8.97 cm (look at the results from <code>confint</code>), so suggests sufficient evidence to detect a difference in these groups (barely!) at the 5% family-wise significance level.</p>

<div class="figure"><span id="fig:Figure3-20"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-20-1.png" alt="Tukey’s HSD confidence interval results at the 95% family-wise confidence level for the overtake distances linear model using the new Condition2 explanatory variable." width="960" />
<p class="caption">
Figure 3.20: Tukey’s HSD confidence interval results at the 95% family-wise confidence level for the overtake distances linear model using the new <code>Condition2</code> explanatory variable.
</p>
</div>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb313-1"><a href="chapter3.html#cb313-1"></a>lm2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span>Condition2, <span class="dt">data=</span>dd)</span>
<span id="cb313-2"><a href="chapter3.html#cb313-2"></a><span class="kw">library</span>(multcomp)</span>
<span id="cb313-3"><a href="chapter3.html#cb313-3"></a>TmOV &lt;-<span class="st"> </span><span class="kw">glht</span>(lm2, <span class="dt">linfct =</span> <span class="kw">mcp</span>(<span class="dt">Condition2 =</span> <span class="st">&quot;Tukey&quot;</span>))</span></code></pre></div>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="chapter3.html#cb314-1"></a><span class="kw">confint</span>(TmOv)</span></code></pre></div>
<pre><code>## 
##   Simultaneous Confidence Intervals
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: lm(formula = Distance ~ Condition2, data = dd)
## 
## Quantile = 2.9486
## 95% family-wise confidence level
##  
## 
## Linear Hypotheses:
##                       Estimate lwr      upr     
## commute - polite == 0  0.55609 -3.69182  4.80400
## racer - polite == 0    2.70403 -1.55015  6.95820
## novice - polite == 0   2.88868 -1.42494  7.20230
## casual - polite == 0   3.55920 -0.79441  7.91281
## hiviz - polite == 0    4.38642 -0.03208  8.80492
## police - polite == 0   8.06968  3.73207 12.40728
## racer - commute == 0   2.14793 -2.11975  6.41562
## novice - commute == 0  2.33259 -1.99435  6.65952
## casual - commute == 0  3.00311 -1.36370  7.36991
## hiviz - commute == 0   3.83033 -0.60118  8.26183
## police - commute == 0  7.51358  3.16273 11.86443
## novice - racer == 0    0.18465 -4.14844  4.51774
## casual - racer == 0    0.85517 -3.51773  5.22807
## hiviz - racer == 0     1.68239 -2.75512  6.11991
## police - racer == 0    5.36565  1.00868  9.72262
## casual - novice == 0   0.67052 -3.76023  5.10127
## hiviz - novice == 0    1.49774 -2.99679  5.99227
## police - novice == 0   5.18100  0.76597  9.59603
## hiviz - casual == 0    0.82722 -3.70570  5.36015
## police - casual == 0   4.51048  0.05637  8.96458
## police - hiviz == 0    3.68326 -0.83430  8.20081</code></pre>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="chapter3.html#cb316-1"></a><span class="kw">cld</span>(TmOv, <span class="dt">abseps=</span><span class="fl">0.1</span>)</span></code></pre></div>
<pre><code>##  polite commute   racer  novice  casual   hiviz  police 
##     &quot;a&quot;     &quot;a&quot;     &quot;a&quot;     &quot;a&quot;     &quot;a&quot;    &quot;ab&quot;     &quot;b&quot;</code></pre>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="chapter3.html#cb318-1"></a>old.par &lt;-<span class="st"> </span><span class="kw">par</span>(<span class="dt">mai=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="fl">2.5</span>,<span class="dv">1</span>,<span class="dv">1</span>)) <span class="co">#Makes room on the plot for the group names</span></span>
<span id="cb318-2"><a href="chapter3.html#cb318-2"></a><span class="kw">plot</span>(TmOv)</span></code></pre></div>
<p>The CLD also reinforces the previous discussion of which levels were detected as different and elucidates the other aspects of the results. Specifically, <em>police</em> is in a group with <em>hiviz</em> only (group “b”, not detectably different). But <em>hiviz</em> is also in a group with all the other levels so also is in group “a”. Figure <a href="chapter3.html#fig:Figure3-21">3.21</a> adds the CLD to the pirate-plot with the sorted means to help visually present these results with the original data, reiterating the benefits of sorting factor levels to make these plots easier to read. To wrap up this example (finally), we can see that we found that there was clear evidence against the null hypothesis of no difference in the true means, so concluded that there was some difference. The follow-up explorations show that we can really only suggest that the <em>police</em> outfit has detectably different mean distances and that is only for five of the six other levels. So if you are bike commuter (in the UK near London?), you are left to consider the size of this difference. The biggest estimated mean difference was 8.07 cm (3.2 inches) between <em>police</em> and <em>polite</em>. Do you think it is worth this potential extra average distance, especially given the wide variability in the distances, to make and then wear this vest? It is interesting that this result is found but it also is a fairly minimal size of a difference. It required an extremely large data set to detect these differences because the differences in the means are not very large relative to the variability in the responses. It seems like there might be many other reasons for why overtake distances vary that were not included our suite of predictors (they explored traffic volume in the paper as one other factor but we don’t have that in our data set) or maybe it is just unexplainably variable. But it makes me wonder whether it matters what I wear when I bike and whether it has an impact that matters for average overtake distances – even in the face of these “statistically significant” results. But maybe there is an impact on the “close calls” as you can see some differences in the lower tails of the distributions across the groups. The authors looked at the rates of “closer” overtakes by classifying the distances as either less than 100 cm (39.4 inches) as <em>closer</em> or not and also found some interesting results. Chapter <a href="chapter5.html#chapter5">5</a> discusses a method called a Chi-square test of Homogeneity that would be appropriate here and allow for an analysis of the rates of closer passes and this study is revisited in the Practice Problems (Section <a href="chapter5.html#section5-14">5.14</a>) there. It ends up showing that rates of “closer passes” are smallest in the <em>police</em> group.</p>

<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb319-1"><a href="chapter3.html#cb319-1"></a><span class="kw">pirateplot</span>(Distance<span class="op">~</span>Condition2, <span class="dt">data=</span>dd, <span class="dt">ylab=</span><span class="st">&quot;Distance (cm)&quot;</span>, <span class="dt">inf.method=</span><span class="st">&quot;ci&quot;</span>,</span>
<span id="cb319-2"><a href="chapter3.html#cb319-2"></a>           <span class="dt">inf.disp=</span><span class="st">&quot;line&quot;</span>, <span class="dt">theme=</span><span class="dv">2</span>) </span>
<span id="cb319-3"><a href="chapter3.html#cb319-3"></a><span class="kw">text</span>(<span class="dt">x=</span><span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,<span class="dt">y=</span><span class="dv">200</span>,<span class="st">&quot;a&quot;</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">cex=</span><span class="fl">1.5</span>) <span class="co">#CLD added</span></span>
<span id="cb319-4"><a href="chapter3.html#cb319-4"></a><span class="kw">text</span>(<span class="dt">x=</span><span class="fl">5.9</span>,<span class="dt">y=</span><span class="dv">210</span>,<span class="st">&quot;a&quot;</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">cex=</span><span class="fl">1.5</span>)</span>
<span id="cb319-5"><a href="chapter3.html#cb319-5"></a><span class="kw">text</span>(<span class="dt">x=</span><span class="fl">6.1</span>,<span class="dt">y=</span><span class="dv">210</span>,<span class="st">&quot;b&quot;</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">cex=</span><span class="fl">1.5</span>)</span>
<span id="cb319-6"><a href="chapter3.html#cb319-6"></a><span class="kw">text</span>(<span class="dt">x=</span><span class="dv">7</span>,<span class="dt">y=</span><span class="dv">215</span>,<span class="st">&quot;b&quot;</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">cex=</span><span class="fl">1.5</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figure3-21"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-21-1.png" alt="Pirate-plot of overtake distances by group, sorted by sample means with Tukey’s HSD CLD displayed." width="576" />
<p class="caption">
Figure 3.21: Pirate-plot of overtake distances by group, sorted by sample means with Tukey’s HSD CLD displayed.
</p>
</div>
</div>
<div id="section3-8" class="section level2">
<h2><span class="header-section-number">3.8</span> Chapter summary</h2>
<p>In this chapter, we explored methods for comparing a quantitative response across
<span class="math inline">\(J\)</span> groups (<span class="math inline">\(J \ge 2\)</span>), with what is called the One-Way ANOVA procedure. The initial
test is based on assessing evidence against a null hypothesis of no difference in the true means for the <span class="math inline">\(J\)</span> groups. There are two different methods for estimating these One-Way ANOVA models:
the cell means model and the reference-coded versions of the model.
There are times when either model will be preferred, but for the rest of the text,
the reference coding is used (sorry!). The ANOVA <span class="math inline">\(F\)</span>-statistic,
often presented with
underlying information in the ANOVA table,

provides a method of assessing evidence
against the null hypothesis either using permutations or via the <span class="math inline">\(F\)</span>-distribution.
Pair-wise comparisons using Tukey’s HSD provide a method for comparing all the groups
and are a nice complement to the overall ANOVA results. A compact letter display was
shown that enhanced the interpretation of Tukey’s HSD result.</p>
<p>In the Guinea Pig example, we are left with some lingering questions based on these
results. It appears that the effect of <em>dosage</em> changes as a function of the
<em>delivery method</em> (OJ, VC) because the size of the differences between OJ and VC
change for different dosages. These methods can’t directly assess the question
of whether the effect of delivery method is the same or not across the
different dosages. In Chapter <a href="chapter4.html#chapter4">4</a>, the two variables, <em>Dosage</em> and
<em>Delivery method</em> are modeled as two separate variables so we can consider their
effects both separately and together. This allows more refined hypotheses, such as
<em>Is the effect of delivery method the same for all dosages?</em>, to be tested. This
will introduce new models and methods for analyzing data where there are two
factors as explanatory variables in a model for a quantitative response variable
in what is called the Two-Way ANOVA.</p>
<div style="page-break-after: always;"></div>
</div>
<div id="section3-9" class="section level2">
<h2><span class="header-section-number">3.9</span> Summary of important R code</h2>
<p>The main components of R code used in this chapter follow with components to
modify in lighter and/or ALL CAPS text, remembering that any R packages mentioned
need to be installed and loaded for this code to have a chance of working:</p>
<ul>
<li><p><strong><font color='red'>MODELNAME</font> &lt;- lm(<font color='red'>Y</font>~<font color='red'>X</font>,
data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li><p>Probably the most frequently used command in R.</p></li>
<li><p>Here it is used to fit the reference-coded One-Way ANOVA model with Y as the
response variable and X as the grouping variable, storing the estimated model
object in MODELNAME. Remember that X should be defined as a factor variable. </p></li>
</ul></li>
<li><p><strong><font color='red'>MODELNAME</font> &lt;- lm(<font color='red'>Y</font>~<font color='red'>X</font>-1,
data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li>Fits the cell means version of the One-Way ANOVA model.</li>
</ul></li>
<li><p><strong>summary(<font color='red'>MODELNAME</font>)</strong></p>
<ul>
<li>Generates model summary information including the estimated model coefficients,
SEs, t-tests, and p-values.</li>
</ul></li>
<li><p><strong>anova(<font color='red'>MODELNAME</font>)</strong></p>
<ul>
<li><p>Generates the ANOVA table but <strong>must only be run on the
reference-coded version of the model</strong>. </p></li>
<li><p>Results are incorrect if run on the cell means model since the reduced model
under the null is that the mean of all the observations is 0!</p></li>
</ul></li>
<li><p><strong>pf(<font color='red'>FSTATISTIC</font>, df1=<font color='red'>NUMDF</font>,
df2=<font color='red'>DENOMDF</font>, lower.tail=F)</strong></p>
<ul>
<li>Finds the p-value for an observed <span class="math inline">\(F\)</span>-statistic with NUMDF and DENOMDF degrees
of freedom. </li>
</ul></li>
<li><p><strong>par(mfrow=c(2,2)); plot(<font color='red'>MODELNAME</font>)</strong></p>
<ul>
<li>Generates four diagnostic plots including the Residuals vs Fitted and
Normal Q-Q plot.</li>
</ul></li>
<li><p><strong>plot(allEffects(<font color='red'>MODELNAME</font>))</strong></p>
<ul>
<li><p>Requires the <code>effects</code> package be loaded.</p></li>
<li><p>Plots the estimated model component. </p></li>
</ul></li>
<li><p><strong>Tm2 &lt;- glht(<font color='red'>MODELNAME</font>, linfct=mcp(<font color='red'>X</font>=“Tukey”));
confint(Tm2); plot(Tm2); summary(Tm2); cld(Tm2)</strong></p>
<ul>
<li><p>Requires the <code>multcomp</code> package to be installed and loaded.</p></li>
<li><p>Can only be run on the reference-coded version of the model.</p></li>
<li><p>Generates the text output and plot for Tukey’s HSD as well as the compact
letter display information. </p></li>
</ul></li>
</ul>
<div style="page-break-after: always;"></div>
</div>
<div id="section3-10" class="section level2">
<h2><span class="header-section-number">3.10</span> Practice problems</h2>
<p>3.1. <strong>Cholesterol Analysis</strong> For the first practice problems, you will work with the cholesterol data set from
the <code>multcomp</code> package that was used to generate the Tukey’s HSD results. To
load the data set and learn more about the study, use the following code:</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="chapter3.html#cb320-1"></a><span class="kw">library</span>(multcomp)</span>
<span id="cb320-2"><a href="chapter3.html#cb320-2"></a><span class="kw">data</span>(cholesterol)</span>
<span id="cb320-3"><a href="chapter3.html#cb320-3"></a><span class="kw">library</span>(tibble)</span>
<span id="cb320-4"><a href="chapter3.html#cb320-4"></a>cholesterol &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(cholesterol)</span>
<span id="cb320-5"><a href="chapter3.html#cb320-5"></a><span class="kw">help</span>(cholesterol)</span></code></pre></div>
<p>3.1.1. Graphically explore the differences in the changes in Cholesterol levels for
the five levels using pirate-plots.</p>
<p>3.1.2. Is the design balanced? How can assess this? </p>
<p>3.1.3. Complete all 6+ steps of the hypothesis test using the parametric <span class="math inline">\(F\)</span>-test,
reporting the ANOVA table and the distribution of the test statistic under the null. When you discuss the scope of inference, make sure you note that the treatment levels
were randomly assigned to volunteers in the study.</p>
<p>3.1.4. Generate the permutation distribution and find the p-value. Compare the parametric
p-value to the permutation test results.</p>
<p>3.1.5. Perform Tukey’s HSD on the data set. Discuss the results – which pairs were
detected as different and which were not? Bigger reductions in cholesterol are
good, so are there any levels you would recommend or that might provide similar
reductions?</p>
<p>3.1.6. Find and interpret the CLD and compare that to your interpretation of results
from 3.1.5.</p>

<p>3.2. <strong>Sting Location Analysis</strong> These data come from <span class="citation">(Smith <a href="#ref-Smith2014" role="doc-biblioref">2014</a>)</span> where the author experimented on himself by daily stinging himself five times on randomly selected body locations over the course of months. You can read more about this fascinating (and cringe inducing) study at <a href="https://peerj.com/articles/338/" class="uri">https://peerj.com/articles/338/</a>. The following code gets the data prepared for analysis by removing the observations he took each day on how painful it was to sting himself on his forearm before and after the other three observations that were of interest each day of the study. It also sorts of the levels (there are many) based on the mean pain rating in each group using the <code>reorder</code> function.</p>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb321-1"><a href="chapter3.html#cb321-1"></a><span class="kw">library</span>(readr)</span>
<span id="cb321-2"><a href="chapter3.html#cb321-2"></a>sd_fixed &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/stingdata_fixed.csv&quot;</span>)</span>
<span id="cb321-3"><a href="chapter3.html#cb321-3"></a>sd_fixed<span class="op">$</span>BLs &lt;-<span class="st"> </span>sd_fixed<span class="op">$</span>Body_Location </span>
<span id="cb321-4"><a href="chapter3.html#cb321-4"></a>sd_fixed<span class="op">$</span>BLs &lt;-<span class="st"> </span><span class="kw">with</span>(sd_fixed, <span class="kw">reorder</span>(Body_Location, Rating, mean))</span>
<span id="cb321-5"><a href="chapter3.html#cb321-5"></a>sd_fixedR &lt;-<span class="st"> </span><span class="kw">subset</span>(sd_fixed,<span class="op">!</span><span class="kw">xor</span>(BLs<span class="op">==</span><span class="st">&quot;Forearm&quot;</span>,BLs<span class="op">==</span><span class="st">&quot;Forearm1&quot;</span>))</span>
<span id="cb321-6"><a href="chapter3.html#cb321-6"></a>sd_fixedR<span class="op">$</span>BLs &lt;-<span class="st"> </span><span class="kw">factor</span>(sd_fixedR<span class="op">$</span>BLs)</span></code></pre></div>
<p>3.2.1. Graphically explore the differences in the pain ratings across the different Body_Location levels using boxplots and pirate-plots. How are boxplots misleading for representing these data? <strong>Hint</strong>: look for discreteness in the responses.</p>
<p>3.2.2. Is the design balanced?</p>
<p>3.2.3. How does taking 3 measurements that are of interest each day lead to a violation of the independence assumption here?</p>
<p>3.2.4. Complete all 6+ steps of the hypothesis test using the parametric <span class="math inline">\(F\)</span>-test,
reporting the ANOVA table and the distribution of the test statistic under the null. For the scope of inference use the information that the sting locations were randomly assigned but only one person (the researcher) participated in the study.</p>
<p>3.2.5. Generate the permutation distribution and find the p-value. Compare the parametric
p-value to the permutation test results.</p>
<p>3.2.6. Often we might consider Tukey’s pairwise comparisons given the initial result here. How many levels are there in Body_Location? How many pairs would be compared if we tried Tukey’s – calculate using the <code>choose</code> function?</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Crampton1947">
<p>Crampton, E. 1947. “The Growth of the Odontoblast of the Incisor Teeth as a Criterion of Vitamin c Intake of the Guinea Pig.” <em>The Journal of Nutrition</em> 33 (5): 491–504. <a href="http://jn.nutrition.org/content/33/5/491.full.pdf">http://jn.nutrition.org/content/33/5/491.full.pdf</a>.</p>
</div>
<div id="ref-R-effects">
<p>Fox, John, Sanford Weisberg, Brad Price, Michael Friendly, and Jangman Hong. 2019. <em>Effects: Effect Displays for Linear, Generalized Linear, and Other Models</em>. <a href="https://CRAN.R-project.org/package=effects">https://CRAN.R-project.org/package=effects</a>.</p>
</div>
<div id="ref-Hothorn2008">
<p>Hothorn, Torsten, Frank Bretz, and Peter Westfall. 2008. “Simultaneous Inference in General Parametric Models.” <em>Biometrical Journal</em> 50 (3): 346–63.</p>
</div>
<div id="ref-R-multcomp">
<p>Hothorn, Torsten, Frank Bretz, and Peter Westfall. 2019. <em>Multcomp: Simultaneous Inference in General Parametric Models</em>. <a href="https://CRAN.R-project.org/package=multcomp">https://CRAN.R-project.org/package=multcomp</a>.</p>
</div>
<div id="ref-Piepho2004">
<p>Piepho, Hans-Peter. 2004. “An Algorithm for a Letter-Based Representation of All-Pairwise Comparisons.” <em>Journal of Computational and Graphical Statistics</em> 13 (2): 456–66.</p>
</div>
<div id="ref-Smith2014">
<p>Smith, Michael L. 2014. “Honey Bee Sting Pain Index by Body Location.” <em>PeerJ</em> 2 (April): e338. <a href="https://doi.org/10.7717/peerj.338">https://doi.org/10.7717/peerj.338</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="53">
<li id="fn53"><p>In Chapter <a href="chapter2.html#chapter2">2</a>, we used <code>lm</code> to get these estimates and focused on the estimate of the difference between the second group and the baseline - that was and still is the difference in the sample means. Now there are potentially more than two groups and we need to formalize notation to handle this more complex situation.<a href="chapter3.html#fnref53" class="footnote-back">↩︎</a></p></li>
<li id="fn55"><p>We can and will select the order of the levels of categorical variables as it can make plots easier to interpret.<a href="chapter3.html#fnref55" class="footnote-back">↩︎</a></p></li>
<li id="fn58"><p>Any further claimed precision is an exaggeration and eventually we might see p-values that approach the precision of the computer at 2.2e-16 and anything below 0.0001 should just be reported as being below 0.0001. Also note the way that R represents small or extremely large numbers using scientific notation such as <code>3e-4</code> which is <span class="math inline">\(3 \cdot 10^{-4} = 0.0003\)</span>.<a href="chapter3.html#fnref58" class="footnote-back">↩︎</a></p></li>
<li id="fn69"><p>In more complex models, this code can be used to create pair-wise comparisons on one of many explanatory variables.<a href="chapter3.html#fnref69" class="footnote-back">↩︎</a></p></li>
<li id="fn71"><p>Note that this method is implemented slightly differently than explained here in some software packages so if you see this in a journal article, read the discussion carefully.<a href="chapter3.html#fnref71" class="footnote-back">↩︎</a></p></li>
<li id="fn72"><p>There is a warning message produced by the default Tukey’s code here related to the algorithms used to generate approximate p-values and then the CLD, but the results seem reasonable and just a few p-values seem to vary in the second or third decimal points.<a href="chapter3.html#fnref72" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter4.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Greenwood_Book.pdf"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
